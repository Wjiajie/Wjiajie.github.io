<!DOCTYPE html>
<html lang="zh-CN">
    <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
    <meta charset="UTF-8" />

    <meta name="generator" content="Hugo 0.119.0"><meta name="theme-color" content="#fff" />
    <meta name="color-scheme" content="light dark">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <meta name="format-detection" content="telephone=no, date=no, address=no, email=no" />
    
    <meta http-equiv="Cache-Control" content="no-transform" />
    
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <title>3D Face 论文索引 | W</title>

    <link rel="stylesheet" href="/css/meme.min.e2a34fd6a7fceb929d3c04f57929b4b8c2e58411d3149771176a65fb330bc9a1.css"/>

    
    
        
            <script src="/js/meme.min.fc822ef956a26b79d064473732b590e06e47caa64a3d17a46efb5223daf463e4.js"></script>

        
    

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />

        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" media="print" onload="this.media='all'" />
        <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" /></noscript>

    <meta name="author" content="JiaJie" /><meta name="description" content="一些3d Face的相关论文，不定期更新。
……" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2a6df4" />
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-title" content="W" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="mobile-web-app-capable" content="yes" />
    <meta name="application-name" content="W" />
    <meta name="msapplication-starturl" content="../../../" />
    <meta name="msapplication-TileColor" content="#fff" />
    <meta name="msapplication-TileImage" content="../../../icons/mstile-150x150.png" />
    <link rel="manifest" href="/manifest.json" />

    
    

    
        <link rel="canonical" href="https://wjiajie.github.io/contents/face-reconstruction/3d-face-papers/" />
    

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "datePublished": "2019-12-04T18:05:33+00:00",
        "dateModified": "2019-12-28T01:46:26+08:00",
        "url": "https://wjiajie.github.io/contents/face-reconstruction/3d-face-papers/",
        "headline": "3D Face 论文索引",
        "description": "一些3d Face的相关论文，不定期更新。\n……",
        "inLanguage" : "zh-CN",
        "articleSection": "contents",
        "wordCount":  2641 ,
        "image": "https://wjiajie.github.io/icons/apple-touch-icon.png",
        "author": {
            "@type": "Person",
            "description": "Viva La Vida",
            "email": "jiajiewu233@gamil.com",
            "image": "https://s2.loli.net/2023/02/25/bTD9PrGNyC8kRi5.png",
            "url": "https://www.jiajiewu.top/",
            "name": "JiaJie"
        },
        "license": "[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)",
        "publisher": {
            "@type": "Organization",
            "name": "W",
            "logo": {
                "@type": "ImageObject",
                "url": "https://wjiajie.github.io/icons/apple-touch-icon.png"
            },
            "url": "https://wjiajie.github.io/"
        },
        "mainEntityOfPage": {
            "@type": "WebSite",
            "@id": "https://wjiajie.github.io/"
        }
    }
</script>

    

<meta name="twitter:card" content="summary" />


    



<meta property="og:title" content="3D Face 论文索引" />
<meta property="og:description" content="一些3d Face的相关论文，不定期更新。
……" />
<meta property="og:url" content="https://wjiajie.github.io/contents/face-reconstruction/3d-face-papers/" />
<meta property="og:site_name" content="W" />
<meta property="og:locale" content="zh" /><meta property="og:image" content="https://wjiajie.github.io/icons/apple-touch-icon.png" />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2019-12-04T18:05:33&#43;00:00" />
    <meta property="article:modified_time" content="2019-12-28T01:46:26&#43;08:00" />
    
    <meta property="article:section" content="contents" />


        <link rel="preconnect" href="https://www.google-analytics.com" crossorigin />

        


    
    <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', '');
    </script>




    
    

    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css">
<script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>



<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@700&amp;text=reuixiy&amp;display=swap" media="print" onload="this.media='all'" />
<noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@700&amp;text=reuixiy&amp;display=swap" /></noscript>





</head>

    <body>
        <div class="container">
            
    <header class="header">
        
            <div class="header-wrapper">
                <div class="header-inner single">
                    
    <div class="site-brand">
        
            <a href="/" class="brand">W</a>
        
    </div>

                    <nav class="nav">
    <ul class="menu" id="menu">
        
            
        
        
        
        
            
                <li class="menu-item"><a href="/contents/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon folder"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg><span class="menu-item-name">Contents</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/contents/photos/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" class="icon eye"><path d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"/></svg><span class="menu-item-name">Photos</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/about/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon plus-circle"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm144 276c0 6.6-5.4 12-12 12h-92v92c0 6.6-5.4 12-12 12h-56c-6.6 0-12-5.4-12-12v-92h-92c-6.6 0-12-5.4-12-12v-56c0-6.6 5.4-12 12-12h92v-92c0-6.6 5.4-12 12-12h56c6.6 0 12 5.4 12 12v92h92c6.6 0 12 5.4 12 12v56z"/></svg><span class="menu-item-name">About</span></a>
                </li>
            
        
            
                
                    
                    
                        <li class="menu-item">
                            <a id="theme-switcher" href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5l48.8-97.5a18 18 0 0128 0l48.8 97.5 103.4 -34.5a18 18 0 0119.8 19.8l-34.5 103.4l97.5 48.8a18 18 0 010 28l-97.5 48.8 34.5 103.4a18 18 0 01-19.8 19.8l-103.4-34.5-48.8 97.5a18 18 0 01-28 0l-48.8-97.5l-103.4 34.5a18 18 0 01-19.8-19.8l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8-34.5-103.4a18 18 0 0119.8-19.8zM256 128a128 128 0 10.01 0M256 160a96 96 0 10.01 0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412a256 256 0 10154-407a11.5 11.5 0 00-5 20a201.5 201.5 0 01-134 374a11.5 11.5 0 00-15 13"/></svg></a>
                        </li>
                    
                
            
        
            
                
            
        
            
                
            
        
    </ul>
</nav>

                    
                </div>
            </div>
            
    <input type="checkbox" id="nav-toggle" aria-hidden="true" />
    <label for="nav-toggle" class="nav-toggle"></label>
    <label for="nav-toggle" class="nav-curtain"></label>


        
    </header>




            
            
    <main class="main single" id="main">
    <div class="main-inner">

        

        <article class="content post h-entry" data-align="justify" data-type="contents" data-toc-num="true">

            <h1 class="post-title p-name">3D Face 论文索引</h1>

            

            
                
            

            
                

<div class="post-meta">
    
        
        <time datetime="2019-12-04T18:05:33&#43;00:00" class="post-meta-item published dt-published"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M148 288h-40c-6.6 0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h48c26.5 0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"/></svg>&nbsp;2019.12.4</time>
    
    
        
        <time datetime="2019-12-28T01:46:26&#43;08:00" class="post-meta-item modified dt-updated"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M400 64h-48V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H160V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H48C21.49 64 0 85.49 0 112v352c0 26.51 21.49 48 48 48h352c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm-6 400H54a6 6 0 0 1-6-6V160h352v298a6 6 0 0 1-6 6zm-52.849-200.65L198.842 404.519c-4.705 4.667-12.303 4.637-16.971-.068l-75.091-75.699c-4.667-4.705-4.637-12.303.068-16.971l22.719-22.536c4.705-4.667 12.303-4.637 16.97.069l44.104 44.461 111.072-110.181c4.705-4.667 12.303-4.637 16.971.068l22.536 22.718c4.667 4.705 4.636 12.303-.069 16.97z"/></svg>&nbsp;2019.12.28</time>
    
    
    
        
        
        
            
        
    
    
        
        <span class="post-meta-item wordcount"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;2641</span>
    
    
        
        <span class="post-meta-item reading-time"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;6&nbsp;分钟</span>
    
    
    
</div>

            

            <nav class="contents">
  <h2 id="contents" class="contents-title">目录</h2><ol class="toc">
    <li><a id="contents:surveys--doctoral-thesis" href="#surveys--doctoral-thesis">Surveys &amp; Doctoral Thesis</a></li>
    <li><a id="contents:papers--codes" href="#papers--codes">Papers &amp; Codes</a>
      <ol>
        <li><a id="contents:reconstruction3d-alignmentcorrespondences" href="#reconstruction3d-alignmentcorrespondences">Reconstruction&amp;3D Alignment&amp;Correspondences</a>
          <ol>
            <li><a id="contents:1998---2015" href="#1998---2015">1998 - 2015</a></li>
            <li><a id="contents:2016" href="#2016">2016</a></li>
            <li><a id="contents:2017" href="#2017">2017</a></li>
            <li><a id="contents:2018" href="#2018">2018</a></li>
          </ol>
        </li>
        <li><a id="contents:production-level-reconstruction" href="#production-level-reconstruction">Production-level Reconstruction</a></li>
        <li><a id="contents:texture" href="#texture">Texture</a></li>
        <li><a id="contents:transferreenactmentapplications" href="#transferreenactmentapplications">Transfer&amp;Reenactment(Applications)</a></li>
        <li><a id="contents:3d-aid-2d-face-recognition" href="#3d-aid-2d-face-recognition">3D-aid 2D face recognition</a></li>
        <li><a id="contents:3d-face-recognition" href="#3d-face-recognition">3D face recognition</a></li>
      </ol>
    </li>
  </ol>
</nav><div class="post-body e-content">
                <p>一些3d Face的相关论文，不定期更新。</p>
<h1 id="3d-face"><a href="#3d-face" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:3d-face" class="headings">3D Face</a></h1>
<h2 id="surveys--doctoral-thesis"><a href="#surveys--doctoral-thesis" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:surveys--doctoral-thesis" class="headings">Surveys &amp; Doctoral Thesis</a></h2>
<ul>
<li>
<p>Face Image Analysis using a Multiple Features Fitting Strategy(2005, Basel)</p>
</li>
<li>
<p>3D Face Modelling for 2D+3D Face Recognition(2007, Surrey)</p>
</li>
<li>
<p>Image Based 3D Face Reconstruction: A Survey(IJIG2009, Georgios Stylianou, Andreas Lanitis, EUC, CUT)</p>
<p><code>early 3D facial acquisition approaches</code></p>
</li>
<li>
<p>animation reconstruction of deformable surfaces(2010, Hao Li, ETHz)</p>
</li>
<li>
<p>Inverse Rendering of Faces with a 3D Morphable Model(2012, Oswald Aldrian, York)</p>
</li>
<li>
<p>Digital Geometry Processing Theory and Applications(2012, Kun Zhou, Zhengjiang, 中文)</p>
</li>
<li>
<p><em><strong>State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications</strong></em></p>
<p><strong>State of the Art on 3D Reconstruction with RGB-D Cameras</strong>(EG2018, MZ, CT, MPI, Stanford, TUM, Disney, Technicolor, UEN) <a href="http://web.stanford.edu/~zollhoef/papers/EG18_FaceSTAR/page.html" target="_blank" rel="noopener">[talks]</a></p>
</li>
</ul>
<h2 id="papers--codes"><a href="#papers--codes" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:papers--codes" class="headings">Papers &amp; Codes</a></h2>
<h3 id="reconstruction3d-alignmentcorrespondences"><a href="#reconstruction3d-alignmentcorrespondences" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:reconstruction3d-alignmentcorrespondences" class="headings">Reconstruction&amp;3D Alignment&amp;Correspondences</a></h3>
<h4 id="1998---2015"><a href="#1998---2015" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:1998---2015" class="headings">1998 - 2015</a></h4>
<ul>
<li>
<p>A Morphable Model For The Synthesis Of 3D Faces</p>
<p>(SIGGRAPH1998, <a href="https://scholar.google.com.hk/citations?user=jYCidWgAAAAJ&amp;hl=zh-CN&amp;oi=sra" target="_blank" rel="noopener">V Blanz</a>, <a href="https://scholar.google.com.hk/citations?user=HKLgZpYAAAAJ&amp;hl=zh-CN&amp;oi=sra" target="_blank" rel="noopener">T Vetter</a> , MPI)</p>
<p><code>3dmm,analysis-by-synthesis(cascaded, coarse to fine, using texture information), mid-detail </code></p>
</li>
<li>
<p>Efficient, Robust and Accurate Fitting of a 3D Morphable Model</p>
<p>(ICCV2003, S Romdhani, <a href="https://scholar.google.com.hk/citations?user=HKLgZpYAAAAJ&amp;hl=zh-CN&amp;oi=sra" target="_blank" rel="noopener">T Vetter</a> , Basel)</p>
<p><code>3dmm, fitting Algorithm needs: Efficient, Robust, Accurate, Automatic. Mid-detail</code></p>
</li>
<li>
<p>Estimating 3D Shape and Texture Using Pixel Intensity, Edges, Specular Highlights, Texture Constraints and a Prior</p>
<p>(CVPR2005, S Romdhani, <a href="https://scholar.google.com.hk/citations?user=HKLgZpYAAAAJ&amp;hl=zh-CN&amp;oi=sra" target="_blank" rel="noopener">T Vetter</a> , Basel)</p>
<p><code>3dmm, multiple features</code></p>
</li>
<li>
<p>A 3D Face Model for Pose and Illumination Invariant Face Recognition</p>
<p>(AVSS2009, Paysan, P., Knothe, R., Amberg, B., Romdhani, S., &amp; Vetter, T. , Basel) [<a href="BFM">data</a>](<a href="https://faces.cs.unibas.ch/bfm/" target="_blank" rel="noopener">https://faces.cs.unibas.ch/bfm/</a>)</p>
</li>
<li>
<p>3D Face Reconstruction from a Single Image Using a Single Reference Face Shape</p>
<p>(TPAMI2011, <a href="https://scholar.google.com.hk/citations?user=P97vI1EAAAAJ&amp;hl=zh-CN&amp;oi=sra" target="_blank" rel="noopener">I Kemelmacher-Shlizerman</a>, Basri R, UW)</p>
<p><code>template, sfs, texture information, mid-detail</code></p>
</li>
<li>
<p>Face Reconstruction in the Wild</p>
<p>(ICCV2011, Kemelmacher-Shlizerman I, Seitz S M , UW)</p>
<p><code>collection, sparse correspondence, warp template,  low-rank approximation(photometric stereo, for expression normalization), mid-detail </code></p>
</li>
<li>
<p>A FACS Valid 3D Dynamic Action Unit Database with Applications to 3D Dynamic Morphable Facial Modeling</p>
<p>(ICCV2011, Cosker D, Krumhuber E, Hilton A. , UofSurrey)</p>
<p><code>aam, expression</code></p>
</li>
<li>
<p>Viewing Real-World Faces in 3D</p>
<p>(ICCV2013, <a href="https://scholar.google.com.hk/citations?user=ehe5pyIAAAAJ&amp;hl=zh-CN&amp;oi=sra" target="_blank" rel="noopener">T Hassner</a>, Open U Israel)</p>
<p><code>template, sparse correspondence, pose adjustment, depth optimization(SIFT)  </code></p>
</li>
<li>
<p>Improving 3D Face Details based on Normal Map of Hetero-source Images</p>
<p>(CVPRW2014, Yang, C., Chen, J., Su, N., &amp; Su, G. , Tsinghua University)</p>
</li>
<li>
<p>Total Moving Face Reconstruction</p>
<p>(LNCS2014, Suwajanakorn S, Kemelmacher-Shlizerman I, Seitz S M. , Washington)</p>
<p><code>video(collections), template, average shape, pose estimation, 3d flow(correspondence), refinement, high-detail </code></p>
</li>
<li>
<p>FaceWarehouse: a 3D Facial Expression Database for Visual Computing</p>
<p>(VCG2014, Cao, C., Weng, Y., Zhou, S., Tong, Y., &amp; Zhou, K., Zhejiang) <a href="">[data]</a></p>
</li>
<li>
<p>Intrinsic Face Image Decomposition with Human Face Priors</p>
<p>(ECCV2014, Li C, Zhou K, Lin S , Zhejiang)</p>
</li>
<li>
<p>Fitting 3D Morphable Models using Local Features</p>
<p>(ICIP2015, Huber, P., Feng, Z. H., Christmas, W., Kittler, J., &amp; Ratsch, M, Surrey)</p>
<p><code>sparse correspondence, 3dmm, regression</code></p>
</li>
<li>
<p>What Makes Tom Hanks Look Like Tom Hanks</p>
<p>(ICCV2015, Suwajanakorn S, Seitz S M, Kemelmacher-Shlizerman I. , Washington)</p>
<p><code>collection, template, average model, 3D flow, correspondence, deformation vector, TPS, expression sililarity weighted, high-frequency details, Laplacian pyramid      </code></p>
</li>
<li>
<p>Unconstrained Realtime Facial Performance Capture</p>
<p>(CVPR2015, Hsieh, P. L., Ma, C., Yu, J., &amp; Li, H. , USC)</p>
<p><code>video,image collections, occlusion, segmentation, landmarks </code></p>
</li>
<li>
<p>Unconstrained 3D Face Reconstruction</p>
<p>(CVPR2015, Roth, J., Tong, Y., &amp; Liu, X, MSU)</p>
<p><code>collection, sparse correspondence(landmarks), template, photometric stereo(SVD), matrix completion,  </code></p>
</li>
<li>
<p>Pose-Invariant 3D Face Alignment</p>
<p>(ICCV2015, Jourabloo, A., &amp; Liu, X., MSU)</p>
<p><code>alignment, dense correspondence, visibility, cascaded regressor, 3DPDM. </code></p>
</li>
<li>
<p>Discriminative 3D Morphable Model Fitting</p>
<p>(CVPR2015)</p>
</li>
</ul>
<h4 id="2016"><a href="#2016" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:2016" class="headings">2016</a></h4>
<p><strong>#CVPR</strong></p>
<ul>
<li>
<p>Large-pose Face Alignment via CNN-based Dense 3D Model Fitting</p>
<p>(CVPR2016, Jourabloo, A., &amp; Liu, X., MSU)</p>
<p><code>alignment, 3dmm </code></p>
</li>
<li>
<p>Automated 3D Face Reconstruction from Multiple Images using Quality Measures</p>
<p>(CVPR2016, Piotraschke, M., &amp; Blanz, V , Siegen)</p>
</li>
<li>
<p>A Robust Multilinear Model Learning Framework for 3D Faces</p>
<p>(CVPR2016, Bolkart, T., &amp; Wuhrer, S., Saarland)</p>
</li>
<li>
<p>Face Alignment Across Large Poses: A 3D Solution</p>
<p>(CVPR2016, Zhu, X., Lei, Z., Liu, X., Shi, H., &amp; Li, S. Z. , MSU, CASIA)</p>
</li>
<li>
<p>Adaptive 3D Face Reconstruction from Unconstrained Photo Collections</p>
<p>(CVPR2016, Roth, J., Tong, Y., &amp; Liu, X, MSU)</p>
<p><code>landmarks, 3dmm, coarse-to-fine,photometric stereo, time: 7 minutes  </code></p>
</li>
<li>
<p>Augmented Blendshapes for Real-time Simultaneous 3D Head Modeling and Facial Motion Capture</p>
<p>(CVPR2016, Thomas, D., &amp; Taniguchi, R. I. , Kyushu University)</p>
</li>
<li>
<p>A 3D Morphable Model learnt from 10,000 faces</p>
<p>(CVPR2016, Booth, J., Roussos, A., Zafeiriou, S., Ponniah, A., &amp; Dunaway, D., ICL)</p>
</li>
</ul>
<p><strong>#ECCV</strong></p>
<ul>
<li>
<p>Joint Face Alignment and 3D Face Reconstruction</p>
<p>(ECCV2016, Liu, F., Zeng, D., Zhao, Q., &amp; Liu, X, MSU, Sichuan U)</p>
<p><code>alignment, landmarks</code></p>
</li>
<li>
<p>Real-Time Facial Segmentation and Performance Capture from RGB Input</p>
<p>(ECCV2016, Saito, S., Li, T., &amp; Li, H. , USC)</p>
<p><code>occlusions, tracking</code></p>
</li>
</ul>
<p><strong>#Others</strong></p>
<ul>
<li>
<p>3D Face Reconstruction by Learning from Synthetic Data</p>
<p>(3DV2016, Richardson, E., Sela, M., &amp; Kimmel, R, IIT)</p>
<p><code>3dmm, cnn, regress 3dmm parameters, sfs</code></p>
</li>
<li>
<p>Face Reconstruction on Mobile Devices Using a Height Map Shape Model and Fast Regularization</p>
<p>(3DV2016, Maninchedda, F., Häne, C., Oswald, M. R., &amp; Pollefeys, M., ETH)</p>
</li>
<li>
<p>A Multiresolution 3D Morphable Face Model and Fitting Framework  <a href="https://github.com/patrikhuber/eos" target="_blank" rel="noopener">[code]</a></p>
<p>(IJCV2016, Huber, P., Hu, G., Tena, R., Mortazavian, P., Koppen, P., Christmas, W. J., ... &amp; Kittler, J. ，Surrey)</p>
</li>
<li>
<p>Rapid Photorealistic Blendshape Modeling from RGB-D Sensors</p>
<p>(2016, USC)</p>
</li>
<li>
<p>3D Face Reconstruction with Region Based Best Fit Blending Using Mobile Phone for Virtual Reality Based Social Media</p>
<p>(2016, Anbarjafari, G., Haamer, R. E., Lusi, I., Tikk, T., &amp; Valgma, L. , Turkey)</p>
<p><code>landmarks, uv texture, region</code></p>
</li>
</ul>
<h4 id="2017"><a href="#2017" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:2017" class="headings">2017</a></h4>
<p><strong>#CVPR</strong></p>
<ul>
<li>
<p>3D Face Morphable Models “In-the-Wild”</p>
<p>(CVPR2017, Booth, J., Antonakos, E., Ploumpis, S., Trigeorgis, G., Panagakis, Y., &amp; Zafeiriou, S., ICL)</p>
<p><code>3dmm, register, UV</code></p>
</li>
<li>
<p>Face Normals “in-the-wild” using Fully Convolutional Networks</p>
<p>(CVPR2017, Trigeorgis, G., Snape, P., Kokkinos, I., &amp; Zafeiriou, S. , ICL)</p>
</li>
<li>
<p>Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network</p>
<p>(CVPR2017, Tran, A. T., Hassner, T., Masi, I., &amp; Medioni, G. , USC)</p>
</li>
<li>
<p>Fast 3D Reconstruction of Faces with Glasses</p>
<p>(CVPR2017, Maninchedda, F., Oswald, M. R., &amp; Pollefeys, M., ETH)</p>
</li>
<li>
<p>DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild</p>
<p>(CVPR2017, Güler, R. A., Trigeorgis, G., Antonakos, E., Snape, P., Zafeiriou, S., &amp; Kokkinos, I. , ICL)</p>
<p><code>dense correspondence, uv</code></p>
</li>
<li>
<p>Learning Detailed Face Reconstruction from a Single Image</p>
<p>(CVPR2017, Richardson, E., Sela, M., Or-El, R., &amp; Kimmel, R. , Washington)</p>
</li>
<li>
<p>End-to-end 3D face reconstruction with deep neural networks</p>
<p>(CVPR2017, Dou, P., Shah, S. K., &amp; Kakadiaris, I. A., UofHouston)</p>
<p><code>3dmm, dl, directly learn 3dmm parameters</code></p>
</li>
<li>
<p>A Generative Model for Depth-based Robust 3D Facial Pose Tracking</p>
<p>(CVPR2017, Cai, L. S. J., Pavlovic, T. J. C. V., &amp; Ngan, K. N. , CUHK)</p>
<p><code>occlusions</code></p>
</li>
</ul>
<p><strong>#ICCV</strong></p>
<ul>
<li>
<p>3D Morphable Models as Spatial Transformer Networks</p>
<p>(ICCV2017, Bas, A., Huber, P., Smith, W. A., Awais, M., &amp; Kittler, J. , York, Surrey )</p>
<p><code>dl, cnn, uv texture, landmarks, stn, 3dmm</code></p>
</li>
<li>
<p>Faster Than Real-time Facial Alignment: A 3D Spatial Transformer Network Approach in Unconstrained Poses</p>
<p>(ICCV2017, Bhagavatula, C., Zhu, C., Luu, K., &amp; Savvides, M., CMU)</p>
</li>
<li>
<p>Pose-Invariant Face Alignment with a Single CNN</p>
<p>(ICCV2017, Jourabloo, A., Ye, M., Liu, X., &amp; Ren, L. , MSU)</p>
<p><code>alignment, 3dmm, dl, cnn</code></p>
</li>
<li>
<p>Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression <a href="https://github.com/AaronJackson/vrn" target="_blank" rel="noopener">code</a></p>
<p>(ICCV2017, Jackson, A. S., Bulat, A., Argyriou, V., &amp; Tzimiropoulos, G. , Nottingham)</p>
<p><code>end-to-end, 3dmm, dl, cnn, landmarks, voxel  </code></p>
</li>
<li>
<p>Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation</p>
<p>(ICCV2017, Sela, M., Richardson, E., &amp; Kimmel, R. , IIT)</p>
</li>
<li>
<p>MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction</p>
<p>(ICCV2017, Tewari, A., Zollhöfer, M., Kim, H., Garrido, P., Bernard, F., Pérez, P., &amp; Theobalt, C. , MPI)</p>
</li>
<li>
<p>Dense Face Alignment <a href="http://cvlab.cse.msu.edu/project-pifa.html" target="_blank" rel="noopener">code</a></p>
<p>(ICCVW2017, Liu, Y., Jourabloo, A., Ren, W., &amp; Liu, X. , MSU)</p>
</li>
<li>
<p>Realtime Dynamic 3D Facial Reconstruction for Monocular Video In-the-Wild</p>
<p>(ICCVW2017)</p>
</li>
<li>
<p>Learning Dense Facial Correspondences in Unconstrained Images</p>
<p>(ICCV2017, Yu, R., Saito, S., Li, H., Ceylan, D., &amp; Li, H.  , USC)</p>
<p><code>dense correspondence</code></p>
</li>
</ul>
<p><strong>#others</strong></p>
<ul>
<li>
<p>Large Scale 3D Morphable Models [<a href="LSFM">data</a>](<a href="https://faces.cs.unibas.ch/bfm/" target="_blank" rel="noopener">https://faces.cs.unibas.ch/bfm/</a>)</p>
<p>(IJCV2017, Booth, J., Roussos, A., Ponniah, A., Dunaway, D., &amp; Zafeiriou, S. , ICL)</p>
<p><code>for alignment, template, cnn, dl, sparse correspondence, landmarks, tps warping </code></p>
</li>
<li>
<p>What does 2D geometric information really tell us about 3D face shape?  (2017, Bas, A., &amp; Smith, W. A )</p>
<p><code>shape from landmarks, shape from contours</code></p>
</li>
<li>
<p>Pix2Face: Direct 3D Face Model Estimation</p>
<p>(2017)</p>
<p><code>dense correspondence, 3dmm</code></p>
</li>
<li>
<p>3D Face Reconstruction with Geometry Details from a Single Image</p>
<p>(TIP2017, Jiang, L., Zhang, J., Deng, B., Li, H., &amp; Liu, L. , USC, USTC)</p>
<p><code>coarse-to-fine, landmarks, corrective deformatio, sfs  </code></p>
</li>
</ul>
<h4 id="2018"><a href="#2018" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:2018" class="headings">2018</a></h4>
<p><strong>#CVPR</strong></p>
<ul>
<li>
<p>Unsupervised Training for 3D Morphable Model Regression  <a href="https://github.com/google/tf_mesh_renderer" target="_blank" rel="noopener">code</a></p>
<p>(CVPR2018, Genova, K., Cole, F., Maschinot, A., Sarna, A., Vlasic, D., &amp; Freeman, W. T , Google)</p>
</li>
</ul>
<ul>
<li>
<p>4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications  <a href="">data</a></p>
<p>(CVPR2018, Cheng, S., Kotsia, I., Pantic, M., &amp; Zafeiriou, S., ICL)</p>
</li>
<li>
<p>Sparse Photometric 3D Face Reconstruction Guided by Morphable Models</p>
<p>(CVPR2018, Cao, X., Chen, Z., Chen, A., Chen, X., Li, S., &amp; Yu, J.  , shanghaitech)</p>
<p><code>5 input images, 3dmm, shadow processing, light calibration, photometric stereo, denoising </code></p>
</li>
<li>
<p>Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition</p>
<p>(CVPR2018, Liu, F., Zhu, R., Zeng, D., Zhao, Q., &amp; Liu, X. , MSU, Sichuan)</p>
</li>
<li>
<p>Mesoscopic Facial Geometry Inference Using Deep Neural Networks</p>
<p>(CVPR2018, Hao Li, USC)</p>
<p><code>high-detail, dl, scan, uv texture, displacement </code></p>
</li>
<li>
<p>Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz</p>
<p>(CVPR2018, Tewari, A., Zollhöfer, M., Garrido, P., Bernard, F., Kim, H., Pérez, P., &amp; Theobalt, C., MPI)</p>
</li>
<li>
<p>SfSNet : Learning Shape, Reflectance and Illuminance of Faces in the Wild</p>
<p>(CVPR2018, Sengupta, S., Kanazawa, A., Castillo, C. D., &amp; Jacobs, D.  , Maryland, UCB )</p>
</li>
<li>
<p>Probabilistic Joint Face-Skull Modelling for Facial Reconstruction</p>
<p>(CVPR2018, Madsen, D., Lüthi, M., Schneider, A., &amp; Vetter, T. , Basel)</p>
</li>
<li>
<p>Alive Caricature from 2D to 3D</p>
<p>(CVPR2018, Wu, Q., Zhang, J., Lai, Y. K., Zheng, J., &amp; Cai, J, USTC)</p>
</li>
<li>
<p>Nonlinear 3D Face Morphable Model</p>
<p>(CVPR2018, Tran, L., &amp; Liu, X. , MSU)</p>
</li>
<li>
<p>InverseFaceNet: Deep Monocular Inverse Face Rendering</p>
<p>(CVPR2018, Kim, H., Zollhöfer, M., Tewari, A., Thies, J., Richardt, C., &amp; Theobalt, C. , MPI)</p>
<p><code>Self-Supervised Bootstrapping</code></p>
</li>
<li>
<p>Extreme 3D Face Reconstruction: Looking Past Occlusions</p>
<p>(CVPR2018, Tran, A. T., Hassner, T., Masi, I., Paz, E., Nirkin, Y., &amp; Medioni, G. , USC)</p>
</li>
<li>
<p>Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies</p>
<p>(CVPR2018, best student paper, Joo, H., Simon, T., &amp; Sheikh, Y. , CMU)</p>
</li>
<li>
<p>Modeling Facial Geometry using Compositional VAEs</p>
<p>(CVPR2018, Bagautdinov, T., Wu, C., Saragih, J., Fua, P., &amp; Sheikh, Y., EPEL, FRL )</p>
</li>
</ul>
<p><strong>#ECCV</strong></p>
<ul>
<li>
<p>3D Face Reconstruction from Light Field Images: A Model-free Approach</p>
<p>(ECCV2018, Feng, M., Gilani, S. Z., Wang, Y., &amp; Mian, A., Western Australia, Hunan)</p>
<p><code>epopolar plane images</code></p>
</li>
<li>
<p>Generating 3D Faces using Convolutional Mesh Autoencoders <a href="https://github.com/anuragranj/coma" target="_blank" rel="noopener">[code]</a></p>
<p>(ECCV2018, Ranjan, A., Bolkart, T., Sanyal, S., &amp; Black, M. J., MPI)</p>
</li>
<li>
<p>Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network <a href="https://github.com/YadiraF/PRNet" target="_blank" rel="noopener">[code]</a></p>
<p>(ECCV2018, Feng, Y., Wu, F., Shao, X., Wang, Y., &amp; Zhou, X., SJTU)</p>
</li>
</ul>
<p><strong>#others</strong></p>
<ul>
<li>
<p>Morphable Face Models - An Open Framework</p>
<p>(FG2018, Gerig, T., Morel-Forster, A., Blumer, C., Egger, B., Luthi, M., Schönborn, S., &amp; Vetter, T. , Basel)</p>
</li>
<li>
<p>CNN-based Real-time Dense Face Reconstruction with Inverse-rendered Photo-realistic Face Images <a href="https://github.com/Juyong/3DFace" target="_blank" rel="noopener">[data&amp;code]</a></p>
<p>(TPAMI2018, Yudong Guo, Juyong Zhang, Jianfei Cai, Boyi Jiang, Jianmin Zheng, USTC)</p>
</li>
<li>
<p>Multilinear Autoencoder for 3D Face Model Learning(WACV 2018,  Universite Grenoble Alpes (LJK), France)</p>
<p><code>3d scan to registered mesh. dl. height map  </code></p>
</li>
<li>
<p>On Face Segmentation, Face Swapping, and Face Perception(AFGR,2018, HT)</p>
</li>
<li>
<p>Evaluation of Dense 3D Reconstruction from 2D Face Images in the Wild(FG2018) <a href="">data</a></p>
</li>
</ul>
<p><strong>#arxiv</strong></p>
<ul>
<li>
<p>Joint Face Alignment and 3D Face Reconstruction with Application to Face Recognition(2017, Feng Liu, Xiaoming Liu)</p>
</li>
<li>
<p>Convolutional Point-set Representation: A Convolutional Bridge Between a Densely Annotated Image and 3D Face Alignment(20180317)</p>
</li>
<li>
<p>Unsupervised Depth Estimation, 3D Face Rotation and Replacement(20180325)</p>
</li>
</ul>
<h3 id="production-level-reconstruction"><a href="#production-level-reconstruction" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:production-level-reconstruction" class="headings">Production-level Reconstruction</a></h3>
<blockquote>
<p>more in computer graphics</p>
</blockquote>
<ul>
<li>
<p>High-Quality Single-Shot Capture of Facial Geometry(TOG2010, ETHZ, Disney)</p>
<p><code>cg, high-detail,stereo system, calibration, surface refinement, normal direction, mesoscopic  </code></p>
</li>
<li>
<p>Multiview Face Capture using Polarized Spherical Gradient Illumination(TOG2011)</p>
<p><code>image collecitons</code></p>
</li>
<li>
<p>High-Quality Passive Facial Performance Capture using Anchor Frames(SIGGRAPH2011, ETHZ, Disney)</p>
<p><code>cg, stereo,anchor frame, tracking, mesh progration, physical movement, motion estimation, refinement </code></p>
</li>
<li>
<p>Lightweight binocular facial perfor- mance capture under uncontrolled lighting(TOG2012, MPI)</p>
<p><code>cg, high-detail, stereo, template,flow,data term, geometry term, smoothness term, mesh tracking, motion refinement, shape refinement, sfs  </code></p>
</li>
<li>
<p>Reconstructing Detailed Dynamic Face Geometry from Monocular Video(TOG2013,  MPI)</p>
<p><code>cg, dynamic, high-detail, blend model, sparse correspondence, dense correspondence(appearance matching, LBP), pose estimation , shape refinement, sfs   </code></p>
</li>
<li>
<p>3D Shape Regression for Real-time Facial Animation(TOG2013, ZJU)</p>
</li>
<li>
<p>Real-Time High-Fidelity Facial Performance Capture (TOG2015, ZJU)</p>
<p><code>cg, landmarks, optical flow, train a regressor to learn detail  </code></p>
</li>
<li>
<p>Dynamic 3D Avatar Creation from Hand-held Video Input(TOG2015,  EPEL)</p>
<p><code>cg, dynamic, mobile, high-detail, avatar, 3dmm,sparse correspondence, eye mesh, tracking, refinement, sfs, detail map     </code></p>
</li>
<li>
<p>Reconstruction of Personalized 3D Face Rigs from Monocular Video(TOG2016,  MPI)</p>
<p><code>parametric shape prior, coarse-scale reconstruction, fine-scale(sfs), coase-&gt;medium-&gt;fine, 3dmm, corrective   </code></p>
</li>
<li>
<p>Production-Level Facial Performance Capture Using Deep Convolutional Neural Networks（ASCA2017,  USC)</p>
</li>
<li>
<p>Multi-View Stereo on Consistent Face Topology(EG2017, USC)</p>
<p><code>cg, high-detail, landmarks, template, pose estimation, refinement</code></p>
</li>
<li>
<p>Avatar Digitization From a Single Image For Real-Time Rendering(SIGGRAPH Asia 2017, USC)</p>
<p><code>cg, avatar, segmentation, head, hair, 3DMM, landmarks, texture completion   </code></p>
</li>
<li>
<p>Learning a model of facial shape and expression from 4D scans(TOG2017, USC, MPI)</p>
</li>
<li>
<p>DeepSketch2Face: A Deep Learning Based Sketching System for 3D Face and Caricature Modeling(SIGGRAPH2017)</p>
</li>
<li>
<p>High-Fidelity Facial Reflectance and Geometry Inference From an Unconstrained Image(SIGGRAPH2018, USC)</p>
</li>
</ul>
<h3 id="texture"><a href="#texture" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:texture" class="headings">Texture</a></h3>
<blockquote>
<p>3D-aid texture generation/ UV texture completion<br>
Keys: GAN</p>
</blockquote>
<ul>
<li>
<p>Face Synthesis from Facial Identity Features(CVPR2017, google)</p>
<p><code>3dmm, dl, landmarks</code></p>
</li>
<li>
<p>Photorealistic Facial Texture Inference Using Deep Neural Networks(CVPR2017, Hao Li, USC)</p>
<p><code>texture completion</code></p>
</li>
<li>
<p>UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition(CVPR2018, SZ, ICL)</p>
<p><code>gan, 3dmm, uv texture completion</code></p>
</li>
<li>
<p>Multi-Attribute Robust Component Analysis for Facial UV Maps(2017, SZ, ICL)</p>
</li>
<li>
<p>Realistic Dynamic Facial Textures from a Single Image using GANs(CVPR2017, Hao Li, USC, DeepMind)</p>
</li>
<li>
<p>Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model(2018)</p>
</li>
<li>
<p>Side Information for Face Completion: a Robust PCA Approach(20180120, SZ, ICL)</p>
<p>​</p>
</li>
</ul>
<h3 id="transferreenactmentapplications"><a href="#transferreenactmentapplications" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:transferreenactmentapplications" class="headings">Transfer&amp;Reenactment(Applications)</a></h3>
<ul>
<li>
<p>Face Transfer with Multilinear Models (SIGGRAPH2005)</p>
<p><code>Cartesian product(ID x EX x VI)</code></p>
</li>
<li>
<p>Online Modeling For Realtime Facial Animation(TOG2013)</p>
<p><code>rgbd, blendshape, corrective field </code></p>
</li>
<li>
<p>Displaced Dynamic Expression Regression for Real-time Facial Tracking and Animation(SIGGRAPH2014)</p>
</li>
<li>
<p>Real-time Expression Transfer for Facial Reenactment(SIGGRAPH AISA 2015)</p>
</li>
<li>
<p>Face2Face: Real-time Face Capture and Reenactment of RGB Videos(CVPR2016)</p>
<p><code>capture, transfer, 3dmm, landmarks, texture, expression, mouth retrieval </code></p>
</li>
<li>
<p>Synthesizing Obama: Learning Lip Sync from Audio(SIGGRAPH2017)</p>
</li>
<li>
<p>Deep Video Portrait(SIGGRAPH2018)</p>
</li>
<li>
<p>HeadOn: Real-time Reenactment of Human Portrait Videos(SIGGRAPH2018)</p>
</li>
</ul>
<h3 id="3d-aid-2d-face-recognition"><a href="#3d-aid-2d-face-recognition" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:3d-aid-2d-face-recognition" class="headings">3D-aid 2D face recognition</a></h3>
<ul>
<li>
<p>Tom-vs-Pete Classifiers and Identity-Preserving Alignment for Face Verification(ECCV2012, Columbia University)</p>
</li>
<li>
<p>Face Recognition from a Single Training Image under Arbitrary Unknown Lighting Using Spherical Harmonics(PAMI2006)</p>
</li>
<li>
<p>3D-aided face recognition robust to expression and pose variations (CVPR2014)</p>
</li>
<li>
<p>Effective 3D based Frontalization for Unconstrained Face Recognition(ICPR2016, MICC, Florence)</p>
</li>
<li>
<p>Effective Face Frontalization in Unconstrained Images(CVPR2015, TH, Israel)</p>
</li>
<li>
<p>Do We Really Need to Collect Millions of Faces for Effective Face Recognition(ECCV2016, TH, USC, Israel)</p>
</li>
<li>
<p>High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild(CVPR2015)</p>
</li>
<li>
<p>When 3D-Aided 2D Face Recognition Meets Deep Learning: An extended UR2D for Pose-Invariant Face Recognition(2017)</p>
</li>
<li>
<p>Towards Large-Pose Face Frontalization in the Wild</p>
</li>
<li>
<p>Fully Automatic Pose-Invariant Face Recognition via 3D Pose Normalization   (ICCV2011, Cambridge, MA, USA)</p>
</li>
</ul>
<h3 id="3d-face-recognition"><a href="#3d-face-recognition" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:3d-face-recognition" class="headings">3D face recognition</a></h3>
<ul>
<li>
<p>Face Identification across Different Poses and Illuminations with a 3D Morphable Model(Automatic Face and Gesture Recognition2002, VB&amp;TV)</p>
</li>
<li>
<p>Preliminary Face Recognition Grand Challenge Results(2006)</p>
</li>
<li>
<p>expression Invariant 3D Face Recognition with a Morphable Model(FG2008, TV, Basel)</p>
</li>
<li>
<p>Bosphorus Database for 3D Face Analysis(2008)<a href="">data</a></p>
</li>
<li>
<p>Robust Learning from Normals for 3D face recognition(ECCV2012, SZ, ICL)</p>
</li>
<li>
<p>Static and dynamic 3D facial expression recognition: A comprehensive survey(IVC2012, SZ, LijunYin)</p>
</li>
<li>
<p>Deep 3D Face Identification(2017, USC)</p>
</li>
<li>
<p>Robust Face Recognition with Deeply Normalized Depth Images (2018)</p>
<p><code>depth image(front&amp;neural)</code></p>
</li>
<li>
<p>Learning from Millions of 3D Scans for Large-scale 3D Face Recognition(CVPR2018, Western Australia)</p>
</li>
</ul>
            </div>

            
    
    
        <ul class="post-copyright">
            <li class="copyright-item author"><span class="copyright-item-text">作者</span>：<a href="https://www.jiajiewu.top/" class="p-author h-card" target="_blank" rel="noopener">JiaJie</a></li>
            
                
                
                
                
                <li class="copyright-item link"><span class="copyright-item-text">链接</span>：<a href="/contents/face-reconstruction/3d-face-papers/" target="_blank" rel="noopener">https://wjiajie.github.io/contents/face-reconstruction/3d-face-papers/</a></li>
            
            <li class="copyright-item license"><span class="copyright-item-text">许可</span>：<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a></li>
            
        </ul>
    



        </article>

        

        
    <div class="updated-badge-container">
        <span title="Updated @ 2019-12-28 01:46:26 CST" style="cursor:help">

<svg xmlns="http://www.w3.org/2000/svg" width="130" height="20" class="updated-badge"><linearGradient id="b" x2="0" y2="100%"><stop offset="0" stop-color="#bbb" stop-opacity=".1"/><stop offset="1" stop-opacity=".1"/></linearGradient><clipPath id="a"><rect width="130" height="20" rx="3" fill="#fff"/></clipPath><g clip-path="url(#a)"><path class="updated-badge-left" d="M0 0h55v20H0z"/><path class="updated-badge-right" d="M55 0h75v20H55z"/><path fill="url(#b)" d="M0 0h130v20H0z"/></g><g fill="#fff" text-anchor="middle" font-size="110"><text x="285" y="150" fill="#010101" fill-opacity=".3" textLength="450" transform="scale(.1)">updated</text><text x="285" y="140" textLength="450" transform="scale(.1)">updated</text><text x="915" y="150" fill="#010101" fill-opacity=".3" textLength="650" transform="scale(.1)">2019-12-28</text><text x="915" y="140" textLength="650" transform="scale(.1)">2019-12-28</text></g></svg>
        </span></div>



        


        


        
    
    
        <div class="related-posts">
            <h2 class="related-title">相关文章：<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon related-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm144 276c0 6.6-5.4 12-12 12h-92v92c0 6.6-5.4 12-12 12h-56c-6.6 0-12-5.4-12-12v-92h-92c-6.6 0-12-5.4-12-12v-56c0-6.6 5.4-12 12-12h92v-92c0-6.6 5.4-12 12-12h56c6.6 0 12 5.4 12 12v92h92c6.6 0 12 5.4 12 12v56z"/></svg></h2>
            <ul class="related-list">
                
                    <li class="related-item">
                        <a href="/contents/dl/pytorch%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/" class="related-link">pytorch图像分类</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/contents/dl/pytorch%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" class="related-link">pytorch迁移学习</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/contents/dl/pytorch%E7%89%B9%E6%80%A7/" class="related-link">pytorch特性</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/contents/dl/pytorch-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B/" class="related-link">pytorch-保存和加载模型</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/contents/dl/pytorch-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%A4%84%E7%90%86/" class="related-link">pytorch-数据加载和处理</a>
                    </li>
                
            </ul>
        </div>
    



        
    
        <div class="post-tags">
            
                
                
                
                
                    
                    <a href="/tags/dl/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>dl</a>
                
            
                
                
                
                
                    
                    <a href="/tags/face-reconstruction/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>face-reconstruction</a>
                
            
        </div>
    



        


        


        
    
        
        
    
    
    
    



        
    

        

        

        

        
            <div id="utterances"></div>
        

        
    



    </div>
</main>


            
    <div id="back-to-top" class="back-to-top">
        <a href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a>
    </div>


            
    <footer id="footer" class="footer">
        <div class="footer-inner">
            <div class="site-info">2019–2023&nbsp;<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon footer-icon"><path d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"/></svg>&nbsp;JiaJie</div>

            
    
        <ul class="socials"><li class="socials-item">
                    <a href="https://applink.feishu.cn/client/chat/chatter/add_by_link?link_token=4f5pfeb3-46f6-4c97-9572-c4698314d3ac" target="_blank" rel="external noopener" title="飞书[Lark]"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon social-icon"><path d="M301.1 212c4.4 4.4 4.4 11.9 0 16.3l-9.7 9.7c-4.4 4.7-11.9 4.7-16.6 0l-10.5-10.5c-4.4-4.7-4.4-11.9 0-16.6l9.7-9.7c4.4-4.4 11.9-4.4 16.6 0l10.5 10.8zm-30.2-19.7c3-3 3-7.8 0-10.5-2.8-3-7.5-3-10.5 0-2.8 2.8-2.8 7.5 0 10.5 3.1 2.8 7.8 2.8 10.5 0zm-26 5.3c-3 2.8-3 7.5 0 10.2 2.8 3 7.5 3 10.5 0 2.8-2.8 2.8-7.5 0-10.2-3-3-7.7-3-10.5 0zm72.5-13.3c-19.9-14.4-33.8-43.2-11.9-68.1 21.6-24.9 40.7-17.2 59.8.8 11.9 11.3 29.3 24.9 17.2 48.2-12.5 23.5-45.1 33.2-65.1 19.1zm47.7-44.5c-8.9-10-23.3 6.9-15.5 16.1 7.4 9 32.1 2.4 15.5-16.1zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-66.2 42.6c2.5-16.1-20.2-16.6-25.2-25.7-13.6-24.1-27.7-36.8-54.5-30.4 11.6-8 23.5-6.1 23.5-6.1.3-6.4 0-13-9.4-24.9 3.9-12.5.3-22.4.3-22.4 15.5-8.6 26.8-24.4 29.1-43.2 3.6-31-18.8-59.2-49.8-62.8-22.1-2.5-43.7 7.7-54.3 25.7-23.2 40.1 1.4 70.9 22.4 81.4-14.4-1.4-34.3-11.9-40.1-34.3-6.6-25.7 2.8-49.8 8.9-61.4 0 0-4.4-5.8-8-8.9 0 0-13.8 0-24.6 5.3 11.9-15.2 25.2-14.4 25.2-14.4 0-6.4-.6-14.9-3.6-21.6-5.4-11-23.8-12.9-31.7 2.8.1-.2.3-.4.4-.5-5 11.9-1.1 55.9 16.9 87.2-2.5 1.4-9.1 6.1-13 10-21.6 9.7-56.2 60.3-56.2 60.3-28.2 10.8-77.2 50.9-70.6 79.7.3 3 1.4 5.5 3 7.5-2.8 2.2-5.5 5-8.3 8.3-11.9 13.8-5.3 35.2 17.7 24.4 15.8-7.2 29.6-20.2 36.3-30.4 0 0-5.5-5-16.3-4.4 27.7-6.6 34.3-9.4 46.2-9.1 8 3.9 8-34.3 8-34.3 0-14.7-2.2-31-11.1-41.5 12.5 12.2 29.1 32.7 28 60.6-.8 18.3-15.2 23-15.2 23-9.1 16.6-43.2 65.9-30.4 106 0 0-9.7-14.9-10.2-22.1-17.4 19.4-46.5 52.3-24.6 64.5 26.6 14.7 108.8-88.6 126.2-142.3 34.6-20.8 55.4-47.3 63.9-65 22 43.5 95.3 94.5 101.1 59z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://chat.google.com/room/AAAA6JWtXbI?cls=7" target="_blank" rel="external noopener" title="Gmail Spaces"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon social-icon"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://sm.ms/image/8nvJgfqrFkDLObK" target="_blank" rel="external noopener" title="微信[Wechat]"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon social-icon"><path d="M0 224h192V32H0v192zM64 96h64v64H64V96zm192-64v192h192V32H256zm128 128h-64V96h64v64zM0 480h192V288H0v192zm64-128h64v64H64v-64zm352-64h32v128h-96v-32h-32v96h-64V288h96v32h64v-32zm0 160h32v32h-32v-32zm-64 0h32v32h-32v-32z"/></svg></a>
                </li></ul>
    



            

<div class="container" role="main" itemscope itemtype="http://schema.org/Article">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
           
            <article role="main" class="blog-post" itemprop="articleBody" id="content">
              
                
                
                <div class="social-share" data-initialized="true" data-wechat-qrcode-title="扫一扫分享到微信">
    <center>
    <font style="font-size:18px;color:darkcyan;">分享到：</font>
    <a href=" " class="social-share-icon icon-weibo"></a >
    <a href="#" class="social-share-icon icon-wechat"></a >
    <a href="#" class="social-share-icon icon-twitter"></a >
    <a href="#" class="social-share-icon icon-linkedin"></a >
    <a href="#" class="social-share-icon icon-facebook"></a >
    <a href="#" class="social-share-icon icon-qq"></a >
    <a href="#" class="social-share-icon icon-qzone"></a >
    </center>
</div>


<link rel="stylesheet" href="https://wjiajie.github.io/css/share.min.css" />
<script src="https://hugo-picture.oss-cn-beijing.aliyuncs.com/social-share.min.js"></script>

                
                
            </article>



        </div>
    </footer>


        </div>
        

        


    <script>
    if (typeof MathJax === 'undefined') {
        window.MathJax = {
            loader: {
                load: ['[tex]/mhchem']
            },
            
            tex: {
                inlineMath: {'[+]': [['$', '$']]},
                tags: 'ams',
                packages: {'[+]': ['mhchem']}
            }
        };
        (function() {
            const script = document.createElement('script');
            script.src = 'https:\/\/cdn.jsdelivr.net\/npm\/mathjax@3.1.2\/es5\/tex-mml-chtml.js';
            script.defer = true;
            document.head.appendChild(script);
        })();
    } else {
        MathJax.texReset();
        MathJax.typeset();
    }
</script>






    

        

        

        
            <script>
    function loadComments() {
        (function() {
            const utterances = document.getElementById("utterances");
            if (!utterances) {
                return;
            }
            const script = document.createElement('script');
            script.src = 'https:\/\/utteranc.es\/client.js';
            script.async = true;
            script.crossOrigin = 'anonymous';
            script.setAttribute('repo', 'Wjiajie\/Wjiajie.github.io');
            script.setAttribute('issue-term', 'pathname');
            const isDark = getCurrentTheme() === 'dark';
        if (isDark) {
            script.setAttribute('theme', 'photon-dark');
        } else {
            script.setAttribute('theme', 'github-light');
        }
            
            utterances.appendChild(script);
        })();
    }
</script>
        

        

    



    <script src="/js/medium-zoom.min.js"></script>

<script>
    mediumZoom(document.querySelectorAll('div.post-body img'), {
        background: 'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'
    })
</script>




    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module" defer></script>






    </body>
</html>
