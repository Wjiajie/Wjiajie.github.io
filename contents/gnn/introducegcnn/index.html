<!DOCTYPE html>
<html lang="zh-CN">
    <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
    <meta charset="UTF-8" />

    <meta name="generator" content="Hugo 0.109.0"><meta name="theme-color" content="#fff" />
    <meta name="color-scheme" content="light dark">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <meta name="format-detection" content="telephone=no, date=no, address=no, email=no" />
    
    <meta http-equiv="Cache-Control" content="no-transform" />
    
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <title>图卷积神经网络总结 | &gt;W&lt;</title>

    <link rel="stylesheet" href="/css/meme.min.f694ecb248990c1307435cb6aa04a8e886bb1e065218bdd0b4f293ed3e681f23.css"/>

    
    
        <script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js" defer></script><script src="/js/meme.min.d8d173e627fe32e2218852ae80f101216ff3cd58276e147d6f46c5dd245f9040.js"></script>

    

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />

        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM&#43;Plex&#43;Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;family=Comfortaa:wght@700&amp;display=swap" media="print" onload="this.media='all'" />
        <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM&#43;Plex&#43;Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;family=Comfortaa:wght@700&amp;display=swap" /></noscript>

    <meta name="author" content="JiaJie" /><meta name="description" content="本文中一些重要的符号定义 " />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2a6df4" />
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-title" content="&gt;W&lt;" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="mobile-web-app-capable" content="yes" />
    <meta name="application-name" content="&gt;W&lt;" />
    <meta name="msapplication-starturl" content="../../../" />
    <meta name="msapplication-TileColor" content="#fff" />
    <meta name="msapplication-TileImage" content="../../../icons/mstile-150x150.png" />
    <link rel="manifest" href="/manifest.json" />

    
    

    
    <link rel="canonical" href="https://wjiajie.github.io/contents/gnn/introducegcnn/" />
    

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "datePublished": "2020-07-13T21:27:52+08:00",
        "dateModified": "2021-06-11T05:03:46+08:00",
        "url": "https://wjiajie.github.io/contents/gnn/introducegcnn/",
        "headline": "图卷积神经网络总结",
        "description": "本文中一些重要的符号定义 ",
        "inLanguage" : "zh-CN",
        "articleSection": "contents",
        "wordCount":  5713 ,
        "image": ["https://i.loli.net/2020/07/14/r5v1FMmBAblk9aD.png","https://i.loli.net/2020/07/14/TpUO7W8C6NDkLJi.png","https://i.loli.net/2020/07/14/6zI29DStZdcVTby.png","https://i.loli.net/2020/07/14/w8sflKjp6F2VS3C.png","https://i.loli.net/2020/07/14/iPsZ7cKtr4RbBdS.png","https://i.loli.net/2020/07/14/RKHgI5aWkGqMf7l.png","https://i.loli.net/2020/07/14/klHXhVj7EtGZwOe.png","https://i.loli.net/2020/07/15/TxsXRNn27bMGPAW.png","https://i.loli.net/2020/07/15/6BjUC3VTqzuFoKd.png","https://i.loli.net/2020/07/15/Igw4JKrzajEVe1u.png","https://i.loli.net/2020/07/15/aDihLckAWG3n5PR.png","https://i.loli.net/2020/07/15/jGtYhxEK1BNviPX.png","https://i.loli.net/2020/08/18/uE6RSM5XFf2cbxU.png","https://i.loli.net/2020/08/18/BS9FH6sRcWL5lM4.png","https://i.loli.net/2020/08/18/ND2wPd7QKW5YLVb.png"],
        "author": {
            "@type": "Person",
            "description": "静心得意",
            "email": "3208920286@qq.com",
            "image": "https://s2.loli.net/2023/02/25/bTD9PrGNyC8kRi5.png",
            "url": "https://wjiajie.github.io/",
            "name": "JiaJie"
        },
        "license": "[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)",
        "publisher": {
            "@type": "Organization",
            "name": "\u003eW\u003c",
            "logo": {
                "@type": "ImageObject",
                "url": "https://wjiajie.github.io/icons/apple-touch-icon.png"
            },
            "url": "https://wjiajie.github.io/"
        },
        "mainEntityOfPage": {
            "@type": "WebSite",
            "@id": "https://wjiajie.github.io/"
        }
    }
</script>

    

<meta name="twitter:card" content="summary_large_image" />



    



<meta property="og:title" content="图卷积神经网络总结" />
<meta property="og:description" content="本文中一些重要的符号定义 " />
<meta property="og:url" content="https://wjiajie.github.io/contents/gnn/introducegcnn/" />
<meta property="og:site_name" content="&gt;W&lt;" />
<meta property="og:locale" content="zh-cn" /><meta property="og:image" content="https://i.loli.net/2020/07/14/r5v1FMmBAblk9aD.png" />
<meta property="og:type" content="article" />
    <meta property="article:published_time" content="2020-07-13T21:27:52&#43;08:00" />
    <meta property="article:modified_time" content="2021-06-11T05:03:46&#43;08:00" />
    
    <meta property="article:section" content="contents" />


        <link rel="preconnect" href="https://www.google-analytics.com" crossorigin />

        


    
    <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', '');
    </script>




    
    

    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css">
<script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>



<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@700&amp;text=reuixiy&amp;display=swap" media="print" onload="this.media='all'" />
<noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@700&amp;text=reuixiy&amp;display=swap" /></noscript>





</head>

    <body>
        <div class="container">
            
    <header class="header">
        
            <div class="header-wrapper">
                <div class="header-inner single">
                    
    <div class="site-brand">
        
            <a href="/" class="brand">&gt;W&lt;</a>
        
    </div>

                    <nav class="nav">
    <ul class="menu" id="menu">
        
            
        
        
        
        
            
                <li class="menu-item"><a href="/tags/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon folder"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg><span class="menu-item-name">建筑群</span></a>
                </li>
            
        
            
                
                    
                    
                        <li class="menu-item">
                            <a id="theme-switcher" href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5l48.8-97.5a18 18 0 0128 0l48.8 97.5 103.4 -34.5a18 18 0 0119.8 19.8l-34.5 103.4l97.5 48.8a18 18 0 010 28l-97.5 48.8 34.5 103.4a18 18 0 01-19.8 19.8l-103.4-34.5-48.8 97.5a18 18 0 01-28 0l-48.8-97.5l-103.4 34.5a18 18 0 01-19.8-19.8l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8-34.5-103.4a18 18 0 0119.8-19.8zM256 128a128 128 0 10.01 0M256 160a96 96 0 10.01 0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412a256 256 0 10154-407a11.5 11.5 0 00-5 20a201.5 201.5 0 01-134 374a11.5 11.5 0 00-15 13"/></svg></a>
                        </li>
                    
                
            
        
            
                
            
        
            
                <li class="menu-item search-item">
                        <form id="search" class="search" role="search">
    <label for="search-input"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon search-icon"><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></label>
    <input type="search" id="search-input" class="search-input">
</form>

<template id="search-result" hidden>
    <article class="content post">
        <h2 class="post-title"><a class="summary-title-link"></a></h2>
        <summary class="summary"></summary>
        <div class="read-more-container">
            <a class="read-more-link"> »</a>
        </div>
    </article>
</template>

                    </li>
                
            
        
    </ul>
</nav>

                    
                </div>
            </div>
            
    <input type="checkbox" id="nav-toggle" aria-hidden="true" />
    <label for="nav-toggle" class="nav-toggle"></label>
    <label for="nav-toggle" class="nav-curtain"></label>


        
    </header>




            
            
    <main class="main single" id="main">
    <div class="main-inner">

        

        <article class="content post h-entry" data-small-caps="true" data-align="justify" data-type="contents" data-toc-num="true">

            <h1 class="post-title p-name">图卷积神经网络总结</h1>

            

            

            
                

<div class="post-meta">
    
        
        <time datetime="2020-07-13T21:27:52&#43;08:00" class="post-meta-item published dt-published"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M148 288h-40c-6.6 0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h48c26.5 0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"/></svg>&nbsp;2020.7.13</time>
    
    
        
        <time datetime="2021-06-11T05:03:46&#43;08:00" class="post-meta-item modified dt-updated"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M400 64h-48V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H160V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H48C21.49 64 0 85.49 0 112v352c0 26.51 21.49 48 48 48h352c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm-6 400H54a6 6 0 0 1-6-6V160h352v298a6 6 0 0 1-6 6zm-52.849-200.65L198.842 404.519c-4.705 4.667-12.303 4.637-16.971-.068l-75.091-75.699c-4.667-4.705-4.637-12.303.068-16.971l22.719-22.536c4.705-4.667 12.303-4.637 16.97.069l44.104 44.461 111.072-110.181c4.705-4.667 12.303-4.637 16.971.068l22.536 22.718c4.667 4.705 4.636 12.303-.069 16.97z"/></svg>&nbsp;2021.6.11</time>
    
    
    
        
        
            
            
                
                
                
                
                    
                    
                    
                        
                            
                            
                        
                    
                
            
            
            
                <span class="post-meta-item category"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>&nbsp;<a href="/contents/gnn/" class="category-link p-category">contents\GNN\</a></span>
            
        
        
    
    
        
        <span class="post-meta-item wordcount"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;5713</span>
    
    
        
        <span class="post-meta-item reading-time"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;12&nbsp;</span>
    
    
        
            
            <span class="post-meta-item busuanzi-page-pv" id="busuanzi_container_page_pv"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" class="icon post-meta-icon"><path d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"/></svg>&nbsp;<span id="busuanzi_value_page_pv"></span></span>
        
    
    
</div>

            

            <nav class="contents">
  <h2 id="contents" class="contents-title"></h2><ol class="toc">
    <li><a id="contents:图卷积神经网络的提出" href="#图卷积神经网络的提出">图卷积神经网络的提出</a></li>
    <li><a id="contents:谱域图卷积与空域图卷积的联系" href="#谱域图卷积与空域图卷积的联系">谱域图卷积与空域图卷积的联系</a></li>
    <li><a id="contents:卷积算子" href="#卷积算子">卷积算子</a>
      <ol>
        <li><a id="contents:谱域图卷积" href="#谱域图卷积">谱域图卷积</a>
          <ol>
            <li><a id="contents:基本概念" href="#基本概念">基本概念</a></li>
            <li><a id="contents:谱域图卷积的不足之处" href="#谱域图卷积的不足之处">谱域图卷积的不足之处</a></li>
          </ol>
        </li>
        <li><a id="contents:空域图卷积" href="#空域图卷积">空域图卷积</a>
          <ol>
            <li><a id="contents:空域图卷积的定义" href="#空域图卷积的定义">空域图卷积的定义</a></li>
            <li><a id="contents:空域图卷积的几个模型介绍" href="#空域图卷积的几个模型介绍">空域图卷积的几个模型介绍</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a id="contents:池化算子" href="#池化算子">池化算子</a>
      <ol>
        <li><a id="contents:层次化池化机制的几种实现思路" href="#层次化池化机制的几种实现思路">层次化池化机制的几种实现思路</a></li>
      </ol>
    </li>
    <li><a id="contents:图卷积神经网络的过平滑现象以及缓解方案" href="#图卷积神经网络的过平滑现象以及缓解方案">图卷积神经网络的过平滑现象以及缓解方案</a>
      <ol>
        <li><a id="contents:gcn的过平滑现象" href="#gcn的过平滑现象">GCN的过平滑现象</a></li>
        <li><a id="contents:过平滑现象的缓解方案" href="#过平滑现象的缓解方案">过平滑现象的缓解方案</a></li>
      </ol>
    </li>
    <li><a id="contents:图卷积神经网络的任务分类" href="#图卷积神经网络的任务分类">图卷积神经网络的任务分类</a></li>
    <li><a id="contents:图卷积神经网络在计算机视觉中的应用" href="#图卷积神经网络在计算机视觉中的应用">图卷积神经网络在计算机视觉中的应用</a></li>
    <li><a id="contents:参考文献" href="#参考文献">参考文献</a></li>
  </ol>
</nav><div class="post-body e-content">
                <p><code>本文中一些重要的符号定义</code>
<img src="https://i.loli.net/2020/07/14/r5v1FMmBAblk9aD.png"  width = "100%" height = "60%" div align = center /></p>
<h2 id="图卷积神经网络的提出"><a href="#图卷积神经网络的提出" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:图卷积神经网络的提出" class="headings">图卷积神经网络的提出</a></h2>
<p>传统的卷积神经网络只能处理欧氏空间数据 (如图像 、文本和语音 )。以图像数据为例，图像数据规则的栅格形式允许我们在数据空间定义具有全局共享的卷积核，这类型的卷积核和传统的池化操作使得图像数据具有平移不变形。基于此 ,卷积神经网络通过学习在每个像素点共享的卷积核来建模局部连接，进而为图片学习到意义丰富的隐层表示。</p>
<img src="https://i.loli.net/2020/07/14/TpUO7W8C6NDkLJi.png"  width = "100%" height = "60%" div align = center />
<img src="https://i.loli.net/2020/07/14/6zI29DStZdcVTby.png"  width = "100%" height = "60%" div align = center />
<p>但对于非欧数据，传统的卷积操作难以应用到上面。比如图数据和流形数据，这两类数据有个特点就是，排列不整齐，比较的随意。具体体现在：对于数据中的某个点，难以定义出其邻居节点出来，或者是不同节点的邻居节点的数量是不同的，这个其实是一个特别麻烦的问题，因为这样就意味着难以在这类型的数据上定义出和图像等数据上相同的卷积操作出来。</p>
<p>图卷积神经网络关注如何在图上构造深度学习模型，图卷积分为谱域图卷积和空域图卷积，谱域图卷积根据图谱理论和卷积定理,将数据由空域转换到谱域做处理。空域图卷积不依靠图谱卷积理论,直接在空间上定义卷积操作。以这两种卷积算子实现的模型称为谱域图卷积模型与空域图卷积模型。基于卷积定理的图卷积神经网络称为谱域图卷积神经网络，基于设计聚合函数来聚合图上每个中心节点和其邻近节点特征的神经网络称为空域图卷积神经网络。</p>
<p>有关图卷积神经网络的研究，自2019年以来顶会接收的相关文章呈现爆炸性的增长，详情见<a href="https://github.com/naganandy/graph-based-deep-learning-literature/tree/master/conference-publications" target="_blank" rel="noopener">此github仓库</a>。</p>
<h2 id="谱域图卷积与空域图卷积的联系"><a href="#谱域图卷积与空域图卷积的联系" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:谱域图卷积与空域图卷积的联系" class="headings">谱域图卷积与空域图卷积的联系</a></h2>
<p>谱域图卷积有比较坚实的理论基础，相较于谱域图卷积，空域图卷积的定义直观，灵活性更高。关于他们的联系，这里先直接摆出结论，便于对谱域图卷积的定义形式到空域图卷积的定义形式的演变有一个整体的把握。</p>
<ul>
<li>谱域图卷积的形式表示为：</li>
</ul>
<p>$$
x_{G}^{*} y=\boldsymbol{U}\left(\left(\boldsymbol{U}^{\mathrm{T}} x\right) \odot\left(\boldsymbol{U}^{\mathrm{T}} y\right)\right)
$$</p>
<p>其中$_G^*$表示图卷积算子，$x,y$ 表示图上节点域的信号，$\odot$ 表示哈达玛乘法，则两个向量的对应元素相乘。</p>
<ul>
<li>空域图卷积可以看做特殊形式的一阶切比雪夫卷积:</li>
</ul>
<p>$$
\mathbf{Y}=\hat{\mathbf{A}} \mathbf{X} \mathbf{W}, \quad \hat{\mathbf{A}} \in R^{N \times N}, \mathbf{X} \in R^{N \times C}, \mathbf{W} \in R^{C \times D}
$$</p>
<p>其中 $\hat{\mathbf{A}} =\tilde{\mathbf{D}}^{-1 / 2} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-1 / 2}$, $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$,$\mathbf{A}$指的是邻接矩阵，$\mathbf{I}$指的是单位阵，$ \tilde{\boldsymbol{D}}_{i i}=\sum_{j} \tilde{\boldsymbol{A}}_{i, j} $,$\hat{\mathbf{A}}$是对$\tilde{\mathbf{A}}$的归一化操作，将它的特征值范围约束到0和1之间。</p>
<p>谱域图卷积到空域图卷积的演变过程将在下文陈述。</p>
<h2 id="卷积算子"><a href="#卷积算子" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:卷积算子" class="headings">卷积算子</a></h2>
<p>在信号与处理邻域，由卷积定理，两信号在空域(或者时域)的卷积的傅里叶变换等于这两信号在
谱域（频域）中的傅里叶变换的乘积，即：</p>
<p>$$
\mathcal{F}\left[f_{1}(t) \star f_{2}(t)\right]=F_{1}(w) \cdot F_{2}(w)
$$</p>
<p>此公式等价为：</p>
<p>$$
f_{1}(t) \star f_{2}(t)=\mathcal{F}^{-1}\left[F_{1}(w) \cdot F_{2}(w)\right]
$$</p>
<p>上面公式可以理解为： 两信号在空域的卷积等价于：先将两信号转到
谱域中进行相乘操作，再将相乘的结果转换到空域。</p>
<p>连续信号的傅里叶变换和反变换：</p>
<p>$$
F(w)=\mathcal{F}[f(t)]=\int_{\mathbb{R}} f(t) e^{-2 \pi i w t} d t
$$</p>
<p>$$
f(t)=\mathcal{F}^{-1}[F(w)]=\int_{\mathbb{R}} F(t) e^{2 \pi i w t} d t
$$</p>
<p>离散信号的傅里叶变换和反变换：</p>
<p>$$
F(w)=\sum_{t=1}^{n} f(t) e^{-i \frac{2 \pi}{n} w t}
$$</p>
<p>$$
f(t)=\frac{1}{n} \sum_{w=1}^{n} F(w) e^{i \frac{2 \pi}{n} w t}
$$</p>
<p>不管是连续形式还是离散形式，有空域转为谱域的关键是找到一组正交基，对于$n$维空间，即找到$n$组线性无关的向量。</p>
<p>经典的傅里叶变换有如下规律：傅里叶变换的基函数是拉普拉斯算子的本征函数。严格的数学证明见引用<a href="#refer-anchor-1"><sup>1</sup></a>。拉普拉斯算子定义为梯度的散度：</p>
<p>$$
\Delta f=\nabla \cdot(\nabla f)=\operatorname{div}(\operatorname{grad}(f))
$$</p>
<p>在$n$维欧几里得空间，可以认为拉普拉斯算子是一个二阶微分算子，即在各个维度求二阶导数后求和。</p>
<p>对于离散情况，譬如对于二维图像，两个变量的离散拉普拉斯算子可以写成：</p>
<p>$$
\frac{\partial f}{\partial x}=f^{\prime}(x)=f(x+1)-f(x)
$$</p>
<p>$$
\frac{\partial^{2} f}{\partial x^{2}}=f^{\prime \prime}(x)=f^{\prime}(x+1)-f^{\prime}(x)=f(x+1)+f(x-1)-2 f(x)
$$</p>
<p>$$
\Delta f=\frac{\partial^{2} f}{\partial x^{2}}+\frac{\partial^{2} f}{\partial y^{2}}=f(x+1, y)+f(x-1, y)+f(x, y-1)+f(x, y+1)-4 f(x, y)
$$</p>
<p>将上面公式的每一个正项和负项结合，可以理解为欧氏空间内,二维的拉普拉斯算子是中心节点与周围节点的差值,然后求和。</p>
<p>将该思想拓展到图(graph)上，可以定义图上的拉普拉斯算子：</p>
<p>$$
\Delta f_{i}=\sum_{(i, j) \in \mathcal{E}}\left(f_{i}-f_{j}\right)
$$</p>
<p>其中 $\quad f=\left(f_{1}, f_{2}, \cdots f_{n}\right),$ 代表n个节点上每个节点的信号。</p>
<h3 id="谱域图卷积"><a href="#谱域图卷积" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:谱域图卷积" class="headings">谱域图卷积</a></h3>
<p>要定义图上的拉普拉斯算子，有必要先把图的几个基本概念阐述清楚：</p>
<h4 id="基本概念"><a href="#基本概念" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:基本概念" class="headings">基本概念</a></h4>
<ul>
<li>下文中指的图都为无向图</li>
</ul>
<p>$$
\quad \mathcal{G}=(\mathcal{V}, \mathcal{E}, A) \quad|\mathcal{V}|=n
$$</p>
<ul>
<li>
<p>邻接矩阵表示顶点与顶点之间的连接关系4
$$
A \in \mathbb{R}^{n \times n}
$$</p>
</li>
<li>
<p>度矩阵</p>
</li>
</ul>
<p>$$
\quad D \in \mathbb{R}^{n \times n} \quad D_{i i}=\sum_{j} W_{i j}
$$</p>
<p>$D_{ii}$ 表示以顶点 $v_i$为端点的边的数目。</p>
<p>图上定义的拉普拉斯矩阵：</p>
<p>$$
L = D - A
$$</p>
<p>上述概念的一个直观的例子：
<img src="https://i.loli.net/2020/07/14/w8sflKjp6F2VS3C.png"  width = "100%" height = "60%" div align = center /></p>
<p>论文<a href="#refer-anchor-2"><sup>2</sup></a>中提出，拉普拉斯矩阵是定义在图上的一种拉普拉斯算子，前面说过傅里叶变换的基函数是拉普拉斯算子的本征函数，类似的,图傅里叶变换的基函数即为拉普拉斯矩阵的特征向量。通过对 $L$的矩阵特征分解，我们可以为图傅里叶变换找到一组正交基。</p>
<p><strong>拉普拉斯矩阵由图的邻接矩阵和度决定，给定一个图，拉普拉斯矩阵是确定的，换句话来说，如果一个图的拓扑形式发生改变，需要重新计算拉普拉斯矩阵。</strong></p>
<p>可以证明，拉普拉斯矩阵$L$是对称半正定矩阵,，对于一个$n$维向量$f$:</p>
<p>$$
f^{T} L f=f^{T} D f-f^{T} W f=\sum_{i=1}^{n} d_{i} f_{i}^{2}-\sum_{i, j=1}^{n} f_{i} f_{j} W_{i j}
$$</p>
<p>$$
=\frac{1}{2}\left(\sum_{i=1}^{n} d_{i} f_{i}^{2}-2 \sum_{i, j=1}^{n} f_{i} f_{j} W_{i j}+\sum_{j=1}^{n} d_{j} f_{j}^{2}\right)
$$</p>
<p>$$
=\frac{1}{2}\left(\sum_{i, j=1}^{n} W_{i j}\left(f_{i}-f_{j}\right)^{2}\right) \geq 0
$$</p>
<p>作为对称半正定矩阵,拉普拉斯矩阵有如下性质:</p>
<ul>
<li>
<p>$n$阶对称矩阵一定有$n$个线性无关的特征向量。</p>
</li>
<li>
<p>对称矩阵的不同特征值对应的特征向量相互正交,这些正交的特征向量构成的矩阵为正交矩阵。</p>
</li>
<li>
<p>实对称矩阵的特征向量一定是实向量</p>
</li>
<li>
<p>半正定矩阵的特征值一定非负。</p>
</li>
</ul>
<p>对拉普拉斯矩阵特征分解：</p>
<p>$$
L=U \Lambda U^{-1}
$$</p>
<p>$$
U=\left(\vec{u}_{1}, \vec{u}_{2}, \cdots, \vec{u}_{n}\right)
$$</p>
<p>$$
\vec{u}_{i} \in \mathbb{R}^{n}, i=1,2, \cdots n
$$</p>
<p>需要说明的是，拉普拉斯矩阵的特征向量组成的矩阵$U$不仅是正交矩阵，它还是标准正交矩阵。这样，通过计算$U=\left(\vec{u}_{1}, \vec{u}_{2}, \cdots, \vec{u}_{n}\right)$,我们可以使用拉普拉斯的特征向量作为基函数,任意的图上的信号可以表示为:</p>
<p>$$
x=\hat{x}\left(\lambda_{1}\right) \vec{u}_{1}+\hat{x}\left(\lambda_{2}\right) \vec{u}_{2}+\cdots+\hat{x}\left(\lambda_{n}\right) \vec{u}_{n}
$$</p>
<p>转为矩阵形式，图傅里叶反变换定义如下：
<img src="https://i.loli.net/2020/07/14/iPsZ7cKtr4RbBdS.png"  width = "100%" height = "60%" div align = center /></p>
<p>图傅里叶正变换定义如下：</p>
<img src="https://i.loli.net/2020/07/14/RKHgI5aWkGqMf7l.png"  width = "100%" height = "60%" div align = center />
<p>图傅里叶正变换将图上任意信号与每个正交基做内积求出各项的傅里叶系数，图傅里叶反变换是说，图上任意信号可以由一组正交基的线性组合来表示。
<img src="https://i.loli.net/2020/07/14/klHXhVj7EtGZwOe.png"  width = "100%" height = "60%" div align = center /></p>
<p>由此得出谱域图卷积的定义：</p>
<p>$$
x_{G}^{*} g=\boldsymbol{U}\left(\left(\boldsymbol{U}^{\mathrm{T}} x\right) \odot\left(\boldsymbol{U}^{\mathrm{T}} g\right)\right)
$$</p>
<p>定义 $g_{\theta} = diag(\boldsymbol{U}^Tg) $为谱域上的卷积算子，上述公式还可以表示为：</p>
<p>$$
x ^\star_{G} g_{\theta}=\boldsymbol{U}g_{\theta}\boldsymbol{U}^{\mathrm{T}} x
$$</p>
<p>$g_{\theta}$是一个$n$维的对角阵。</p>
<p>有不少关于谱域卷积的工作，<em>SCNN</em> 用可学习的对角矩阵来代替谱域的卷积核；<em>ChebNet</em> 采用切比雪夫多项式来代替谱域的卷积核；<em>GCN</em> 仅考虑1阶切比雪夫多项式，且每个卷积核仅只有一个参数。</p>
<h4 id="谱域图卷积的不足之处"><a href="#谱域图卷积的不足之处" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:谱域图卷积的不足之处" class="headings">谱域图卷积的不足之处</a></h4>
<p>上面的公式在图数据的应用上存在一定的局限性：</p>
<ol>
<li>参与卷积的图上信号$x$是全图的所有节点的信号（$x \in \mathbb{R}^{n \times c}$）,它不是一个局部链接的结构。</li>
<li>由于矩阵$U$是由拉普拉斯矩阵$L$特征分解而来，$L$是一个对称半正定矩阵，这要求处理的图解钩是无向图。</li>
<li>由于进行谱域图卷积需要事先确定$L$，在模型训练过程中，图结构不能变化，而在某些场景下，图结构是可能发生变化的（社交网络数据，交通数据等。</li>
</ol>
<h3 id="空域图卷积"><a href="#空域图卷积" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:空域图卷积" class="headings">空域图卷积</a></h3>
<h4 id="空域图卷积的定义"><a href="#空域图卷积的定义" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:空域图卷积的定义" class="headings">空域图卷积的定义</a></h4>
<p>前面提到，在</p>
<p>$$
x ^\star_{G} g_{\theta}=\boldsymbol{U}g_{\theta}\boldsymbol{U}^{\mathrm{T}} x
$$</p>
<p>中，$g_{\theta}$是一个$n$维的对角阵。一个谱域图卷积网络 <strong>ChebyNet</strong><a href="#refer-anchor-3"><sup>3</sup></a>对$g_{\theta}$参数化：</p>
<p>$$
\boldsymbol{g}_{\theta}=\sum_{i=0}^{K-1} \theta_{k} T_{k}(\hat{\Lambda})
$$
其中 $ \theta_{k}$ 是需要学的系数 ,$\hat{\Lambda}=\frac{2 \Lambda}{\lambda_{\max }}-\mathbf{I}_{n} $。 切比雪夫多项式是通过递归得到，递归表达式为</p>
<p>$$
T_{k}(x)=2 x T_{k-1}(x)-T_{k-2}(x)
$$</p>
<p>其中,</p>
<p>$$
T_{0}(x)=1, T_{1}(x)=x
$$</p>
<p>这样，有：</p>
<p>$$
x ^\star_{G} g_{\theta} =U g_{\theta} U^{T} x =U \sum_{k=0}^{K} \beta_{k} T_{k}(\hat{\Lambda}) U^{T}x
$$</p>
<p>$$
= \sum_{k=0}^{K} \beta_{k} T_{k}(U \hat{\Lambda} U^{T}) x =\sum_{k=0}^{K} \beta_{k} T_{k}(\hat{L}) x
$$</p>
<p>其中$\hat{L}=\frac{2}{\lambda_{\max }}\tilde L-I_{n}$, $\lambda_{\max } = 2$,$\tilde L$表示归一化的拉普拉斯矩阵： $\tilde L = I_n - D^{-1/2}AD^{-1/2}$,故$\hat{L}$的表达式最终化简为：</p>
<p>$$
\hat{L} = - D^{-1/2}AD^{-1/2}
$$</p>
<p>当仅考虑一阶切比雪夫多项式，即$K = 1$时，</p>
<p>$$
x \star_{G} g_{\theta} =\sum_{k=0}^{K} \beta_{k} T_{k}(\hat{L}) x=\sum_{k=0}^{1} \beta_{k} T_{k}(\hat{L}) x
$$</p>
<p>$$
=\beta_{0} T_{0}(\hat{L}) x+\beta_{1} T_{1}(\hat{L}) x =\left(\beta_{0}+\beta_{1} \hat{L}\right) x
$$</p>
<p>令 $\beta_{1} = -\beta_{0}$，</p>
<p>故：</p>
<p>$$
x \star_{G} g_{\theta} = \left(\beta_{0}+\beta_{1} \hat{L}\right) x  = \beta_{0}\left( I_n + D^{-1/2}AD^{-1/2}\right)x
$$</p>
<p>由于$I_n + D^{-1/2}AD^{-1/2}$具有[0,2]的特征值，如果在深度神经网络模型中使
用该算子,则反复应用该算子会导致数值不稳定(发散)和梯度爆炸/消失,
为了解决该问题, 引入了一个 <strong>renormalization trick</strong>:</p>
<p>$$
I_{n}+D^{-1 / 2} A D^{-1 / 2} \rightarrow \tilde{D}^{-1 / 2} \tilde{A} \tilde{D}^{-1 / 2}
$$</p>
<p>$$
\tilde{A}=A+I_{n} \quad \tilde{D}_{i i}=\sum \tilde{A}_{i j}
$$</p>
<p>这其实是谱域<code>GCN</code>（图卷积网络）参数化的卷积函数，但从空域的角度来看，一阶图卷积网络将$\tilde{D}^{-1 / 2} \tilde{A} \tilde{D}^{-1 / 2}$当做聚合函数，令$\hat{\mathbf{A}} =\tilde{\mathbf{D}}^{-1 / 2} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-1 / 2}$，有：</p>
<p>$$
x \star_{G} g_{\theta}  = \beta_{0}\hat{\mathbf{A}}x
$$</p>
<p>从空域的理解上面的式子，它的物理意义是：</p>
<ul>
<li>为图上节点添加自连接边</li>
<li>局部空间信息融合</li>
<li>归一化</li>
</ul>
<p>这里的 $\beta_{0}$是一个尺度因子，在神经网络模型中会被归一化操作替代，因此不妨设$\beta_{0} = 1$，为了加强网络的拟合能力，有研究引入一个参数化的权重矩阵$W$对输入的图信号矩阵$x$做仿射变换，于是有：</p>
<p>$$
\mathbf{Y} = x \star_{G} g_{\theta}  = \hat{\mathbf{A}}x \mathbf{W}
$$</p>
<p>$$
\hat{\mathbf{A}} \in R^{N \times N}, x \in R^{N \times C}, \mathbf{W} \in R^{C \times D}
$$</p>
<p>为和文章开始引入的公式对齐，令 $x = \mathbf{X}$,故单层的空域上的图卷积层可以定义为：</p>
<p>$$
\mathbf X&rsquo; = \sigma \left(\hat{\mathbf{A}}\mathbf{X} \mathbf{W}\right)
$$</p>
<p>可以看到空间域图卷积实际上可以由一阶切比雪夫卷积导出，它也可以看做<code>GCN</code>在空域视角的表达形式。</p>
<h4 id="空域图卷积的几个模型介绍"><a href="#空域图卷积的几个模型介绍" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:空域图卷积的几个模型介绍" class="headings">空域图卷积的几个模型介绍</a></h4>
<p>作为深度学习域图数据结合的代表性方法，<code>GCN</code>的出现带动了将神经网络技术运用到图数据的学习任务中去的一大类方法。从空域角度看<code>GCN</code>，本质上是一个迭代式地聚合邻居的过程，这启发了一大类模型对于这种聚类操作的重新设计，以加强模型对于图数据的适应性，下面将介绍几个空域图卷积的模型。</p>
<hr>
<ul>
<li><strong>GAT</strong></li>
</ul>
<p>GAT<a href="#refer-anchor-4"><sup>4</sup></a>即 GRAPH ATTENTION NETWORKS,其核心思想为将attention引入到图卷积模型中。图注意力网络利用注意力机制定义聚合函数，与以往关心边上信息的模型不同，在图注意力网络中，邻接矩阵仅被用来定义相关节点，而关联权重的计算则是依赖于节点的特征表达。图注意力网络的每一层结构如下图所示：
<img src="https://i.loli.net/2020/07/15/TxsXRNn27bMGPAW.png"  width = "100%" height = "60%" div align = center /></p>
<p><strong>1.使用注意力机制计算节点之间的关联度</strong></p>
<p>$$
e_{i j}=\alpha\left(\mathbf{W} \vec{h}_{i}, \mathbf{W} \vec{h}_{j}\right)=\vec{a}^{T}\left[\mathbf{W} \vec{h}_{i}|| \mathbf{W} \vec{h}_{j}\right]
$$</p>
<p>其中 $\vec{h}_{i} \in \mathbb{R}^{F}$ 表示i节点的特征, F表示节点特征 的通道数。$W \in \mathbb{R}^{F \times F^{\prime}}$ 是可学习的线性变换参数。
$\alpha(\cdot): \mathbb{R}^{F^{\prime}} \times \mathbb{R}^{F^{\prime}} \rightarrow \mathbb{R}$ 表示注意力机制, 其输出 $e_{i j}$ 为注意力系数。
在实验中 $\alpha(\cdot)$ 的实现是一个单层前馈神经网络。 ||表示拼接。 $\vec{a}^{T} \in \mathbb{R}^{2 F^{\prime}}$ 为可学习参数。</p>
<p>用<em>softmax</em> 归一化注意力系数：
$$
\alpha_{i j}=\operatorname{softmax}_{j}\left(e_{i j}\right)=\frac{\exp \left(e_{i j}\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(e_{i k}\right)}
$$
<img src="https://i.loli.net/2020/07/15/6BjUC3VTqzuFoKd.png" width = "100%" height = "60%" div align = center /></p>
<p>若将$\alpha_{i j}$和$\mathbf{W} $结合起来看做一个系数,实际上GAT对邻域的每个节点隐式地(implicitly)分配了不同的卷积核参数。</p>
<p>从图注意力网络开始 ,节点之间的权重计算开始从依赖于网络的结构信息转移到依赖于节点的
特征表达 ,但是以上模型在处理时需要加载整个网络的节点特征 ,这阻碍了模型在大规模网络上的应用。</p>
<hr>
<ul>
<li><strong>GraphSAGE</strong></li>
</ul>
<p>GraphSAGE<a href="#refer-anchor-5"><sup>5</sup></a>不同于以往模型考虑所有邻近节点 ,图采样聚合网络对邻近节点做随机采样 ,使得每个节点的邻近节点都小于给定的采样个数.</p>
<p>GraphSAGE认为卷积可以理解为采样和信息聚合两个步骤的结合，邻域的节点不需要进行排序，设计的聚合函数需要与输入顺序无关。</p>
<p>GraphSAGE 的聚合函数有多种形式，分别是基于最大值的聚合，基于均值的聚合和长短时记忆力网络(LSTM)。最大值聚合和均值聚合分别指取相关节点的最大值和均值作为聚合结果。长短时记忆网络指将相关节点输入LSTM，并把输出作为聚合结果。
<img src="https://i.loli.net/2020/07/15/Igw4JKrzajEVe1u.png" width = "100%" height = "60%" div align = center />
图采样聚合网络提出用分批量处理数据的方法训练模型 ,在每个批量输入数据下只需要加载对应节点的局部结构 ,避免了整张网络的加载，这使得在大规模数据集上搭建图卷积神经网络成为可能。</p>
<hr>
<ul>
<li><strong>PGC</strong></li>
</ul>
<p>PGC<a href="#refer-anchor-6"><sup>6</sup></a>将卷积理解为特定的采样函数与特定的权重函数相乘后求和。与GraphSAGE认为邻域不需要排序，并且使用均值采样确定邻域不同，PGC采用更加泛化的方式，为邻域节点根据某种度量分类，不同的类别分配不同的权重函数。
<img src="https://i.loli.net/2020/07/15/aDihLckAWG3n5PR.png" width = "100%" height = "60%" div align = center /></p>
<img src="https://i.loli.net/2020/07/15/jGtYhxEK1BNviPX.png" width = "100%" height = "60%" div align = center />
<h2 id="池化算子"><a href="#池化算子" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:池化算子" class="headings">池化算子</a></h2>
<p>在传统的卷积神经网络中，卷积会和池化相结合 ,池化算子一方面能够减少学习的参数 ,另外一方面能反应输入数据的层次结构 ,而在图卷积神经网络中 ,在解决节点级别的任务如节点分类，链接预测时，池化算子并非必要。但在图层面的学习任务中，如图分类问题需要关注图数据的全局信息，既包含图的结构信息，也包含各个节点的属性信息，模型的重点在于如何学习一个优秀的全图表示向量，在这个层面上，如何实现层次化池化机制是GNN需要解决的基础问题。</p>
<h3 id="层次化池化机制的几种实现思路"><a href="#层次化池化机制的几种实现思路" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:层次化池化机制的几种实现思路" class="headings">层次化池化机制的几种实现思路</a></h3>
<ul>
<li>基于图坍塌的池化机制</li>
</ul>
<p>图坍塌是将图划分为不同的子图，然后将子图视为超级节点，从而形成一个坍塌的图。</p>
<ul>
<li>基于<strong>TopK</strong> 的池化机制</li>
</ul>
<p>对图中每个节点学习出一个分数，基于这个分数的排序丢弃一些低分数的节点，这类方法借鉴了CNN中最大池化操作的思路：将更重要的信息筛选出来。所不同的是，图数据难以实现局部滑窗操作，因此需要依据分数进行全局筛选。</p>
<ul>
<li>基于边收缩的池化操作</li>
</ul>
<p>边收缩是指并行地将图中的边移除，并将移除边的两个节点合并，同时保持被移除节点的连接关系，该思路通过归并操作来逐步学习图的全局信息的方法。</p>
<h2 id="图卷积神经网络的过平滑现象以及缓解方案"><a href="#图卷积神经网络的过平滑现象以及缓解方案" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:图卷积神经网络的过平滑现象以及缓解方案" class="headings">图卷积神经网络的过平滑现象以及缓解方案</a></h2>
<h3 id="gcn的过平滑现象"><a href="#gcn的过平滑现象" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:gcn的过平滑现象" class="headings">GCN的过平滑现象</a></h3>
<p>论文<a href="#refer-anchor-7"><sup>7</sup></a>在图卷积神经网络的协同训练展开详细分析 ,指出 GCN 的本质是拉普拉斯平滑。</p>
<p>对图做拉普拉斯平滑,其形式如下：
$$
\left(\boldsymbol{I}-\gamma \hat{\boldsymbol{D}}^{-1} \hat{\boldsymbol{L}}\right) X
$$
其中 $, \hat{L}=\hat{D}-\hat{A}, 0&lt;\gamma&lt;1$ 是超参数，用以调节源节点
和周围节点的权重.令 $\gamma=1$,则拉普拉斯平滑形式为 $\hat{\mathbf{D}}^{-1} \hat{\boldsymbol{A}} X,$ 此时如果用对称标准化的拉普拉斯算子
$\hat{\boldsymbol{D}}^{-\frac{1}{2}} \hat{\boldsymbol{L}} \hat{\boldsymbol{D}}^{-\frac{1}{2}}$ 替代 $\hat{\boldsymbol{D}}^{-1} \hat{\boldsymbol{L}},$ 则上式等价为 $\hat{\boldsymbol{D}}^{-\frac{1}{2}} \hat{\boldsymbol{A}} \hat{\boldsymbol{D}}^{-\frac{1}{2}} X$
由此可见，GCN 的本质是在网络上做拉普拉斯平
滑.拉普拉斯平滑使用每个节点的邻居及自身表达 的加权和作为当前节点的新特征，由于在分类任务
中，同类节点倾向于稠密连接，平滑使得同类节点的
特征更为相似，进而提升了下游的分类任务.同时文
章指出，多层的 GCN 在更新目标节点表达时，会混合其他类节点的信息，进而导致效果降低，这是多层GCN存在的过平滑现象。</p>
<h3 id="过平滑现象的缓解方案"><a href="#过平滑现象的缓解方案" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:过平滑现象的缓解方案" class="headings">过平滑现象的缓解方案</a></h3>
<p>浅层图卷积网络的感受野，特征提取能力有限，多层图卷积网络具有强大特征提取能力，但多次拉普拉斯平滑会使输出特征过度平滑，同时会带来梯度消失，过拟合的问题。</p>
<p>缓解过平滑问题的一些方法：</p>
<ul>
<li>深度拓展：高层特征融合底层特征
<img src="https://i.loli.net/2020/08/18/uE6RSM5XFf2cbxU.png" width = "100%" height = "60%" div align = center /></li>
<li>宽度拓展：全局特征融合局部特征
<img src="https://i.loli.net/2020/08/18/BS9FH6sRcWL5lM4.png" width = "100%" height = "60%" div align = center /></li>
</ul>
<h2 id="图卷积神经网络的任务分类"><a href="#图卷积神经网络的任务分类" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:图卷积神经网络的任务分类" class="headings">图卷积神经网络的任务分类</a></h2>
<img src="https://i.loli.net/2020/08/18/ND2wPd7QKW5YLVb.png" width = "100%" height = "60%" div align = center />
<h2 id="图卷积神经网络在计算机视觉中的应用"><a href="#图卷积神经网络在计算机视觉中的应用" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:图卷积神经网络在计算机视觉中的应用" class="headings">图卷积神经网络在计算机视觉中的应用</a></h2>
<ol>
<li>基于图匹配的直线匹配</li>
<li>基于谱聚类的模型拟合</li>
</ol>
<h2 id="参考文献"><a href="#参考文献" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:参考文献" class="headings">参考文献</a></h2>
<div id="refer-anchor-1"></div>
<ul>
<li>[1] The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and other Irregular Domains.
IEEE Signal Processing Magazine</li>
</ul>
<div id="refer-anchor-2"></div>
<ul>
<li>[2] Discrete Regularization on Weighted Graphs for Image and Mesh Filtering</li>
</ul>
<div id="refer-anchor-3"></div>
<ul>
<li>[3] Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in neural information processing systems (NIPS), 2016.</li>
</ul>
<div id="refer-anchor-4"></div>
<ul>
<li>[4] Graph Attention Networks ICLR 2018</li>
</ul>
<div id="refer-anchor-5"></div>
<ul>
<li>[5] Inductive representation learning on large graphs, in Proc. of NIPS, 2017</li>
</ul>
<div id="refer-anchor-6"></div>
<ul>
<li>[6] Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action. AAAI. 2018.</li>
</ul>
<div id="refer-anchor-7"></div>
<ul>
<li>[7] Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, in AAAI 2019.</li>
</ul>
            </div>

            


        </article>

        

        
    <div class="updated-badge-container">
        <span title="Updated @ 2021-06-11 05:03:46 CST" style="cursor:help">

<svg xmlns="http://www.w3.org/2000/svg" width="130" height="20" class="updated-badge"><linearGradient id="b" x2="0" y2="100%"><stop offset="0" stop-color="#bbb" stop-opacity=".1"/><stop offset="1" stop-opacity=".1"/></linearGradient><clipPath id="a"><rect width="130" height="20" rx="3" fill="#fff"/></clipPath><g clip-path="url(#a)"><path class="updated-badge-left" d="M0 0h55v20H0z"/><path class="updated-badge-right" d="M55 0h75v20H55z"/><path fill="url(#b)" d="M0 0h130v20H0z"/></g><g fill="#fff" text-anchor="middle" font-size="110"><text x="285" y="150" fill="#010101" fill-opacity=".3" textLength="450" transform="scale(.1)">updated</text><text x="285" y="140" textLength="450" transform="scale(.1)">updated</text><text x="915" y="150" fill="#010101" fill-opacity=".3" textLength="650" transform="scale(.1)">2021-06-11</text><text x="915" y="140" textLength="650" transform="scale(.1)">2021-06-11</text></g></svg>
        </span></div>



        


        <div class="post-share">

        
            <div class="share-text"></div>
        

        <div class="share-items">

            

            

            

            

            

            

            

            

            
                <div class="share-item qrcode">
                    <div class="qrcode-container" title=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon qrcode-icon"><path d="M0 224h192V32H0v192zM64 96h64v64H64V96zm192-64v192h192V32H256zm128 128h-64V96h64v64zM0 480h192V288H0v192zm64-128h64v64H64v-64zm352-64h32v128h-96v-32h-32v96h-64V288h96v32h64v-32zm0 160h32v32h-32v-32zm-64 0h32v32h-32v-32z"/></svg><div id="qrcode-img"></div>
                    </div>
                    <script src="https://cdn.jsdelivr.net/npm/qrcode-generator@1.4.4/qrcode.min.js"></script>

<script>
    const typeNumber = 0;
    const errorCorrectionLevel = 'L';
    const qr = qrcode(typeNumber, errorCorrectionLevel);
    qr.addData('https:\/\/wjiajie.github.io\/contents\/gnn\/introducegcnn\/');
    qr.make();
    document.getElementById('qrcode-img').innerHTML = qr.createImgTag();
</script>

                </div>
            

        </div>

    </div>




        
    
    
        <div class="related-posts">
            <h2 class="related-title"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon related-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm144 276c0 6.6-5.4 12-12 12h-92v92c0 6.6-5.4 12-12 12h-56c-6.6 0-12-5.4-12-12v-92h-92c-6.6 0-12-5.4-12-12v-56c0-6.6 5.4-12 12-12h92v-92c0-6.6 5.4-12 12-12h56c6.6 0 12 5.4 12 12v92h92c6.6 0 12 5.4 12 12v56z"/></svg></h2>
            <ul class="related-list">
                
                    <li class="related-item">
                        <a href="/contents/gnn/model_fit/" class="related-link">图卷积网络在多模型拟合中的应用</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/contents/dl/basic_network_struct/" class="related-link">pytorch 搭建网络结构</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/contents/dl/hrnet/" class="related-link">HRNet 网络结构以及代码分析</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/contents/dl/mask-rcnn/" class="related-link">Mask Rcnn 网络结构总结</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/contents/dl/keypoint-rcnn-caffe2-c&#43;&#43;/" class="related-link">Detectron2 keypoint_rcnn 网络c++版本部署</a>
                    </li>
                
            </ul>
        </div>
    



        


        
    <footer class="minimal-footer">
        
            <div class="post-tag"><a href="/tags/gnn/" rel="tag" class="post-tag-link">#gnn</a> <a href="/tags/dl/" rel="tag" class="post-tag-link">#dl</a></div>
        
        
            <div class="post-category">
                <a href="/contents/" class="post-category-link active">contents</a>
            </div>
        
        
    </footer>



        


        
    
        
        
    
    
    
    
        <ul class="post-nav">
            
                <li class="post-nav-prev">
                    <a href="/contents/gnn/model_fit/" rel="prev">&lt; 图卷积网络在多模型拟合中的应用</a>
                </li>
            
            
        </ul>
    



        
    

        

        

        

        
            <div id="utterances"></div>
        

        
    



    </div>
</main>


            
    <div id="back-to-top" class="back-to-top">
        <a href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a>
    </div>


            
    <footer id="footer" class="footer">
        <div class="footer-inner">
            <div class="site-info">2019–2023&nbsp;<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon footer-icon"><path d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"/></svg>&nbsp;JiaJie</div>

            
    
        <ul class="socials"><li class="socials-item">
                    <a href="https://s2.loli.net/2022/03/20/f6lhgGukJ9QT2Yv.jpg" target="_blank" rel="external noopener" title="wechat"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon social-icon"><path d="M0 224h192V32H0v192zM64 96h64v64H64V96zm192-64v192h192V32H256zm128 128h-64V96h64v64zM0 480h192V288H0v192zm64-128h64v64H64v-64zm352-64h32v128h-96v-32h-32v96h-64V288h96v32h64v-32zm0 160h32v32h-32v-32zm-64 0h32v32h-32v-32z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://github.com/Wjiajie" target="_blank" rel="external noopener" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a>
                </li></ul>
    



            

<div class="container" role="main" itemscope itemtype="http://schema.org/Article">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
           
            <article role="main" class="blog-post" itemprop="articleBody" id="content">
              
                
                
                <div class="social-share" data-initialized="true" data-wechat-qrcode-title="扫一扫分享到微信">
    <center>
    <font style="font-size:18px;color:darkcyan;">分享到：</font>
    <a href=" " class="social-share-icon icon-weibo"></a >
    <a href="#" class="social-share-icon icon-wechat"></a >
    <a href="#" class="social-share-icon icon-twitter"></a >
    <a href="#" class="social-share-icon icon-linkedin"></a >
    <a href="#" class="social-share-icon icon-facebook"></a >
    <a href="#" class="social-share-icon icon-qq"></a >
    <a href="#" class="social-share-icon icon-qzone"></a >
    </center>
</div>


<link rel="stylesheet" href="https://wjiajie.github.io/css/share.min.css" />
<script src="https://hugo-picture.oss-cn-beijing.aliyuncs.com/social-share.min.js"></script>

                
                
            </article>



        </div>
    </footer>


        </div>
        

        


    <script>
    if (typeof MathJax === 'undefined') {
        window.MathJax = {
            loader: {
                load: ['[tex]/mhchem']
            },
            
            tex: {
                inlineMath: {'[+]': [['$', '$']]},
                tags: 'ams',
                packages: {'[+]': ['mhchem']}
            }
        };
        (function() {
            const script = document.createElement('script');
            script.src = 'https:\/\/cdn.jsdelivr.net\/npm\/mathjax@3.1.2\/es5\/tex-mml-chtml.js';
            script.defer = true;
            document.head.appendChild(script);
        })();
    } else {
        MathJax.texReset();
        MathJax.typeset();
    }
</script>






    

        

        

        
            <script>
    function loadComments() {
        (function() {
            const utterances = document.getElementById("utterances");
            if (!utterances) {
                return;
            }
            const script = document.createElement('script');
            script.src = 'https:\/\/utteranc.es\/client.js';
            script.async = true;
            script.crossOrigin = 'anonymous';
            script.setAttribute('repo', 'Wjiajie\/Wjiajie.github.io');
            script.setAttribute('issue-term', 'pathname');
            const isDark = getCurrentTheme() === 'dark';
        if (isDark) {
            script.setAttribute('theme', 'photon-dark');
        } else {
            script.setAttribute('theme', 'github-light');
        }
            
            utterances.appendChild(script);
        })();
    }
</script>
        

        

    



    <script src="/js/medium-zoom.min.js"></script>

<script>
    mediumZoom(document.querySelectorAll('div.post-body img'), {
        background: 'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'
    })
</script>




    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module" defer></script>



    
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    




    </body>
</html>
