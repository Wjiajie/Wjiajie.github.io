[{"categories":["contents"],"content":"中期学习计划\n学习项目 学习taichi语法和数据结构taichi-lang，汇总文档。 图形渲染引擎学习ogre，汇总文档。 学习emscripten, 汇总文档。 C++持续学习 TheAlgorithms/C-Plus-Plus。 json-tutorial, 汇总文档。 技术架构 技术架构文章学习总结，汇总文档。 2023读书笔记，汇总文档。 视野扩展 AIGC研究，汇总文档。 图形学论文，汇总文档。 计算机视觉论文，汇总文档。 学习midjourney文档，汇总文档。 ","title":"2023 Todo List","uri":"/contents/day-day-up/2023-todo-list/"},{"categories":["contents"],"content":"最近我希望能将实现的python算子通过taichi封装后打包成C++静态库，结合C++代码形成应用，并希望这个应用能在web上自由地运行，用户不需要关心具体平台，登录任何小程序就可以体验到发布的程序，于是有了这篇文章的尝试。选择从C++编译至webassembly主要是出于性能上的考量，这条链路应该是一个比较成熟的方案了。\n方案选择 C++编译至webassembly的方案有挺多的。\nEmscripten 是一个 LLVM 到 JavaScript 编译器，它可以将 C/C++ 代码编译成 JavaScript 代码，以便在浏览器中运行。它通过模拟 POSIX 环境，为编译的代码提供了一个可移植的运行环境。\nPNaCl：PNaCl 是一个插件，可以将 C/C++ 代码编译成适用于 Native Client 平台的代码。PNaCl 为您提供了比 Emscripten 更好的性能和可移植性。\nLLJS：LLJS 是一个 JavaScript 实现的 LLVM 编译器，可以将 LLVM 的 IR 代码编译成 JavaScript 代码。\nCheerp：Cheerp 是一个 C++ 到 JavaScript 编译器，专门针对高性能的要求进行了优化。\n这些工具的选择取决于个人的具体需求，如果需要高性能，那么 PNaCl 和 Cheerp 可能是更好的选择；如果需要高度可移植性，那么 Emscripten 和 PNaCl 可能是更好的选择。\n综合考虑，还是选择了目前最为主流的Emscripten。\nEmscripten配置 参考Download and install的安装步骤安装即可， Emscripten使用emsdk来管理依赖包环境，程序更新，相当于一个隔离的虚拟环境。可以使用emsdk拉去 Emscripten最新代码，安装激活依赖库。\n安装好环境以后，可以使用emsdk/emcmdprompt终端激活虚拟环境，并在终端内完成其他操作。 emsdk list 可以查看安装依赖库详情 emsdk install xxx 安装具体依赖库 Emscripten推荐使用Cmake1来组织C++工程结构，Emscripten使用emcmake和emmake命令生成makefile和最终构建产物，注意生成makefile不是必要的，使用：\n1 2 emcmake cmake . -B cmake-build-emscripten -G Ninja cmake --build cmake-build-emscripten 则不需要生成makefile，如果想生成makefile，可以使用：\n1 2 emcmake cmake . -B cmake-build-emscripten -G \"CodeBlocks - MinGW Makefiles\" cmake --build cmake-build-emscripten 注意MinGW和Ninja都可以通过emsdk install 安装2。\nC++转webassembly的一个例子 在这里以一个yaml-cpp调用作为例子,首先进入你的demo工程根目录，执行下面命令\n1 2 3 4 5 mkdir thirdpart \u0026\u0026 cd thirdpart git clone git@github.com:jbeder/yaml-cpp.git cd yaml-cpp \u0026\u0026 mkdir build \u0026\u0026 cd build cmake -G \"CodeBlocks - MinGW Makefiles\" .. make -j 这会pull yaml-cpp并在build目录编译好静态库libyaml-cpp.a。回到demo工程根目录， 新增 CmakeLists.txt:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cmake_minimum_required(VERSION 3.15) # 根据你的需求进行修改 project(main) set(CMAKE_CXX_STANDARD 11) # 根据你的C++编译器支持情况进行修改 # 如果要编译纯C++工程，把该行命令注释掉 set(CMAKE_EXECUTABLE_SUFFIX \".html\") # 编译生成.html include_directories(thirdpart) # 使得我们能引用第三方库的头文件 add_subdirectory(thirdpart/yaml-cpp) add_executable(main main.cpp) # 设置Emscripten的编译链接参数，我们等等会讲到一些常用参数 # 如果要编译纯C++工程，把该行命令注释掉 set_target_properties(main PROPERTIES LINK_FLAGS \"-s EXIT_RUNTIME=1\") target_link_libraries(main yaml-cpp) # 将第三方库与主程序进行链接 新增main.cpp:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u003ciostream\u003e #include \"yaml-cpp/yaml.h\" int main() { YAML::Node lineup = YAML::Load(\"{1B: Prince Fielder, 2B: Rickie Weeks, LF: Ryan Braun}\"); for(YAML::const_iterator it=lineup.begin();it!=lineup.end();++it) { std::cout \u003c\u003c \"Playing at \" \u003c\u003c it-\u003efirst.as\u003cstd::string\u003e() \u003c\u003c \" is \" \u003c\u003c it-\u003esecond.as\u003cstd::string\u003e() \u003c\u003c \"\\n\"; } lineup[\"RF\"] = \"Corey Hart\"; lineup[\"C\"] = \"Jonathan Lucroy\"; return 0; } 编译：\n1 2 emcmake cmake . -B cmake-build-emscripten -G Ninja cmake --build cmake-build-emscripten 注意这里用上了emcmake，如果只想编译纯C++工程，除了上述的CmakeLists需要改动以外，编译命令可以改为：\n1 2 cmake . -B cmake-build-emscripten -G Ninja cmake --build cmake-build-emscripten 编译的产物是：*.html, *.js, *.wasm， *.html可以通过静态服务器程序启动(NPM中的static-server, http-server,)。比如我可以通过http-server启动刚刚的yaml-cpp的demo:\n推荐阅读 快速上手WebAssembly应用开发：Emscripten使用入门\n为什么说 WebAssembly 是 Web 的未来？\n参考文献 CMakeTutorial ↩︎\nemcmake-with-emscripten ↩︎\n","title":"利用Emscripten编译C++至webassembly","uri":"/contents/c++/emscripten_intro/"},{"categories":["contents"],"content":"Json benchmark环境配置 使用nativejson-benchmark作性能测试benchmark。 按照nativejson-benchmark中[Build and Run]一节编译环境，注意对于苹果m1电脑，需要在nativejson-benchmark/build/premake5.lua中修改：\n1 buildoptions \"-mcpu=apple-m1 -Wall -Wextra\" 如果用xcode自带clang编译器编译失败，可以安装llvm：\n1 2 3 4 5 6 7 brew install llvm open ~/.bash_profile // 添加一下命令 到bash_profile // export PATH=\"/opt/homebrew/opt/llvm/bin:$PATH\" // export LDFLAGS=\"-L/opt/homebrew/opt/llvm/lib\" // export CPPFLAGS=\"-I/opt/homebrew/opt/llvm/include\" source ~/.bash_profile 新增新的json测试单元，以yaml-cpp为例子 Git clone yaml-cpp 到nativejson-benchmark/thirdparty/yaml-cpp并按照官方教程编译静态库。 在nativejson-benchmark/src/tests新增yamlcpptest.cpp： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 #include \"../test.h\" #include \"yaml-cpp/yaml.h\" using namespace YAML; static void GenStat(Stat\u0026 stat, const Node \u0026v) { switch (v.Type()) { case NodeType::Null: { stat.nullCount++; break; } case NodeType::Scalar: { std::string scalar = v.Scalar(); if(scalar == \"true\") { stat.trueCount++; stat.stringLength += scalar.size(); } else if(scalar == \"false\") { stat.falseCount++; stat.stringLength += scalar.size(); } else if (scalar == \"null\") { stat.nullCount++; stat.stringLength += scalar.size(); } double tDouble = 0.0; if (convert\u003cdouble\u003e::decode(v, tDouble)) { stat.numberCount++; } else stat.stringCount++; break; } case NodeType::Sequence: { uint32_t size = 0; for(const auto\u0026 i : v) { size++; GenStat(stat, i); } stat.arrayCount++; stat.elementCount += size; break; } case NodeType::Map: { uint32_t size = 0; for (auto it = v.begin(); it != v.end(); ++it) { size++; std::string key = it-\u003efirst.as\u003cstd::string\u003e(); Node value = it-\u003esecond; GenStat(stat, value); stat.stringLength += key.size(); } stat.objectCount++; stat.memberCount += size; stat.stringCount += size; break; } default: break; } } class YamlCppParseResult : public ParseResultBase { public: YamlCppParseResult() {} ~YamlCppParseResult() {} Node node; }; class YamlCppStringResult : public StringResultBase { public: virtual const char* c_str() const { return str.c_str(); } std::string str; }; class YamlCppTest : public TestBase { public: #if TEST_INFO virtual const char* GetName() const { return \"YamlCpp (C++11)\"; } virtual const char* GetFilename() const { return __FILE__; } #endif #if TEST_PARSE virtual ParseResultBase* Parse(const char* json, size_t length) const { (void)length; YamlCppParseResult *pr = new YamlCppParseResult; try { pr-\u003enode = Load(json); } catch (...) { delete pr; return 0; } return pr; } #endif #if TEST_STRINGIFY virtual StringResultBase* Stringify(const ParseResultBase* parseResult) const { const YamlCppParseResult* pr = static_cast\u003cconst YamlCppParseResult*\u003e(parseResult); YamlCppStringResult* sr = new YamlCppStringResult; // this implement is not right, just for Stringify test sr-\u003estr = pr-\u003enode.Scalar(); return sr; } #endif #if TEST_PRETTIFY virtual StringResultBase* Prettify(const ParseResultBase* parseResult) const { const YamlCppParseResult* pr = static_cast\u003cconst YamlCppParseResult*\u003e(parseResult); YamlCppStringResult* sr = new YamlCppStringResult; // this implement is not right, just for Stringify test sr-\u003estr = pr-\u003enode.Scalar(); return sr; } #endif #if TEST_STATISTICS virtual bool Statistics(const ParseResultBase* parseResult, Stat* stat) const { const YamlCppParseResult* pr = static_cast\u003cconst YamlCppParseResult*\u003e(parseResult); memset(stat, 0, sizeof(Stat)); GenStat(*stat, pr-\u003enode); return true; } #endif #if TEST_CONFORMANCE virtual bool ParseDouble(const char* json, double* d) const { Node node; node = Load(json); if(!node.IsNull() \u0026\u0026 node.IsSequence() \u0026\u0026 node.size() == 1 \u0026\u0026 node[0].IsScalar()) { double tDouble = 0.0; if (convert\u003cdouble\u003e::decode(node[0], tDouble)) { *d = tDouble; return true; } } return false; } // virtual bool ParseString(const char* json, std::string\u0026 s) const { // Node node; // node = Load(json); // if (!node.IsNull() \u0026\u0026 node.IsScalar()) { // std::string str = node.Scalar(); // s = str; // return true; // } // return false; // } #endif }; REGISTER_TEST(YamlCppTest); 在nativejson-benchmark/build/premake5.lua中新增yaml-cpp到头文件和库文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 function setTargetObjDir(outDir) targetdir(outDir) objdir(string.lower(\"../intermediate/%{cfg.shortname}/\" .. _ACTION)) targetsuffix(string.lower(\"_%{cfg.shortname}_\" .. _ACTION)) end function copyfiles(dstDir, srcWildcard) os.mkdir(dstDir) local matches = os.matchfiles(srcWildcard) for _, f in ipairs(matches) do local filename = string.match(f, \".-([^\\\\/]-%.?[^%.\\\\/]*)$\") os.copyfile(f, dstDir .. \"/\" .. filename) end end function gmake_common() buildoptions \"-mcpu=apple-m1 -Wall -Wextra\" -- buildoptions \"-march=x86_64 -Wall -Wextra\" if (os.findlib(\"boost_system\")) then defines { \"HAS_BOOST=1\" } links { \"boost_system\" } end -- On some boost distributions, the naming contains -mt suffixes if (os.findlib(\"boost_thread\")) then links { \"boost_thread\" } elseif (os.findlib(\"boost_thread-mt\")) then links { \"boost_thread-mt\" } end if (os.findlib(\"boost_locale\")) then links { \"boost_locale\" } elseif (os.findlib(\"boost_locale-mt\")) then links { \"boost_locale-mt\" } end -- For clock_gettime in jvar if (os.findlib(\"rt\")) then links { \"rt\" } end if (os.findlib(\"PocoJSON\")) then defines { \"HAS_POCO=1\" } links { \"PocoFoundation\", \"PocoJSON\" } end if (os.findlib(\"folly\")) then defines { \"HAS_FOLLY=1\" } links { \"folly\" } end if (os.findlib(\"v8\")) then defines { \"HAS_V8=1\" } links { \"v8_base\", \"v8_libbase\", \"v8_libplatform\", \"v8_nosnapshot\" } end if (os.findlib(\"libcpprest\")) then defines { \"HAS_CPPREST=1\" } links { \"cpprest\"} end configuration \"macosx\" if (os.isdir(\"/usr/local/opt/qt5/include\")) then defines { \"HAS_QT=1\" } links { \"QtCore.framework\" } includedirs { \"/usr/local/opt/qt5/include\" } linkoptions { \"-F /usr/local/opt/qt5/lib\" } end -- Temp fix for OSX brew + V8 include path issue if (os.isdir(\"/usr/local/opt/v8/\")) then includedirs { \"/usr/local/opt/v8/\" } end end solution \"benchmark\" configurations { \"release\" } platforms { \"x32\", \"x64\" } location (\"./\" .. (_ACTION or \"\")) language \"C++\" flags { \"ExtraWarnings\" } defines { \"__STDC_FORMAT_MACROS=1\" } configuration \"release\" defines { \"NDEBUG\" } optimize \"Full\" configuration \"vs*\" defines { \"_CRT_SECURE_NO_WARNINGS\" } configuration \"gmake\" gmake_common() project \"jsonclibs\" kind \"StaticLib\" includedirs { \"../thirdparty/\", \"../thirdparty/include/\", \"../thirdparty/ujson4c/3rdparty/\", \"../thirdparty/pjson/inc/\", \"../thirdparty/udp-json-parser/\", \"../thirdparty/facil.io/lib/facil/core/types\", \"../thirdparty/facil.io/lib/facil/core/types/fiobj\", } files { \"../src/**.c\", } setTargetObjDir(\"../bin\") copyfiles(\"../thirdparty/include/yajl\", \"../thirdparty/yajl/src/api/*.h\" ) project \"nativejson\" kind \"ConsoleApp\" includedirs { \"../thirdparty/\", \"../thirdparty/fastjson/include/\", \"../thirdparty/jsonbox/include/\", \"../thirdparty/jsoncpp/include/\", \"../thirdparty/rapidjson/include/\", \"../thirdparty/yaml-cpp/include/\", \"../thirdparty/udp-json-parser/\", \"../thirdparty/include/\", \"../thirdparty/json-voorhees/include\", \"../thirdparty/json-voorhees/src\", \"../thirdparty/jsoncons/src\", \"../thirdparty/ArduinoJson/include\", \"../thirdparty/include/jeayeson/include/dummy\", \"../thirdparty/jvar/include\", \"../thirdparty/pjson/inc\", \"../thirdparty/facil.io/lib/facil/core/types\", \"../thirdparty/facil.io/lib/facil/core/types/fiobj\", \"../thirdparty/simdjson/singleheader\", \"../thirdparty/boost/libs/json/include\", \"../thirdparty/boost/libs/config/include\", \"../thirdparty/boost/libs/assert/include\", \"../thirdparty/boost/libs/exception/include\", \"../thirdparty/boost/libs/throw_exception/include\", \"../thirdparty/boost/libs/core/include\", \"../thirdparty/boost/libs/container/include\", \"../thirdparty/boost/libs/move/include\", \"../thirdparty/boost/libs/static_assert/include\", \"../thirdparty/boost/libs/intrusive/include\", \"../thirdparty/boost/libs/system/include\", \"../thirdparty/boost/libs/mp11/include\", \"../thirdparty/boost/libs/align/include\", } linkoptions { \"../../thirdparty/yaml-cpp/build/libyaml-cpp.a\" } files { \"../src/*.h\", \"../src/*.cpp\", \"../src/tests/*.cpp\" } libdirs { \"../bin\", \"../thirdparty/simdjson\", } setTargetObjDir(\"../bin\") -- linkLib(\"jsonclibs\") links \"jsonclibs\" configuration \"gmake\" buildoptions \"-std=c++14\" solution \"jsonstat\" configurations { \"release\" } platforms { \"x32\", \"x64\" } location (\"./\" .. (_ACTION or \"\")) language \"C++\" flags { \"ExtraWarnings\" } defines { \"USE_MEMORYSTAT=0\", \"TEST_PARSE=1\", \"TEST_STRINGIFY=0\", \"TEST_PRETTIFY=0\", \"TEST_TEST_STATISTICS=1\", \"TEST_SAXROUNDTRIP=0\", \"TEST_SAXSTATISTICS=0\", \"TEST_SAXSTATISTICSUTF16=0\", \"TEST_CONFORMANCE=0\", \"TEST_INFO=0\" } includedirs { \"../thirdparty/\", \"../thirdparty/fastjson/include/\", \"../thirdparty/jsonbox/include/\", \"../thirdparty/jsoncpp/include/\", \"../thirdparty/rapidjson/include/\", \"../thirdparty/yaml-cpp/include/\", \"../thirdparty/udp-json-parser/\", \"../thirdparty/include/\", \"../thirdparty/json-voorhees/include\", \"../thirdparty/json-voorhees/src\", \"../thirdparty/jsoncons/src\", \"../thirdparty/ArduinoJson/include\", \"../thirdparty/include/jeayeson/include/dummy\", \"../thirdparty/jvar/include\", \"../thirdparty/pjson/inc\", \"../thirdparty/facil.io/lib/facil/core/types\", \"../thirdparty/facil.io/lib/facil/core/types/fiobj\", \"../thirdparty/simdjson/singleheader\", \"../thirdparty/boost/libs/json/include\", \"../thirdparty/boost/libs/config/include\", \"../thirdparty/boost/libs/assert/include\", \"../thirdparty/boost/libs/exception/include\", \"../thirdparty/boost/libs/throw_exception/include\", \"../thirdparty/boost/libs/core/include\", \"../thirdparty/boost/libs/container/include\", \"../thirdparty/boost/libs/move/include\", \"../thirdparty/boost/libs/static_assert/include\", \"../thirdparty/boost/libs/intrusive/include\", \"../thirdparty/boost/libs/system/include\", \"../thirdparty/boost/libs/mp11/include\", \"../thirdparty/boost/libs/align/include\", } configuration \"release\" defines { \"NDEBUG\" } optimize \"Full\" configuration \"vs*\" defines { \"_CRT_SECURE_NO_WARNINGS\" } configuration \"gmake\" gmake_common() project \"jsonclibs2\" kind \"StaticLib\" includedirs { \"../thirdparty/\", \"../thirdparty/include/\", \"../thirdparty/ujson4c/3rdparty/\", \"../thirdparty/udp-json-parser/\", \"../thirdparty/facil.io/lib/facil/core/types\", \"../thirdparty/facil.io/lib/facil/core/types/fiobj\", } files { \"../src/**.c\", } setTargetObjDir(\"../bin/jsonstat\") copyfiles(\"../thirdparty/include/yajl\", \"../thirdparty/yajl/src/api/*.h\", \"../thirdparty/simdjson/src/simdjson.cpp\") local testfiles = os.matchfiles(\"../src/tests/*.cpp\") for _, testfile in ipairs(testfiles) do project(\"jsonstat_\" .. path.getbasename(testfile)) kind \"ConsoleApp\" files { \"../src/jsonstat/jsonstatmain.cpp\", \"../src/memorystat.cpp\", testfile } libdirs { \"../bin/jsonstat\" } links \"jsonclibs2\" setTargetObjDir(\"../bin/jsonstat\") linkoptions { \"../../thirdparty/yaml-cpp/build/libyaml-cpp.a\" } configuration \"gmake\" buildoptions \"-std=c++14\" end json数据对比 ","title":"json 性能比较","uri":"/contents/c++/json_compare/"},{"categories":["contents"],"content":"Midjourney学习文档以及 Midjourney关键词\nMidjourney技术背景 Midjourney使用方法 Midjourney使用技巧","title":"Midjourney学习","uri":"/contents/aigc/midjourneylearning/"},{"categories":["contents"],"content":"taichi官网 1 taichi zoo2 taichi learn forum3 github links4\ntaichi官网 ↩︎\ntaichi zoo ↩︎\ntaichi forum ↩︎\ngithub ↩︎\n","title":"Taichi语法记录","uri":"/contents/cg/taichi_intro/"},{"categories":["contents"],"content":"\n这是在青海湖一家民宿附近拍的图，第一次住蒙古包，从西宁拼了面包车到青海湖附近一个小镇，店家老板开私家车接我过去，晚上别人吃手抓羊肉，我问老板一个人能吃吗，他说不行，然后让我和他的家人一起吃饭……夜晚还有篝火晚会，怀念当时的感觉。\n第二天被老板叫醒去看日出，大夏天的早上贼冷……当时有一群刚本科毕业的学姐，让我给她们拍照，看完日出后她们继续去拉萨，我预算不足只能回西安了。\n寒假收假回学校前先去了一趟杭州玩，杭州的马路真的宽大，浙大留学生饭堂的菜真好吃……走题了，拍摄于西湖，当时想如果在杭州工作，平时心情不好来西湖走走得有多好。\n这是俺滴老家……拍摄于18年的春节。\n上面四张照片都是用我的canon g9x拍摄的，21年买了单反，把小卡片机挂闲鱼卖掉了，当时让买家一定要好好对待它，时隔几个月后开始后悔，不知道它现在怎么样了。\n时间隔了好久好久没有拍照了，21年朋友让我给她推荐微单，勾起了我购买设备的欲望，最后她没买成相机，我买了……早在本科16年买canon g9x之前，我想买的是pentax的单反，但是太贵了，当时太穷买不起，时隔五年终于买得起pentax K1了，虽然它很大很沉，但我认为它是最好的相机(对我来说)，我喜欢pentax这个牌子的相机，认可它的理念，附带也喜欢理光。咳咳又跑题了，继续介绍明信片……\n这张拍摄于终南山寨，是我们实验室去年的团建地点，老师想去爬山，结果大家投票去了玩水，然后老师没去(嫌弃)，我们自己去玩了，咳咳。团建完的第二天，我坐上高铁去南京出差了，没想到一去就是三个月。\n我为pentax买了一个手动的50mm f1.4定焦头，当天夜晚去玄武湖公园扫街，拍摄于某条斑马线，南京的6月挺热的，闷热的夜晚，冰西瓜，烤鸭，因为买烤鸭和我熟悉的大叔大妈(年轻时在广东东莞打工，他们的女儿现在在读大学)，青旅老板的咖啡，电影，民谣和电子烟，我怀念它们。\n拍摄于夫子庙附近，摄影师他们在给模特拍照，我乱入，说能不能也拍一张。\n他叫瓜子，喜欢踩棉被，发出咕咕咕的声音，喜欢趴我的手臂。和我一样没有脑子，22年再次去南京，瓜子已经不认得我了。\n拍摄于某个炎热的扫街的夜晚，我就站在路的一边，举着相机对着墙等待路人经过（从路人视角看来我一定很让人害怕…）\n拍摄于先锋书店，一位大姐困了，我不好意思拍她，于是我对着窗拍了一张，照片还记录了窗外的花草。\n一个雨夜和好友去扫街，发现墙角上的猫猫。\n拍摄于西安的西仓，这个地儿的烟火气太足了，想和你有机会去一次。\n拍摄于瓜大老校区附近的商业区，还好大哥没有对着我不然肯定被追着打……\n拍摄于21年的圣诞，举着四斤的pantex K1老久了，终于有一只小鸟乱入，我的相机虽然是最好的相机，它就是有那么一点点重！\n拍摄于玄武湖，枯萎的荷叶，结合倒影组成多边形。\n终南山，我爬过的最累的山了，但是爬的淋漓畅快。\n","title":"明信片简介","uri":"/contents/broken-thoughts/2022_05_20_to_xiaomai/"},{"categories":["contents"],"content":"Swin Transformer的提出让Transformer代替CNN作为计算机视觉领域多类下游任务的特征提取骨干网络，下文主要记录Swin Transformer的基本结构。 这里是官方代码链接1和论文链接2。\nSwin Transformer的提出动机 Transformer在NLP领域大展身手，但将Transformer迁移到CV领域，会面临两个困难：\n尺度问题：在一些CV任务中，比如目标检测问题，具有相同语义的实例由于尺度的问题，在图像中占据不同的像素规模，但目前基于Transformer的方法中，图像切片都是固定大小的。 计算复杂度：图像的特征张量展开成向量的方式计算自注意力这个过程，当图像分辨率稍大时，该过程的复杂度将变得难以忍受，因为此时的计算复杂度将与图像大小的平方成正比，这让Transformer在诸如语义分割的下游任务(需要像素级别的标签预测)中遇到困难。 为了解决上述问题，作者提出的Swin Transformer仅在局部窗口计算自注意力，并提出用Shifted windows得到特征的全局上下文信息(全局特征)。\n网络结构 Swin Transformer(Swin-T)网络结构如下图：\n假设网络输入图像尺寸是$H \\times W \\times 3$，经过Patch Partition模块后被分为互不重叠的patches，注意在论文中patch是最小的计算单元，一个patch区域的特征后续会被展开成一个向量用于计算自注意力。在论文中patch被设置为$4 \\times 4$的像素方块，所以经过Patch Partition模块后的特征图维度是$\\frac{H}{4} \\times \\frac{W}{4} \\times 48$。网络后续部分被分成四个Stage，除了Stage1是Linear Embedding模块与Swin Transformer Block的组合，后续的Stage都是Patch Merging与Swin Transformer Block的组合。Stage1中的linear embedding层将特征维度变换为$\\frac{H}{4} \\times \\frac{W}{4} \\times C$，Swin Transformer Block即论文提出的修改版本的Transformer模块，将在下文具体介绍。Patch Merging对特征图采用步长为2的等间隔采样，并将采样后的特征图在通道维度上合并，此时特征图的分辨率将降采样为原来的$\\frac{1}{4}$，通道为从$C$提升为$4C$，并通过一个线性变换层转换为$2C$。\n标准的Transformer由于需要计算全局自注意力，它的计算复杂度随着图像大小增长呈平方关系。为了解决计算复杂度的问题，论文提出让Swin Transformer Block只在一个区域(windows，论文将windows固定为7$\\times$7个patch，patch是最小的计算单元)中计算自注意力。全局的MSA和基于窗口的MSA的计算复杂度如下所示(仅考虑乘法操作)： 然而这样的方式中只能得到局部注意力，论文的主要贡献点在于提出了Shifted windows 的操作，Swin Transformer Block总是两两组合的，每次运算都会包括在原始windows中计算自注意力，以及在Shifted windows中计算自注意力，所以看上图中的虚线框每个stage的Swin Transformer Block的数目都是偶数，这样得到的特征已经能得到较大的感受野，多次运行Swin Transformer Block后，最终的特征能得到全局的感受野。\n两个Swin Transformer Block的组合方式如图一中(b) Two Successive Swin Transformer Blocks所示，W-MSA代表当前windows的多头自注意力网络，SW-MSA代表Shifted windows的多头自注意力网络。\n至于为了解决尺度的问题，Swin Transformer引入Patch Merging操作，模拟CNN中的池化操作，经过Patch Merging后特征图的分辨率下降，但特征通道维数增加了，这与CNN的骨干网络的效果是一样的。Shifted windows和Patch Merging的操作会在下文介绍。\nShifted windows Shifted windows的示意图如下：\n在上图中，一个大小为$M \\times M$($M$在论文里设定为7，如果特征图大小不能被$M$整除，则先零填充特征图)经过基础版本的Shifted windows操作后，4个windows会被分为9个windows，每个windows将分别计算自注意力，这样存在的问题是：由于每个windows并不是保持一样的大小，这给代码实现时并行化处理带来麻烦，降低了运算效率，论文中Shifted windows的实现巧妙地利用掩码(mask)的MSA层解决该问题，如下图所示：\n从上图中看出，论文将9个windows重新填补成4个windows并做好标记(标记该张量原本来自哪里)，然后对该4个windows计算W-MSA，但这样的操作会为本不应该计算注意力系数的向量之间也计算了注意力系数，因为重新填充后的窗口的向量很可能是不相关的。masked MSA的操作在重新填补的windows计算MSA时，为不应该计算注意力系数的区域填充一个很大的负数，这样MSA经过后续的softmax操作后，该区域将归零。这样，通过masked MSA的处理，虽然总的计算量提高了，但由于windows大小变的一致，硬件并行化实现反而然模型的运算效率提高了。masked MSA的计算示意图如下所示：\nmask的可视化如下所示： Patch Merging 如上图，Patch Merging会在每个stage开始时调整特征图分辨率，改变特征图的通道数，由于Swin Transformer Block不改变向量的通道数和特征图分辨率，这两者的改变都由Patch Merging实现。Patch Merging的做法时在行方向和列方向上，间隔2选取元素，然后拼接在一起作为一整个张量，最后展开。此时通道维度会变成原先的4倍（因为H,W各缩小2倍），此时再通过一个全连接层再调整通道维度为原来的两倍。\n相对位置编码 对于一个大小是$Wh \\times Ww$的窗口，每个patch的横坐标到其他patch的横坐标的距离(偏移)的取值范围是$(-(Wh-1), (Wh-1))$，每个patch的纵坐标到其他patch的纵坐标的距离(偏移)的取值范围是$(-(Ww-1), (Ww-1))$，分别把取值调整为$(0, (2 \\times Wh-1))$，$(0, (2 \\times Ww-1))$，故作者维持一个大小是$(2Wh-1) \\times (2Ww-1)$的偏置矩阵$\\hat{B}$(relative_position_bias_table)，该偏置矩阵是一个可学习参数组成的二维矩阵。对于一个$Wh \\times Ww$的窗口，需要得到每个patch到其他patch的相对位置编码，方法是计算一个相对坐标矩阵(relative_coords)，该矩阵的维度是$(Wh * Ww)\\times(Wh * Ww)$，如下图所示，最终每个位置的相对位置编码根据relative_coords的值当作索引从relative_position_bias_table中取得。关于相对位置编码可以结合下文的代码理解。\n代码结构 首先从model = build_model(config)进入模型构建的代码：\n1 2 3 4 5 6 if model_type == 'swin': model = SwinTransformer(cfg ...) elif model_type == 'swin_mlp': model = SwinMLP(cfg ...) 这里我们关注SwinTransformer的构建，swin_mlp将SwinTransformer中的Swin Transformer Block改为了mlp实现。\n我们看SwinTransformer的forward函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def forward_features(self, x): x = self.patch_embed(x) if self.ape: x = x + self.absolute_pos_embed x = self.pos_drop(x) for layer in self.layers: x = layer(x) x = self.norm(x) # B L C x = self.avgpool(x.transpose(1, 2)) # B C 1 x = torch.flatten(x, 1) # B C return x def forward(self, x): x = self.forward_features(x) # 分类头 self.head = nn.Linear(self.num_features,num_classes) x = self.head(x) return x 其中在forward_features函数中，特征x首先经过patch_embed，然后依次经过SwinTransformer的layer:\n1 2 for layer in self.layers: x = layer(x) 最后是送进分类头之前将特征reshape成$B \\times C$的维度：\n1 2 3 x = self.norm(x) # B L C x = self.avgpool(x.transpose(1, 2)) # B C 1 x = torch.flatten(x, 1) # B C 在这里再次贴上Swim Transformer的网络结构图(Swim-T):\n代码中patch_embed其实做了结构图中的Patch Partition和Linear Embedding的工作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class PatchEmbed(nn.Module): def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None): super().__init__() img_size = to_2tuple(img_size) patch_size = to_2tuple(patch_size) patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]] self.img_size = img_size self.patch_size = patch_size self.patches_resolution = patches_resolution self.num_patches = patches_resolution[0] * patches_resolution[1] self.in_chans = in_chans self.embed_dim = embed_dim # 论文中的Patch Partition，卷积核是patch_size(4*4), stride是4 # 注意这里顺便把论文中的Linear Embedding也做了，输出维度是96而不是48 self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) if norm_layer is not None: self.norm = norm_layer(embed_dim) else: self.norm = None def forward(self, x): B, C, H, W = x.shape # FIXME look at relaxing size constraints assert H == self.img_size[0] and W == self.img_size[1], \\ f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\" x = self.proj(x).flatten(2).transpose(1, 2) # B Ph*Pw C if self.norm is not None: x = self.norm(x) return x ... 再看self.layers:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # build layers self.layers = nn.ModuleList() for i_layer in range(self.num_layers): layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer), input_resolution=(patches_resolution[0] // (2 ** i_layer), patches_resolution[1] // (2 ** i_layer)), depth=depths[i_layer], num_heads=num_heads[i_layer], window_size=window_size, mlp_ratio=self.mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])], norm_layer=norm_layer, downsample=PatchMerging if (i_layer \u003c self.num_layers - 1) else None, use_checkpoint=use_checkpoint) self.layers.append(layer) 代码中downsample=PatchMerging if (i_layer \u003c self.num_layers - 1) else None，除了最后一层，前面每一层都添加downsample，也即上图中的Patch Merging操作。我们再看BasicLayer，它主要由SwinTransformerBlock和downsample组成(除了最后一层）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class BasicLayer(nn.Module): def __init__(self, dim, input_resolution, depth, num_heads, window_size, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False): super().__init__() self.dim = dim self.input_resolution = input_resolution self.depth = depth self.use_checkpoint = use_checkpoint # build blocks self.blocks = nn.ModuleList([ SwinTransformerBlock(dim=dim, input_resolution=input_resolution, num_heads=num_heads, window_size=window_size, shift_size=0 if (i % 2 == 0) else window_size // 2, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, attn_drop=attn_drop, drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path, norm_layer=norm_layer) for i in range(depth)]) # patch merging layer # 经过patch merging layer之后特征图分辨率讲到1/4，通道数提升2倍(不是4倍) if downsample is not None: self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer) else: self.downsample = None #这里是先经过len(depth)次SwinTransformerBlock再做patch merging，所以最后一层没有patch merging，和论文的示意图稍微不同 def forward(self, x): for blk in self.blocks: if self.use_checkpoint: x = checkpoint.checkpoint(blk, x) else: x = blk(x) if self.downsample is not None: x = self.downsample(x) return x ... 重点看SwinTransformerBlock:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class SwinTransformerBlock(nn.Module): def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm): super().__init__() self.dim = dim self.input_resolution = input_resolution self.num_heads = num_heads self.window_size = window_size self.shift_size = shift_size self.mlp_ratio = mlp_ratio if min(self.input_resolution) \u003c= self.window_size: # if window size is larger than input resolution, we don't partition windows self.shift_size = 0 self.window_size = min(self.input_resolution) assert 0 \u003c= self.shift_size \u003c self.window_size, \"shift_size must in 0-window_size\" self.norm1 = norm_layer(dim) self.attn = WindowAttention( dim, window_size=to_2tuple(self.window_size), num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop) #在进行残差连接之前，丢弃了一部分的参数 self.drop_path = DropPath(drop_path) if drop_path \u003e 0. else nn.Identity() self.norm2 = norm_layer(dim) mlp_hidden_dim = int(dim * mlp_ratio) self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop) if self.shift_size \u003e 0: # calculate attention mask for SW-MSA H, W = self.input_resolution img_mask = torch.zeros((1, H, W, 1)) # 1 H W 1 #列表切片 h_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) w_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) #做好标记(0到8) cnt = 0 for h in h_slices: for w in w_slices: img_mask[:, h, w, :] = cnt cnt += 1 #划分方式仍然按照(B, H, W, C)--\u003e(num_windows*B, window_size, window_size, C)的方式划分 mask_windows = window_partition(img_mask, self.window_size) # nW, window_size, window_size, 1 mask_windows = mask_windows.view(-1, self.window_size * self.window_size) #广播相减，得到的attn_mask，下面是第一个mask_windows的维度 # mask_windows.unsqueeze(1): torch.Size([64, 1, 49]) # mask_windows.unsqueeze(2): torch.Size([64, 49, 1]) # attn_mask: torch.Size([64, 49, 49]) #相当于拿每个元素与self.window_size * self.window_size的张量进行广播减，非零的地方填充-100.0 attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2) attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0)) else: attn_mask = None # mask作为一个常量数据存在 self.register_buffer(\"attn_mask\", attn_mask) def forward(self, x): H, W = self.input_resolution B, L, C = x.shape assert L == H * W, \"input feature has wrong size\" shortcut = x x = self.norm1(x) x = x.view(B, H, W, C) # cyclic shift # 用二维张量举例子，torch.roll(input, shifts, dims=None)的意思是将行向量(dims==0)的第一行移动到shifts的位置， # 或者将列向量(dims==1)的第一列移动到shifts的位置， # 把张量想象成一个循环数组，为了将目标向量移动到目标位置，需要将其他的向量依次循环滚动 # 如果shifts和dims是列表，则按顺序依次滚动，比如下面的代码，先执行shifts=-self.shift_size， dims=1，再执行shifts=-self.shift_size， dims=2 if self.shift_size \u003e 0: shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)) else: shifted_x = x # partition windows x_windows = window_partition(shifted_x, self.window_size) # nW*B, window_size, window_size, C x_windows = x_windows.view(-1, self.window_size * self.window_size, C) # nW*B, window_size*window_size, C # W-MSA/SW-MSA attn_windows = self.attn(x_windows, mask=self.attn_mask) # nW*B, window_size*window_size, C # merge windows attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C) shifted_x = window_reverse(attn_windows, self.window_size, H, W) # B H' W' C # reverse cyclic shift # 如果不做reverse，自注意力的计算会一直往图像右下角偏移 if self.shift_size \u003e 0: x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)) else: x = shifted_x x = x.view(B, H * W, C) # FFN x = shortcut + self.drop_path(x) x = x + self.drop_path(self.mlp(self.norm2(x))) return x SwinTransformerBlock类处理带有Shifted windows的自注意力计算和正常windows的自注意力计算。SwinTransformerBlock首先会对特征进行窗口划分partition windows，计算完窗口注意力后再做窗口合并merge windows：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # partition windows x_windows = window_partition(shifted_x, self.window_size) # nW*B, window_size, window_size, C x_windows = x_windows.view(-1, self.window_size * self.window_size, C) # nW*B, window_size*window_size, C ... # merge windows attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C) shifted_x = window_reverse(attn_windows, self.window_size, H, W) # B H' W' C ... def window_partition(x, window_size): \"\"\" Args: x: (B, H, W, C) window_size (int): window size Returns: windows: (num_windows*B, window_size, window_size, C) \"\"\" B, H, W, C = x.shape x = x.view(B, H // window_size, window_size, W // window_size, window_size, C) # .permute()交换维度顺序，.contiguous()保证内存上的连续 windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C) return windows def window_reverse(windows, window_size, H, W): \"\"\" Args: windows: (num_windows*B, window_size, window_size, C) window_size (int): Window size H (int): Height of image W (int): Width of image Returns: x: (B, H, W, C) \"\"\" B = int(windows.shape[0] / (H * W / window_size / window_size)) x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1) x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1) return x 这里的重点代码在于中间的窗口注意力WindowAttention的计算过程:\n1 2 3 4 5 6 self.attn = WindowAttention( dim, window_size=to_2tuple(self.window_size), num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop) ... `# W-MSA/SW-MSA attn_windows = self.attn(x_windows, mask=self.attn_mask) # nW*B, window_size*window_size, C` WindowAttention部分代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class WindowAttention(nn.Module): def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.): super().__init__() self.dim = dim self.window_size = window_size # Wh, Ww self.num_heads = num_heads head_dim = dim // num_heads self.scale = qk_scale or head_dim ** -0.5 # define a parameter table of relative position bias # 相对位置编码 self.relative_position_bias_table = nn.Parameter( torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)) # 2*Wh-1 * 2*Ww-1, nH # get pair-wise relative position index for each token inside the window coords_h = torch.arange(self.window_size[0]) coords_w = torch.arange(self.window_size[1]) coords = torch.stack(torch.meshgrid([coords_h, coords_w])) # 2, Wh, Ww coords_flatten = torch.flatten(coords, 1) # 2, Wh*Ww # 广播相减 2, Wh*Ww --\u003e 2, Wh*Ww, Wh*Ww # 一共Wh*Ww行，每一行代表一个坐标到其他Wh*Ww个坐标的差值 relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :] # 2, Wh*Ww, Wh*Ww relative_coords = relative_coords.permute(1, 2, 0).contiguous() # Wh*Ww, Wh*Ww, 2 relative_coords[:, :, 0] += self.window_size[0] - 1 # shift to start from 0 # 此时值从-(Wh-1)~(Wh-1) --\u003e 0~(2*Wh-1) relative_coords[:, :, 1] += self.window_size[1] - 1 # 此时值从-(Ww-1)~(Ww-1) --\u003e 0~(2*Ww-1) # 把relative_coords的第一个维度乘以(2*Ww-1) relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1 # relative_position_index = relative_coords.sum(-1) # Wh*Ww, Wh*Ww self.register_buffer(\"relative_position_index\", relative_position_index) self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) trunc_normal_(self.relative_position_bias_table, std=.02) self.softmax = nn.Softmax(dim=-1) def forward(self, x, mask=None): \"\"\" Args: x: input features with shape of (num_windows*B, N, C) mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None \"\"\" B_, N, C = x.shape qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) q = q * self.scale attn = (q @ k.transpose(-2, -1)) # 虽然relative_position_index的大小是Wh*Ww, Wh*Ww，但其值域在(2*Wh-1)*(2*Ww-1),所以维护一个(2*Wh-1)*(2*Ww-1)的table即可 # 意思是如果relative_position_index中相同的值，应该从relative_position_bias_table相同位置取值，这是相对位置编码的意思 relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view( self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1) # Wh*Ww,Wh*Ww,nH relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous() # nH, Wh*Ww, Wh*Ww # 广播加 attn = attn + relative_position_bias.unsqueeze(0) # SW-MSA if mask is not None: nW = mask.shape[0] attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0) attn = attn.view(-1, self.num_heads, N, N) attn = self.softmax(attn) # W-MSA else: attn = self.softmax(attn) attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B_, N, C) x = self.proj(x) x = self.proj_drop(x) return x ... 关于相对位置编码，代码中维护一个relative_position_bias_table和relative_coords，每个windows的相对位置编码relative_position_bias从中取值：\n1 2 3 4 5 6 relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view( self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1) # Wh*Ww,Wh*Ww,nH relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous() # nH, Wh*Ww, Wh*Ww # 广播加 attn = attn + relative_position_bias.unsqueeze(0) 对于SW-MSA，它比W-MSA多了一个cyclic shift和mask操作，cyclic shift的操作之后还有reverse cyclic shift操作，防止自注意力的计算会一直往图像右下角偏移：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # cyclic shift # 用二维张量举例子，torch.roll(input, shifts, dims=None)的意思是将行向量(dims==0)的第一行移动到shifts的位置， # 或者将列向量(dims==1)的第一列移动到shifts的位置， # 把张量想象成一个循环数组，为了将目标向量移动到目标位置，需要将其他的向量依次循环滚动 # 如果shifts和dims是列表，则按顺序依次滚动，比如下面的代码，先执行shifts=-self.shift_size， dims=1，再执行shifts=-self.shift_size， dims=2 if self.shift_size \u003e 0: shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)) else: shifted_x = x ... # reverse cyclic shift # 如果不做reverse，自注意力的计算会一直往图像右下角偏移 if self.shift_size \u003e 0: x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)) else: x = shifted_x mask的生成代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 if self.shift_size \u003e 0: # calculate attention mask for SW-MSA H, W = self.input_resolution img_mask = torch.zeros((1, H, W, 1)) # 1 H W 1 #列表切片 h_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) w_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) #做好标记(0到8) cnt = 0 for h in h_slices: for w in w_slices: img_mask[:, h, w, :] = cnt cnt += 1 #划分方式仍然按照(B, H, W, C)--\u003e(num_windows*B, window_size, window_size, C)的方式划分 mask_windows = window_partition(img_mask, self.window_size) # nW, window_size, window_size, 1 mask_windows = mask_windows.view(-1, self.window_size * self.window_size) #广播相减，得到的attn_mask，下面是第一个mask_windows的维度 # mask_windows.unsqueeze(1): torch.Size([64, 1, 49]) # mask_windows.unsqueeze(2): torch.Size([64, 49, 1]) # attn_mask: torch.Size([64, 49, 49]) #相当于拿每个元素与self.window_size * self.window_size的张量进行广播减，非零的地方填充-100.0 attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2) attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0)) else: attn_mask = None # mask作为一个常量数据存在 self.register_buffer(\"attn_mask\", attn_mask) attn_mask作为一个常量数据被注册。这样SwinTransformerBlock就走完了，对于一个BasicLayer，在SwinTransformerBlock之后还有downsample层，它负责将特征图降采样并提升通道维数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class PatchMerging(nn.Module): def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm): super().__init__() self.input_resolution = input_resolution self.dim = dim self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False) self.norm = norm_layer(4 * dim) def forward(self, x): \"\"\" x: B, H*W, C \"\"\" H, W = self.input_resolution B, L, C = x.shape assert L == H * W, \"input feature has wrong size\" assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\" x = x.view(B, H, W, C) # 采样间隔是2 x0 = x[:, 0::2, 0::2, :] # B H/2 W/2 C x1 = x[:, 1::2, 0::2, :] # B H/2 W/2 C x2 = x[:, 0::2, 1::2, :] # B H/2 W/2 C x3 = x[:, 1::2, 1::2, :] # B H/2 W/2 C x = torch.cat([x0, x1, x2, x3], -1) # B H/2 W/2 4*C x = x.view(B, -1, 4 * C) # B H/2*W/2 4*C x = self.norm(x) x = self.reduction(x) return x self.reduction将通道维数从$4C$降到$2C$，这样特征图通过downsample层后，通道维数只提高两倍。\n特征图经过多层BasicLayer后，最终被送进分类头，完成整个正向过程。\n更多关于Swin Transformer 可以看b站李沐的Swin Transformer论文精读3，代码讲解看知乎文章图解Swin Transformer4。\nSwin-Transformer github ↩︎\narxiv 链接 ↩︎\n李沐 Swin Transformer论文精读 ↩︎\n图解Swin Transformer ↩︎\n","title":"Swin Transformer总结","uri":"/contents/attention/swim_transformer/"},{"categories":["contents"],"content":"记录侯捷老师C++面向对象课程1的笔记。\n面向对象上篇 C++编程简介 当以类(class)来组织编写C++，可以把C++编写方式分为:Object Oriented(面向对象)和Object Based(基于对象)，其区别主要看类与类之间是否有关联(继承，复合，委托)。 单一类有包含指针的类和不包含指针的类，以此为区分的类有很大的不同。 C++由C++语言和C++标准库组成。 C++经典书籍推荐：《C++ Primer》、《The C++ programming language(C++11)》、《Effective C++》、《The standard library》、《STL 源码剖析》。 头文件与类的声明 C语言与C++的明显的不同在于，在C语言中，数据变量是暴露在所有处理函数面前的，而C++中，特定的数据由特定的函数体处理(由class，struct包装起来的变量和函数)。 在C++中引用C语言的库，#include\u003cstdio.h\u003e或者 #include\u003ccstdio\u003e都可以。 头文件的防卫式声明 1 2 3 4 #ifndef __COMPLEX__ #define __COMPLEX__ ... #endif 类的声明 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #ifndef __COMPLEX__ #define __COMPLEX__ template\u003ctypename T\u003e class complex { public: complex (T r = 0, T i = 0) :re(r), im(i){} T real () const {return re;} T imag () const {return im;} private: T re, im; friend complex\u0026 __doapl__ (complex*, const complex\u0026); } #endif 模板类的实例化：\n1 2 complex\u003cdouble\u003e c1(2.5, 1.5); complex\u003cint\u003e c2(2,6); 在这里，使用模板template的好处在于：不需要为不同类型double、int等单独定义不同的复数类，而这些类的区别仅仅在于数据类型不同而已。\n构造函数 inline（内联）函数：若函数在class内定义完成，则成为inline函数，inline函数相较于一般函数，编译速度更快，但如果函数体过于复杂，编译器实际上不会把它当作inline函数。简洁来说，我们在函数前面加inline关键字，只不过是对编译器的建议而已，该函数最终不一定是inline函数。 大部分的函数访问级别标记为public，而所有的类的数据的访问级别标记为private。 类的构造函数写法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class complex { public: //构造函数的名称与类的名称相同，没有返回类型 complex (double r = 0, double i = 0) //默认实参 :re(r), im(i) //initialization list(初值列表)：如果构造时引入参数，则使用该参数为类的变量赋值 {} //构造函数的函数体 complex\u0026 operator += (const complex\u0026); double real () const {return re;} double imag () const {return im;} private: double re, im; friend complex\u0026 __doapl__ (complex*, const complex\u0026); } 使用initialization list(初值列表)为类内变量赋值效率会更高，因为为一个变量赋值分为初始化和赋值(即{}内完成的部分)两个阶段，initialization list会在初始化阶段直接为变量赋值，省了一步。\n不带指针的类多半不需要写析构函数。 构造函数可以有很多个–overloading（C++允许函数重载，不仅仅限于构造函数） 但需要注意下面的例子，这两个构造函数却不可以共存。 1 2 3 4 5 complex(double r = 0, double i = 0) //该构造函数有默认参数 :re(r), im(i) {} complex() : re(0), im(0) {} //上面的构造函数有默认参数，导致该构造函数不能成功构建， //因为对于一个例子：conplex C;该实例化会让编译器无法决定使用哪一个， //因为上面两个构造函数都是相同的效果 参数传递与返回值 构造函数放在private作用域：不允许随意外界创建该类的实例。(singleton) const：当确保函数不改变数据的内容，请加const。 1 2 double real () const {return re;} double imag () const {return im;} 尽量使用引用传递参数(pass by reference)。 常量引用：const conplex\u0026, 引用保证参数传递很快，常量const保证参数不可修改。 尽量使用引用返回函数值(return by reference)。 相同class的各个objects互为友元(friends) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class complex { public: //构造函数的名称与类的名称相同，没有返回类型 complex (double r = 0, double i = 0) //默认实参 :re(r), im(i) {} int func(const complex\u0026 param){ return param.re + param.im; } private: double re, im; } 当返回的变量来自于一个局部变量的赋值，这种情况不可以返回引用，因为局部变量超出函数作用域后就会消亡。 操作符重载与临时对象 可以认为操作符就是函数 操作符重载–写成成员函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 传递者无需知道接收者是否以 reference形式接收参数(reference的另外一个好处) inline complex\u0026 __doapl(complex* ths, const complex\u0026 r) { ths-\u003ere += r.re; ths-\u003eim += r.im; return *ths; } #所有的成员函数都有一个隐藏的this #之所以这里不能是void 是考虑到：a+=b+=c 这种用法 inline complex\u0026 complex::operator += (const complex\u0026 r) { return __doapl(this, r); # this在这里显式调用 } 操作符重载–写成非成员函数，它是全局函数，没有this指针，必须显示调用对象的实例 临时对象的返回不可以返回引用 1 2 3 4 5 inline complex conj (const complex\u0026 x) { return complex (real(x), -imag(x)); } 重载 « 时只能用全局的非成员函数的形式 1 2 3 4 5 6 7 #include \u003ciostream.h\u003e # 返回类型不是void 是考虑到 cout\u003c\u003cc1\u003c\u003cc2;的形式 ostream\u0026 operator \u003c\u003c (ostream\u0026 os, const complex\u0026 x) { return os \u003c\u003c '(' \u003c\u003c real(x)\u003c\u003c','\u003c\u003cimag(x)\u003c\u003c')'; } 百度网盘链接，提取码：kb9y ↩︎\n","title":"C++面向对象总结","uri":"/contents/c++/oop_sumary/"},{"categories":["contents"],"content":"还有三个月就得正式工作了，计算机图形学和C++要学起来！\n计算机图形学 计算机图形学入门学习路线 可以先看一些概括性的书《Fundamentals of Computer Graphics 4th Edition》，结合闫令琪老师1的GAMES1012还有官网代码来学习，对计算机图形学有一个大概的把握。《实时渲染》可以先看毛星云同志3的《Real-Time Rendering 3rd》提炼总结，对里面的概念有基本的了解，就可以去啃第四版的英文版了，不太理解的地方，可以结合网上第四版的解读来学习4。\n另外可以在LearnOpenGL CN5上学习现在opengl的知识，了解虚幻引擎6的基础概念。\n计算机图形学进阶 根据工作内容和个人兴趣选择，对于各个子领域的拓展，毛星云有一个仓库7收集了 各个子领域的拓展论文。\nC++ 侯捷老师的C++教程8值得反复看，关键是侯老师长得帅。有空可以刷Cherno的C++视频9。\n最后，从项目实践中学习语言，多思考和总结记录。\n链接 闫令琪主页 ↩︎\nGAMES101 ↩︎\n毛星云-知乎 ↩︎\nReal-Time Rendering 4rd 文章解读 ↩︎\nLearnOpenGL CN ↩︎\n虚幻引擎 ↩︎\nReal-Time-Rendering-4th-Bibliography-Collection ↩︎\n侯捷老师的c++ ↩︎\nCherno的C++视频 ↩︎\n","title":"计算机图形学和C++资源整合","uri":"/contents/day-day-up/cg_and_cpp/"},{"categories":["contents"],"content":"记录几种常用的排序算法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 #include\u003ciostream\u003e #include\u003cvector\u003e using namespace std; /** *各种搜索方法 **/ //*****************************插入排序*************************************// //直接插入排序,稳定排序 //最好O(n),最坏O(n^2),平均O(n^2) void insertSort(vector\u003cint\u003e \u0026raw_arr){ int n = int(raw_arr.size()); if(n==0) return; for(int i = 1;i\u003cn;++i){ //保存当前元素 int temp = raw_arr[i]; int end = i-1; //大的元素往后挪 while(end\u003e=0 \u0026\u0026 raw_arr[end]\u003etemp){ raw_arr[end+1] = raw_arr[end]; end--; } raw_arr[end+1] = temp; } } //*****************************希尔排序*************************************// //希尔排序,不稳定排序 //最好O(N),最坏O(N^2),平均O(N^1.3) void shellSort(vector\u003cint\u003e \u0026raw_arr){ int n = int(raw_arr.size()); int step = n; while(step\u003e1){ step = step/3 + 1; for(int i = 0;i\u003cn-step;++i){ int temp = raw_arr[i+step]; int end = i; while(end\u003e=0 \u0026\u0026 raw_arr[end]\u003etemp){ raw_arr[end+step] = raw_arr[end]; end-=step; } raw_arr[end+step] = temp; } } } //*****************************选择排序*************************************// //选择排序,不稳定排序 (每次循环找到最大值和最小值，然后缩小begin 和 end的范围) //最好O(N^2),最坏O(N^2),平均O(N^2) void selectSort(vector\u003cint\u003e \u0026raw_arr){ int n = int(raw_arr.size()); int begin = 0, end = n-1; while(begin\u003c=end){ int min_idx = begin, max_idx = end; for(int i = begin;i\u003c=end;++i){ if(raw_arr[i]\u003craw_arr[min_idx]) min_idx = i; if(raw_arr[i]\u003eraw_arr[max_idx]) max_idx = i; } swap(raw_arr[begin], raw_arr[min_idx]); //注意这个条件，因为存在最大值在“begin”处的可能性 if(max_idx == begin) max_idx = min_idx; swap(raw_arr[end], raw_arr[max_idx]); begin++; end--; } } //*****************************堆排序*************************************// void heapAjust(vector\u003cint\u003e \u0026raw_arr, int root, int n){ int child = 2 * root + 1; while(child\u003cn){ //寻找值最大的叶子节点 if(child+1\u003cn \u0026\u0026 raw_arr[child]\u003craw_arr[child+1]) child++; //由于从最后一个非叶子节点开始遍历，当满足该条件时，一定保证该子堆是最大堆了(因为更小的子堆在之前就处理好了) if(raw_arr[root]\u003eraw_arr[child]) break; swap(raw_arr[root], raw_arr[child]); //下一级 root = child; child = 2 * root + 1; } } //堆排序，不稳定排序 //最好O(N*log(N)),最坏O(N*log(N)),平均O(N*log(N)) void heapSort(vector\u003cint\u003e \u0026raw_arr){ int n = int(raw_arr.size()); //首先建一次大堆 //i=n/2 - 1是最后一个非叶子节点 for(int i = n/2 - 1;i\u003e=0;--i){ heapAjust(raw_arr, i, n); } int end = n-1; //把最大值交换到数组尾部，再执行n-1次heapAjust while(end\u003e0){ swap(raw_arr[0], raw_arr[end]); //由于索引0处不满足最大堆定义，需要调整 heapAjust(raw_arr, 0, end); end--; } } //*****************************冒泡排序*************************************// //冒泡排序,稳定排序 //最好O(N),最坏O(N^2),平均O(N^2) void bubbleSort(vector\u003cint\u003e \u0026raw_arr){ int n = int(raw_arr.size()); int end = n-1; while(end\u003e0){ for(int i = 0;i\u003c=end;++i){ if(i\u003e0 \u0026\u0026 raw_arr[i]\u003craw_arr[i-1]) swap(raw_arr[i], raw_arr[i-1]); } end--; } } //*****************************快速排序*************************************// int partion(vector\u003cint\u003e \u0026raw_arr, int begin, int end){ if(begin\u003e=end) return begin; //小技巧：如果key定在end, 要先递增begin;反之也成立。 //注意raw_arr[begin]\u003c=raw_arr[key]的“\u003c=”号，不可以写成\"\u003c\"，否则对于有重复数字的排序会出错。 int key = end; while(begin\u003cend){ while(begin\u003cend \u0026\u0026 raw_arr[begin]\u003c=raw_arr[key]) begin++; while(begin\u003cend \u0026\u0026 raw_arr[end]\u003e=raw_arr[key]) end--; swap(raw_arr[begin], raw_arr[end]); } swap(raw_arr[begin], raw_arr[key]); return end; } void _quickSort(vector\u003cint\u003e \u0026raw_arr, int s_idx, int e_idx){ if(s_idx \u003c e_idx){ int part = partion(raw_arr, s_idx, e_idx); _quickSort(raw_arr, s_idx, part-1); _quickSort(raw_arr, part+1, e_idx); } } //快速排序,不稳定排序 //最好O(N*log(N)),,最坏O(N^2),平均O(N*log(N)) void quickSort(vector\u003cint\u003e \u0026raw_arr){ int n = int(raw_arr.size()); _quickSort(raw_arr, 0, n-1); } //*****************************归并排序*************************************// void _mergeSort(vector\u003cint\u003e \u0026raw_arr, int *temp, int begin, int end){ if(begin\u003e=end) return; //注意右移符号的优先级很低，需要用()括起来 int mid = begin + ((end-begin)\u003e\u003e1); _mergeSort(raw_arr, temp, begin, mid); _mergeSort(raw_arr, temp, mid+1, end); //合并 int begin1 = begin, end1 = mid; int begin2 = mid+1, end2 = end; int temp_s_idx = begin; while(begin1\u003c=end1 \u0026\u0026 begin2\u003c=end2){ if(raw_arr[begin1]\u003craw_arr[begin2]) temp[begin++] = raw_arr[begin1++]; else { temp[begin++] = raw_arr[begin2++]; } } while(begin1\u003c=end1){ temp[begin++] = raw_arr[begin1++]; } while(begin2\u003c=end2){ temp[begin++] = raw_arr[begin2++]; } //复制到原来的数组 begin = temp_s_idx; while(begin\u003c=end){ raw_arr[begin] = temp[begin]; begin++; } } //归并排序,稳定排序 //最好O(N*log(N)),,最坏O(N*log(N)),平均O(N*log(N)) void mergeSort(vector\u003cint\u003e \u0026raw_arr){ int n = int(raw_arr.size()); int *temp = new int[n]; _mergeSort(raw_arr, temp, 0, n-1); delete[] temp; } int main(){ vector\u003cint\u003e raw_arr = {3,3,15,4,1,3,24,8,45,7,1,65,9,17}; // insertSort(raw_arr); // shellSort(raw_arr); // selectSort(raw_arr); // heapSort(raw_arr); // bubbleSort(raw_arr); // quickSort(raw_arr); // mergeSort(raw_arr); for(int ra:raw_arr){ cout\u003c\u003cra\u003c\u003c\" \"; } cout\u003c\u003cendl; return 0; } ","title":"排序算法总结","uri":"/contents/c++/sort_sumary/"},{"categories":["contents"],"content":"要经常参考：TensorRT开发者文档\nTensorRT 介绍 下载TensorRT:\n从 这个链接下载tar包后解压，添加环境变量：\n1 2 3 4 vim ~/.bashrc # 添加以下内容 export LD_LIBRARY_PATH=/path/to/TensorRT-7.2.3.4/lib:$LD_LIBRARY_PATH export LIBRARY_PATH=/path/to/TensorRT-7.2.3.4/lib::$LIBRARY_PATH 具体参考内卷成啥了还不知道TensorRT？超详细入门指北，来看看吧！\nTensorRT的工作流程分为五个步骤：\n导出模型–\u003e选择推断的batch size–\u003e选择精度–\u003e转换模型–\u003e部署模型\n要将pytorch模型转为trt模型有两个方法：\nautomatic ONNX conversion from .onnx files 当遇到trt不支持的op时，需要手写插件。\npytorch模型转换为onnx模型:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import torch import torchvision torch_model_path = \"./checkpoints/res50.pth\" export_onnx_path = \"./checkpoints/model.onnx\" device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") torch_model = torch.load(torch_model_path) model = torchvision.models.resnet50() model.load_state_dict(torch_model) #set the model to inference mode model.eval().to(device) batch_size = 1 #批处理大小 input_shape = (3, 64, 64) #输入数据,改成自己的输入shape x = torch.randn(batch_size, *input_shape).to(device)\t# 生成张量 y = model(x) print(\"y shape: \",y.shape) torch.onnx.export(model, # 定义的模型 x, # 输入数据 export_onnx_path, # 导出路径 opset_version=13, #opset_version和onnx版本有对应关系 do_constant_folding=True,\t# 是否执行常量折叠优化 input_names=[\"input\"],\t# 输入名 output_names=[\"output\"],\t# 输出名 verbose = True, # 开启 verbose方便调试 ) 可以利用netron来可视化导出的onnx模型。\npip install netron netron 注意输入输出的名称。\n然后利用onnxruntime验证导出模型是不是正确的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import onnx import numpy as np import onnxruntime as rt model_path = \"./checkpoints/model.onnx\" # 验证模型合法性 onnx_model = onnx.load(model_path) onnx.checker.check_model(onnx_model) # 设置模型session以及输入信息 sess = rt.InferenceSession(model_path, None) input_name = sess.get_inputs()[0].name output_name = sess.get_outputs()[0].name print('Input Name:', input_name) print('Output Name:', output_name) # 输入数据并调整为输入维度 x = np.random.rand(1, 3, 64, 64).astype(np.float32) output = sess.run(None, {input_name: x}) print(output) onnx模型转为trt模型：\n1 trtexec --onnx=path_to/model.onnx --saveEngine=path_to/resnet_engine.trt --explicitBatch 上面的命令将onnx转为trt模型并且序列化到文件中了。后续可以调用python或者c++api来反序列化.trt文件转为runtime模型，速度会比先解析onnx模型为trt模型再运行要快。\n如果要支持动态尺度，这样写：\n1 ./trtexec --explicitBatch --onnx=demo.onnx --minShapes=input:1x1x256x256 --optShapes=input:1x1x2048x2048 --maxShapes=input:1x1x2560x2560 --shapes=input:1x1x2048x2048 --saveEngine=demo.trt --workspace=6000 动态尺度支持NCHW中的N、H以及W，也就是batch、高以及宽。 对于动态模型，我们在转换模型的时候需要额外指定三个维度信息即可(最小、最优、最大)。\nTensorRT 对网络模型做了算子融合、动态显存分配、精度校准、多steam流、自动调优等操作，在很多模型上推理速度大大提升。\nmanually constructing a network using the TensorRT API (either in C++ or Python) 第二种方法更灵活(当然也更难), 这个方法我还在探索中。\nTensorRT工作流程 不管是借助onnx通过parser转为到trt模型，还是手工通过API搭建网络加载参数，整个运行流程可以分为下面几个步骤：\n创建builder(build) 1 IBuilder* builder = createInferBuilder(gLogger); gLogger通过下面的方式来创建：\n1 2 3 4 5 6 7 8 9 class Logger : public ILogger { void log(Severity severity, const char* msg) override { // suppress info-level messages if (severity != Severity::kINFO) std::cout \u003c\u003c msg \u003c\u003c std::endl; } } gLogger; 创建 engine 1 2 3 4 IBuilderConfig* config = builder-\u003ecreateBuilderConfig(); //这里的空间大小需要根据具体需求设置 config-\u003esetMaxWorkspaceSize(1 \u003c\u003c 20); ICudaEngine* engine = builder-\u003ebuildEngineWithConfig(*network, *config); 其中的network可以通过parser或者手动装载的方式实现， 比如通过onnx：\n1 2 3 4 5 6 7 8 9 const auto explicitBatch = 1U \u003c\u003c static_cast\u003cuint32_t\u003e(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); INetworkDefinition* network = builder-\u003ecreateNetworkV2(explicitBatch); nvonnxparser::IParser* parser = nvonnxparser::createParser(*network, gLogger); parser-\u003eparseFromFile(onnx_filename, ILogger::Severity::kWARNING); for (int i = 0; i \u003c parser.getNbErrors(); ++i) { std::cout \u003c\u003c parser-\u003egetError(i)-\u003edesc() \u003c\u003c std::endl; } 或者手动装载：\n1 2 3 4 5 6 7 8 9 INetworkDefinition* m_network = builder-\u003ecreateNetworkV2(1U \u003c\u003c static_cast\u003cuint32_t\u003e(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH)); // tensor nvinfer1::ITensor *input = m_network-\u003eaddInput(\"data\",nvinfer1::DataType::kFLOAT, nvinfer1::DimsCHW{static_cast\u003cint\u003e(input_c), static_cast\u003cint\u003e(input_h), static_cast\u003cint\u003e(input_w)}); Layers[\"input\"] = input; //...很长的网络定义 //... //... Layers[\"relu_eng\"] -\u003esetName(\"output1\"); m_network-\u003emarkOutput(*Layers[\"relu_eng\"]); 序列化到磁盘下(这一步不是必须的) 1 2 3 4 5 6 7 8 9 10 nvinfer1::IHostMemory *serializedModel = engine-\u003eserialize(); ofstream engFile; engFile.open(engPath,ios_base::binary); engFile.write(static_cast\u003cconst char*\u003e(serializedModel-\u003edata()),serializedModel-\u003esize()); engFile.close(); ... network-\u003edestroy(); engine-\u003edestroy(); builder-\u003edestroy(); serializedModel-\u003edestroy(); 反序列化(这一步也不是必须的) 1 2 IRuntime* runtime = createInferRuntime(gLogger); ICudaEngine* engine = runtime-\u003edeserializeCudaEngine(modelData, modelSize, nullptr); 执行推断(infer) 1 2 3 4 5 6 7 // 创建 RAII buffer 用于管理数据对象 samplesCommon::BufferManager buffers(mEngine); //context用于保存网络定义和训练参数等 IExecutionContext *context = engine-\u003ecreateExecutionContext(); //管理输入数据的buffer:输入数据拷贝到buffer中方便管理 ... context-\u003eexecuteV2(buffers.getDeviceBindings().data()); TensorRT SampleOnnxMNIST 解读 首先编写CMakeLists.txt:\ncmake_minimum_required(VERSION 2.8) project(sampleOnnxMNIST) set(CMAKE_CXX_FLAGS \"${CAMKE_CXX_FLAGS} -std=c++11 -pthread\") find_package(CUDA REQUIRED) include_directories(/usr/local/cuda/include) link_directories(/usr/local/cuda/lib64) include_directories(../common) aux_source_directory(../common SRC_LIST) # tensorrt include_directories(/home/jiajie/Learn/TensorRT-7.2.1.61/include/) link_directories(/home/jiajie/Learn/TensorRT-7.2.1.61/lib/) find_package(OpenCV) include_directories(OpenCV_INCLUDE_DIRS) link_libraries(\"/home/jiajie/Learn/TensorRT-7.2.1.61/targets/x86_64-linux-gnu/lib/libnvonnxparser.so\") link_libraries(\"/home/jiajie/Learn/TensorRT-7.2.1.61/targets/x86_64-linux-gnu/lib/libnvcaffe_parser.so\") add_executable(sampleOnnxMNIST ${PROJECT_SOURCE_DIR}/sampleOnnxMNIST.cpp ${SRC_LIST}) target_link_libraries(sampleOnnxMNIST nvinfer) target_link_libraries(sampleOnnxMNIST cudart) target_link_libraries(sampleOnnxMNIST ${OpenCV_LIBS}) SampleOnnxMNIST解读 按照上面TensorRT工作流程， SampleOnnxMNIST只是在这基础上加了很多logging的代码还有代码稳健性的检查，同时没有上面步骤中的序列化到磁盘的步骤。\n首先看main()函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 int main(int argc, char** argv) { //1. 传参和gLogger，先不用扣细节 samplesCommon::Args args; bool argsOK = samplesCommon::parseArgs(args, argc, argv); if (!argsOK) { sample::gLogError \u003c\u003c \"Invalid arguments\" \u003c\u003c std::endl; printHelpInfo(); return EXIT_FAILURE; } if (args.help) { printHelpInfo(); return EXIT_SUCCESS; } auto sampleTest = sample::gLogger.defineTest(gSampleName, argc, argv); sample::gLogger.reportTestStart(sampleTest); //2. 创建SampleOnnxMNIST对象并初始化参数 SampleOnnxMNIST sample(initializeSampleParams(args)); sample::gLogInfo \u003c\u003c \"Building and running a GPU inference engine for Onnx MNIST\" \u003c\u003c std::endl; //3. 构建trt引擎 if (!sample.build()) { return sample::gLogger.reportFail(sampleTest); } //执行推断 if (!sample.infer()) { return sample::gLogger.reportFail(sampleTest); } return sample::gLogger.reportPass(sampleTest); } 第二步的initializeSampleParams定义了一些外部参数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 //! //! \\brief Initializes members of the params struct using the command line args //! samplesCommon::OnnxSampleParams initializeSampleParams(const samplesCommon::Args\u0026 args) { samplesCommon::OnnxSampleParams params; if (args.dataDirs.empty()) //!\u003c Use default directories if user hasn't provided directory paths { params.dataDirs.push_back(\"data/mnist/\"); params.dataDirs.push_back(\"data/samples/mnist/\"); params.dataDirs.push_back(\"/home/jiajie/baidunetdiskdownload/Find_a_job/Learn_tensorRT/practice/data/mnist/\"); } else //!\u003c Use the data directory provided by the user { params.dataDirs = args.dataDirs; } params.onnxFileName = \"mnist.onnx\"; params.inputTensorNames.push_back(\"Input3\"); params.outputTensorNames.push_back(\"Plus214_Output_0\"); params.dlaCore = args.useDLACore; params.int8 = args.runInInt8; params.fp16 = args.runInFp16; return params; } 再看第三步build就很清晰了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 //! //! \\brief Creates the network, configures the builder and creates the network engine //! //! \\details This function creates the Onnx MNIST network by parsing the Onnx model and builds //! the engine that will be used to run MNIST (mEngine) //! //! \\return Returns true if the engine was created successfully and false otherwise //! bool SampleOnnxMNIST::build() { //创建builder auto builder = SampleUniquePtr\u003cnvinfer1::IBuilder\u003e(nvinfer1::createInferBuilder(sample::gLogger.getTRTLogger())); if (!builder) { return false; } const auto explicitBatch = 1U \u003c\u003c static_cast\u003cuint32_t\u003e(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); //创建network auto network = SampleUniquePtr\u003cnvinfer1::INetworkDefinition\u003e(builder-\u003ecreateNetworkV2(explicitBatch)); if (!network) { return false; } auto config = SampleUniquePtr\u003cnvinfer1::IBuilderConfig\u003e(builder-\u003ecreateBuilderConfig()); if (!config) { return false; } auto parser = SampleUniquePtr\u003cnvonnxparser::IParser\u003e(nvonnxparser::createParser(*network, sample::gLogger.getTRTLogger())); if (!parser) { return false; } //用parser的方式构造网络 auto constructed = constructNetwork(builder, network, config, parser); if (!constructed) { return false; } //buildEngineWithConfig mEngine = std::shared_ptr\u003cnvinfer1::ICudaEngine\u003e( builder-\u003ebuildEngineWithConfig(*network, *config), samplesCommon::InferDeleter()); if (!mEngine) { return false; } assert(network-\u003egetNbInputs() == 1); mInputDims = network-\u003egetInput(0)-\u003egetDimensions(); assert(mInputDims.nbDims == 4); assert(network-\u003egetNbOutputs() == 1); mOutputDims = network-\u003egetOutput(0)-\u003egetDimensions(); assert(mOutputDims.nbDims == 2); return true; } 其中的constructNetwork是：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 //! //! \\brief Uses a ONNX parser to create the Onnx MNIST Network and marks the //! output layers //! //! \\param network Pointer to the network that will be populated with the Onnx MNIST network //! //! \\param builder Pointer to the engine builder //! bool SampleOnnxMNIST::constructNetwork(SampleUniquePtr\u003cnvinfer1::IBuilder\u003e\u0026 builder, SampleUniquePtr\u003cnvinfer1::INetworkDefinition\u003e\u0026 network, SampleUniquePtr\u003cnvinfer1::IBuilderConfig\u003e\u0026 config, SampleUniquePtr\u003cnvonnxparser::IParser\u003e\u0026 parser) { auto parsed = parser-\u003eparseFromFile(locateFile(mParams.onnxFileName, mParams.dataDirs).c_str(), static_cast\u003cint\u003e(sample::gLogger.getReportableSeverity())); if (!parsed) { return false; } config-\u003esetMaxWorkspaceSize(16_MiB); if (mParams.fp16) { config-\u003esetFlag(BuilderFlag::kFP16); } if (mParams.int8) { config-\u003esetFlag(BuilderFlag::kINT8); samplesCommon::setAllTensorScales(network.get(), 127.0f, 127.0f); } samplesCommon::enableDLA(builder.get(), config.get(), mParams.dlaCore); return true; } 再看第四步infer：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 //! //! \\brief Runs the TensorRT inference engine for this sample //! //! \\details This function is the main execution function of the sample. It allocates the buffer, //! sets inputs and executes the engine. //! bool SampleOnnxMNIST::infer() { // 创建 RAII buffer 管理数据对象 samplesCommon::BufferManager buffers(mEngine); auto context = SampleUniquePtr\u003cnvinfer1::IExecutionContext\u003e(mEngine-\u003ecreateExecutionContext()); if (!context) { return false; } // Read the input data into the managed buffers assert(mParams.inputTensorNames.size() == 1); if (!processInput(buffers)) { return false; } // Memcpy from host input buffers to device input buffers buffers.copyInputToDevice(); bool status = context-\u003eexecuteV2(buffers.getDeviceBindings().data()); if (!status) { return false; } // Memcpy from device output buffers to host output buffers buffers.copyOutputToHost(); // Verify results if (!verifyOutput(buffers)) { return false; } return true; } 其中processInput是：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //! //! \\brief Reads the input and stores the result in a managed buffer //! bool SampleOnnxMNIST::processInput(const samplesCommon::BufferManager\u0026 buffers) { const int inputH = mInputDims.d[2]; const int inputW = mInputDims.d[3]; // Read a random digit file srand(unsigned(time(nullptr))); std::vector\u003cuint8_t\u003e fileData(inputH * inputW); mNumber = rand() % 10; readPGMFile(locateFile(std::to_string(mNumber) + \".pgm\", mParams.dataDirs), fileData.data(), inputH, inputW); // Print an ascii representation sample::gLogInfo \u003c\u003c \"Input:\" \u003c\u003c std::endl; for (int i = 0; i \u003c inputH * inputW; i++) { sample::gLogInfo \u003c\u003c (\" .:-=+*#%@\"[fileData[i] / 26]) \u003c\u003c (((i + 1) % inputW) ? \"\" : \"\\n\"); } sample::gLogInfo \u003c\u003c std::endl; float* hostDataBuffer = static_cast\u003cfloat*\u003e(buffers.getHostBuffer(mParams.inputTensorNames[0])); for (int i = 0; i \u003c inputH * inputW; i++) { hostDataBuffer[i] = 1.0 - float(fileData[i] / 255.0); } return true; } 其中verifyOutput是：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 //! //! \\brief Classifies digits and verify result //! //! \\return whether the classification output matches expectations //! bool SampleOnnxMNIST::verifyOutput(const samplesCommon::BufferManager\u0026 buffers) { const int outputSize = mOutputDims.d[1]; float* output = static_cast\u003cfloat*\u003e(buffers.getHostBuffer(mParams.outputTensorNames[0])); float val{0.0f}; int idx{0}; // Calculate Softmax float sum{0.0f}; for (int i = 0; i \u003c outputSize; i++) { output[i] = exp(output[i]); sum += output[i]; } sample::gLogInfo \u003c\u003c \"Output:\" \u003c\u003c std::endl; for (int i = 0; i \u003c outputSize; i++) { output[i] /= sum; val = std::max(val, output[i]); if (val == output[i]) { idx = i; } sample::gLogInfo \u003c\u003c \" Prob \" \u003c\u003c i \u003c\u003c \" \" \u003c\u003c std::fixed \u003c\u003c std::setw(5) \u003c\u003c std::setprecision(4) \u003c\u003c output[i] \u003c\u003c \" \" \u003c\u003c \"Class \" \u003c\u003c i \u003c\u003c \": \" \u003c\u003c std::string(int(std::floor(output[i] * 10 + 0.5f)), '*') \u003c\u003c std::endl; } sample::gLogInfo \u003c\u003c std::endl; return idx == mNumber \u0026\u0026 val \u003e 0.9f; } 精度比较 通常用下面的公式做精度比较：\n1 2 3 4 5 y = model(x) y_onnx = model_onnx(x) # check the output against PyTorch print(torch.max(torch.abs(y - y_trt))) TensorRT自定义算子 文章实现TensorRT自定义插件(plugin)自由！介绍的很详细。\n如何添加自定义算子 可以模仿官方的plugin库，添加自己的plugin后再相应的修改CMakeLists.txt，重新编译libnvinfer_plugin.so.7 模仿官方plugin库的层级结构，生成一个新的.so，然后在自己的工程中调用它。 Example: Adding A Custom Layer Using C++ 要创建一个plugin自定义类， 必须继承IPluginV2Ext，IPluginV2IOExt，IPluginV2DynamicExt中的一个，并且重写其中的虚函数。 1 2 3 4 class FooPlugin : public IPluginV2IOExt { ...override all pure virtual methods of IPluginV2IOExt with definitions for your plugin. Do not override the TRT_DEPRECATED methods. }; 要实现一个工厂类FooPluginCreator,用于创建FooPlugin的实例 1 class MyCustomPluginCreator : public BaseCreator IPluginCreator is a creator class for custom layers using which users can get plugin name, version, and plugin field parameters. It also provides methods to create the plugin object during the network build phase and deserialize it during inference.\nNote: In versions of TensorRT prior to 6.0.1, you derived custom layers from IPluginV2 or IPluginV2Ext. While these APIs are still supported, we highly encourage you to move to IPluginV2IOExt or IPluginV2DynamicExt to be able to use all the new plugin functionalities.\nTensorRT also provides the ability to register a plugin by calling REGISTER_TENSORRT_PLUGIN(pluginCreator) which statically registers the Plugin Creator to the Plugin Registry. During runtime, the Plugin Registry can be queried using the extern function getPluginRegistry(). The Plugin Registry stores a pointer to all the registered Plugin Creators and can be used to look up a specific Plugin Creator based on the plugin name and version.\n注册 然后通过REGISTER_TENSORRT_PLUGIN(pluginCreator)注册plugin（下文会涉及）.\nNote: To use TensorRT registered plugins in your application, the libnvinfer_plugin.so library must be loaded and all plugins must be registered. This can be done by calling initLibNvInferPlugins(void* logger, const char* libNamespace)() in your application code.\n或者通过initLibNvInferPlugins(void* logger, const char* libNamespace)()来注册。（注册的方法二选一）\n调用 Using the Plugin Creator, the IPluginCreator::createPlugin() function can be called which returns a plugin object of type IPluginV2. This object can be added to the TensorRT network using addPluginV2() which creates and adds a layer to a network and then binds the layer to the given plugin. The method also returns a pointer to the layer (of type IPluginV2Layer), which can be used to access the layer or the plugin itself (via getPlugin()).\nExample: Adding A Custom Layer With Dynamic Shape Support Using C++(动态shape) 与静态shape类似，需要重写IPluginV2DynamicExt中的虚函数\n1 2 3 4 class BarPlugin : public IPluginV2DynamicExt { ...override virtual methods inherited from IPluginV2DynamicExt. }; 一个plugin的例子 在main函数的网络定义中，加入plugin:\n1 2 3 4 5 6 7 8 //网络的定义 ... //添加plugin AddPlugin addPlugin(Weights{DataType::kFLOAT, \u0026valueToAdd, 1}); ITensor *aInputTensor[] = {tensor}; //调用 tensor = network-\u003eaddPluginV2(aInputTensor, 1, addPlugin)-\u003egetOutput(0); network-\u003emarkOutput(*tensor); 在AddPlugin.h中实现AddPlugin : public nvinfer1::IPluginV2IOExt和它的工厂类AddPluginCreator : public nvinfer1::IPluginCreator\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 #include \"NvInfer.h\" #include \u003ciostream\u003e #include \u003ccstring\u003e #include \u003cassert.h\u003e using namespace std; class AddPlugin: public nvinfer1::IPluginV2IOExt { public: //创建该插件时调用的构造函数，需要传递权重信息以及参数 AddPlugin(nvinfer1::Weights valueToAdd) { m.valueToAdd = *(float *)valueToAdd.values; } //可以在反序列化时，调用这个构造函数 AddPlugin(const void *buffer, size_t length) { memcpy(\u0026m, buffer, sizeof(m)); } //返回序列化时需要写多少字节到buffer中。 virtual size_t getSerializationSize() const override { return sizeof(m); } //序列化 virtual void serialize(void *buffer) const override { memcpy(buffer, \u0026m, sizeof(m)); } //拷贝函数,将这个plugin对象克隆一份给TensorRT的builder、network或者engine。(该方法会调用拷贝构造函数) nvinfer1::IPluginV2IOExt* clone() const override { return new AddPlugin(\u0026m, sizeof(m)); } //检查类型支持 //TensorRT调用此方法以判断pos索引的输入/输出是否支持inOut[pos].format和inOut[pos].type指定的格式/数据类型。 //如果插件支持inOut[pos]处的格式/数据类型，则返回true。如果是否支持取决于其他的输入/输出格式/数据类型，则插件可以使其结果取决于inOut[0..pos-1]中的格式/数据类型，该格式/数据类型将设置为插件支持的值。这个函数不需要检查inOut[pos + 1..nbInputs + nbOutputs-1]，pos的决定必须仅基于inOut[0..pos]。 bool supportsFormatCombination(int pos, const nvinfer1::PluginTensorDesc* inOut, int nbInputs, int nbOutputs) const override { switch(pos) { case 0: printf(\"inOut[0].type = %d, format[0]=%d\\n\", (int)inOut[0].type, (int)inOut[0].format); return ((inOut[0].type == nvinfer1::DataType::kFLOAT || inOut[0].type == nvinfer1::DataType::kHALF) \u0026\u0026 inOut[0].format == nvinfer1::TensorFormat::kLINEAR) || (inOut[0].type == nvinfer1::DataType::kINT8 \u0026\u0026 inOut[0].format == nvinfer1::TensorFormat::kCHW4); case 1: printf(\"inOut[1].type = %d, format[1]=%d\\n\", (int)inOut[1].type, (int)inOut[1].format); return inOut[0].format == inOut[1].format \u0026\u0026 inOut[0].type == inOut[1].type; } return false; } //返回的输出tensor数目 int getNbOutputs() const override { return 1; } //定义返回的输出tensor维度 nvinfer1::Dims getOutputDimensions(int index, const nvinfer1::Dims* pInputDim, int nInputDim) override { return pInputDim[0]; } //返回结果的类型 nvinfer1::DataType getOutputDataType(int index, const nvinfer1::DataType* inputTypes, int nbInputs) const override { return inputTypes[0] == nvinfer1::DataType::kFLOAT ? nvinfer1::DataType::kFLOAT : nvinfer1::DataType::kINT8; } //配置这个插件op，判断输入和输出类型数量是否正确。官方还提到通过这个配置信息可以告知TensorRT去选择合适的算法(algorithm)去调优这个模型。 virtual void configurePlugin(const nvinfer1::PluginTensorDesc* in, int nbInput, const nvinfer1::PluginTensorDesc* out, int nbOutput) override { m.dataType = in[0].type; m.inputDim = in[0].dims; m.scale = in[0].scale; printf(\"configurePlugin type=%d, m.scale=%f\\n\", (int)out[0].type, m.scale); } //我们需要在这里确定这个op需要多大的显存空间去运行,尽量不要自己取申请显存空间 //而是让Tensorrt官方接口传过来的workspace指针来管理显存空间 size_t getWorkspaceSize(int nMaxBatchSize) const override {return 0;} //实际插件op的执行函数，我们自己实现的cuda操作就放到这里 //它的具体实现看下面的`AddPlugin.cu` int enqueue(int nBatch, const void * const *inputs, void **outputs, void* workspace, cudaStream_t stream) override; int initialize() override {return 0;} void terminate() override {} void destroy() override { delete this; } //设置插件的namespace的名字 void setPluginNamespace(const char* szNamespace) override {} //如果设置插件的namespace的名字，默认是\"\" const char* getPluginNamespace() const override {return \"\";} const char* getPluginType() const override {return \"AddPlugin\";} const char* getPluginVersion() const override {return \"0\";} bool canBroadcastInputAcrossBatch(int inputIndex) const override {return false;} bool isOutputBroadcastAcrossBatch(int outputIndex, const bool* inputIsBroadcasted, int nbInputs) const {return false;} //如果这个op使用到了一些其他东西，例如cublas handle，可以直接借助TensorRT内部提供的cublas handle: void attachToContext(cudnnContext* /*cudnn*/, cublasContext* /*cublas*/, nvinfer1::IGpuAllocator* /*allocator*/) {} void detachFromContext() {} private: using nvinfer1::IPluginV2Ext::configurePlugin; //插件中的权重、超参数定义在private里面 struct { nvinfer1::DataType dataType; nvinfer1::Dims inputDim; float valueToAdd; float scale; } m; }; class AddPluginCreator : public nvinfer1::IPluginCreator { public: //这个函数会被onnx-tensorrt的一个叫做TRT_PluginV2的转换op调用，这个op会读取onnx模型的data数据将其反序列化到network中。 nvinfer1::IPluginV2* deserializePlugin(const char* name, const void* serialData, size_t serialLength) override { return new AddPlugin(serialData, serialLength); } const char* getPluginName() const override {return \"AddPlugin\";} const char* getPluginVersion() const override {return \"0\";} void setPluginNamespace(const char* szNamespace) override {} const char* getPluginNamespace() const override {return \"\";} //这个是成员变量，也会作为getFieldNames成员函数的返回类型。\tPluginFieldCollection的主要作用是传递这个插件op所需要的权重和参数，在实际的engine推理过程中并不使用，而在parse中会用到(例如caffe2trt、onnx2trt)。 const nvinfer1::PluginFieldCollection* getFieldNames() override { std::cout \u003c\u003c __FUNCTION__ \u003c\u003c std::endl; return nullptr; } nvinfer1::IPluginV2* createPlugin(const char* name, const nvinfer1::PluginFieldCollection* fc) override { std::cout \u003c\u003c __FUNCTION__ \u003c\u003c std::endl; float valueToAdd = 0; for (int i = 0; i \u003c fc-\u003enbFields; i++) { if (!strcmp(fc-\u003efields[i].name, \"valueToAdd\")) { valueToAdd = *(float *)fc-\u003efields[i].data; } } return new AddPlugin({nvinfer1::DataType::kFLOAT, \u0026valueToAdd, 1}); } }; 再看AddPlugin.cu中的算子实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include \"AddPlugin.h\" #include \"cuda_fp16.h\" #include \u003cchrono\u003e #include \u003cthread\u003e template\u003ctypename T\u003e __global__ void AddValue(T *pDst, T *pSrc, int n, T valueToAdd) { int x = blockIdx.x * blockDim.x + threadIdx.x; if (x \u003e= n) return; pDst[x] = pSrc[x] + valueToAdd; } int AddPlugin::enqueue(int nBatch, const void * const *inputs, void **outputs, void* workspace, cudaStream_t stream) { int n = nBatch; for (int i = 0; i \u003c m.inputDim.nbDims; i++) { n *= m.inputDim.d[i]; } printf(\"n=%d, nBatch=%d\\n\", n, nBatch); if (m.dataType == nvinfer1::DataType::kFLOAT) { std::cout \u003c\u003c \"Running fp32 kernel\" \u003c\u003c std::endl; std::this_thread::sleep_for(20ms); AddValue\u003c\u003c\u003c(n + 1023) / 1024, 1024\u003e\u003e\u003e((float *)outputs[0], (float *)inputs[0], n, m.valueToAdd); } else if (m.dataType == nvinfer1::DataType::kHALF) { std::cout \u003c\u003c \"Running fp16 kernel\" \u003c\u003c std::endl; std::this_thread::sleep_for(10ms); AddValue\u003c\u003c\u003c(n + 1023) / 1024, 1024\u003e\u003e\u003e((__half *)outputs[0], (__half *)inputs[0], n, (__half)m.valueToAdd); } else { std::cout \u003c\u003c \"Running int8 kernel\" \u003c\u003c std::endl; std::this_thread::sleep_for(0ms); float valueToAdd = m.valueToAdd / m.scale; std::cout \u003c\u003c \"valueToAdd (int8 scaled): \" \u003c\u003c valueToAdd \u003c\u003c \", \" \u003c\u003c (int)valueToAdd \u003c\u003c std::endl; AddValue\u003c\u003c\u003c(n + 1023) / 1024, 1024\u003e\u003e\u003e((int8_t *)outputs[0], (int8_t *)inputs[0], n, (int8_t)valueToAdd); } return 0; } //通过REGISTER_TENSORRT_PLUGIN注册plugin REGISTER_TENSORRT_PLUGIN(AddPluginCreator); onnx-tensorrt中的plugin注册 同样地我们需要定义上述的AddPlugin.h和AddPlugin.cu,在mian函数中用DEFINE_BUILTIN_OP_IMPORTER注册算子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 DEFINE_BUILTIN_OP_IMPORTER(AddPlugin) { ASSERT(inputs.at(0).is_tensor(), ErrorCode::kUNSUPPORTED_NODE); ... const std::string pluginName = \"AddPlugin\"; const std::string pluginVersion = \"001\"; // 这个f保存这个op需要的权重和参数，从onnx模型中获取 std::vector\u003cnvinfer1::PluginField\u003e f; f.emplace_back(\"in_channel\", \u0026in_channel, nvinfer1::PluginFieldType::kINT32, 1); f.emplace_back(\"weight\", kernel_weights.values, nvinfer1::PluginFieldType::kFLOAT32, kernel_weights.count()); f.emplace_back(\"bias\", bias_weights.values, nvinfer1::PluginFieldType::kFLOAT32, bias_weights.count); // 这个从将plugin工厂中获取该插件，并且将权重和参数传递进去 nvinfer1::IPluginV2* plugin = importPluginFromRegistry(ctx, pluginName, pluginVersion, node.name(), f); RETURN_FIRST_OUTPUT(ctx-\u003enetwork()-\u003eaddPluginV2(tensors.data(), tensors.size(), *plugin)); } 更多–工具和官方文档 TensorRT官方文档\nonnx到trt的众多转换工具(模型库) TensorRT Backend For ONNX\ntorch2trt\ntorch2trt_dynamic\nTRTorch\n一些官方开源的小工具，帮助我们更好地调试和可视化： TensorRTTools\n值得仔细研读的工程 tensorrtx\ntrt-samples-for-hackathon-cn\n","title":"TensorRT学习笔记","uri":"/contents/dl/tensorrt_intro/"},{"categories":["contents"],"content":"记录几种常用的设计模式\n设计模式的六大原则 单一职责原则（SRP，Single Responsibility Principle） 一个类应该仅有一个引起它变化的原因。 变化的方向隐含着类的责任。\n里氏替换原则（LSP，Liskov Substitution Principle） 子类必须能够替换它们的基类(IS-A)。 继承表达类型抽象。\n依赖倒置原则（DIP，Dependence Inversion Principle） 高层模块(稳定)不应该依赖于低层模块(变化)，二者都应该依赖 于抽象(稳定) 。 抽象(稳定)不应该依赖于实现细节(变化) ，实现细节应该依赖于 抽象(稳定)。\n接口隔离原则（ISP，Interface Segregation Principle） 不应该强迫客户程序依赖它们不用的方法。 接口应该小而完备。\n迪米特法则（LoD，Law of Demeter） 一个对象应该对其他对象保持最少的了解。\n开放封闭原则（OCP，Open Close Principle） 对扩展开放，对更改封闭。 类模块应该是可扩展的，但是不可修改。\n具体可以看设计模式六大原则\n几个常用的设计模式 具体阅读：设计模式目录\n把几个常用的设计模式分为： 创建型模式， 结构型模式， 行为模式三类\n创建型模式 创建型模式–单例模式(Singleton) 单例模式是一种创建型设计模式， 让你能够保证一个类只有一个实例， 并提供一个访问该实例的全局节点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class Singleton{ private: Singleton(); Singleton(const Singleton\u0026 other); public: static Singleton* getInstance(); static Singleton* m_instance; }; Singleton* Singleton::m_instance=nullptr; //线程非安全版本 Singleton* Singleton::getInstance() { if (m_instance == nullptr) { m_instance = new Singleton(); } return m_instance; } //线程安全版本，但锁的代价过高 Singleton* Singleton::getInstance() { Lock lock; if (m_instance == nullptr) { m_instance = new Singleton(); } return m_instance; } //双检查锁，但由于内存读写reorder不安全 Singleton* Singleton::getInstance() { if(m_instance==nullptr){ Lock lock; if (m_instance == nullptr) { m_instance = new Singleton(); } } return m_instance; } //C++ 11版本之后的跨平台实现 (volatile) std::atomic\u003cSingleton*\u003e Singleton::m_instance; std::mutex Singleton::m_mutex; Singleton* Singleton::getInstance() { Singleton* tmp = m_instance.load(std::memory_order_relaxed); std::atomic_thread_fence(std::memory_order_acquire);//获取内存fence if (tmp == nullptr) { std::lock_guard\u003cstd::mutex\u003e lock(m_mutex); tmp = m_instance.load(std::memory_order_relaxed); if (tmp == nullptr) { tmp = new Singleton; std::atomic_thread_fence(std::memory_order_release);//释放内存fence m_instance.store(tmp, std::memory_order_relaxed); } } return tmp; } 创建型模式–工厂模式(Factory) 在软件系统中，经常面临着创建对象的工作；由于需求的变化， 需要创建的对象的具体类型经常变化。 如何应对这种变化？如何绕过常规的对象创建方法(new)，提供一 种“封装机制”来避免客户程序和这种“具体对象创建工作”的紧 耦合？\n定义一个用于创建对象的接口，让子类决定实例化哪一个类。 Factory Method使得一个类的实例化延迟（目的：解耦， 手段：虚函数）到子类。\nISplitterFactory.cpp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //抽象类 class ISplitter{ public: virtual void split()=0; virtual ~ISplitter(){} }; //工厂基类 class SplitterFactory{ public: virtual ISplitter* CreateSplitter()=0; virtual ~SplitterFactory(){} }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class MainForm : public Form { SplitterFactory* factory;//工厂 public: MainForm(SplitterFactory* factory){ this-\u003efactory=factory; } void Button1_Click(){ ISplitter * splitter= factory-\u003eCreateSplitter(); //多态new splitter-\u003esplit(); } }; 创建型模式–抽象工厂模式(Abstract Factory) 在软件系统中，经常面临着“一系列相互依赖的对象”的创建工 作；同时，由于需求的变化，往往存在更多系列对象的创建工作。 如何应对这种变化？如何绕过常规的对象创建方法(new)，提供一 种“封装机制”来避免客户程序和这种“多系列具体对象创建工作” 的紧耦合？\n抽象工厂模式是一种创建型设计模式， 它能创建一系列相关的对象， 而无需指定其具体类。\n提供一个接口，让该接口负责创建一系列“相关或者相互依 赖的对象”，无需指定它们具体的类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 //数据库访问有关的基类 class IDBConnection{ }; class IDBCommand{ }; class IDataReader{ }; class IDBFactory{ public: virtual IDBConnection* CreateDBConnection()=0; virtual IDBCommand* CreateDBCommand()=0; virtual IDataReader* CreateDataReader()=0; }; //支持SQL Server class SqlConnection: public IDBConnection{ }; class SqlCommand: public IDBCommand{ }; class SqlDataReader: public IDataReader{ }; class SqlDBFactory:public IDBFactory{ public: virtual IDBConnection* CreateDBConnection()=0; virtual IDBCommand* CreateDBCommand()=0; virtual IDataReader* CreateDataReader()=0; }; //支持Oracle class OracleConnection: public IDBConnection{ }; class OracleCommand: public IDBCommand{ }; class OracleDataReader: public IDataReader{ }; class EmployeeDAO{ IDBFactory* dbFactory; public: EmployeeDAO(IDBFactory* dbFactory){ this-\u003edbFactory = dbFactory; } vector\u003cEmployeeDO\u003e GetEmployees(){ IDBConnection* connection = dbFactory-\u003eCreateDBConnection(); connection-\u003eConnectionString(\"...\"); IDBCommand* command = dbFactory-\u003eCreateDBCommand(); command-\u003eCommandText(\"...\"); command-\u003eSetConnection(connection); //关联性 IDBDataReader* reader = command-\u003eExecuteReader(); //关联性 while (reader-\u003eRead()){ } } }; 结构型模式 结构型模式– 适配模式(Adapter) 适配器模式是一种结构型设计模式， 它能使接口不兼容的对象能够相互合作。\n将一个类的接口转换成客户希望的另一个接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 //目标接口（新接口） class ITarget{ public: virtual void process()=0; }; //遗留接口（老接口） class IAdaptee{ public: virtual void foo(int data)=0; virtual int bar()=0; }; //遗留类型 class OldClass: public IAdaptee{ //.... }; //对象适配器 class Adapter: public ITarget{ //继承 protected: IAdaptee* pAdaptee;//组合 public: Adapter(IAdaptee* pAdaptee){ this-\u003epAdaptee=pAdaptee; } virtual void process(){ int data=pAdaptee-\u003ebar(); pAdaptee-\u003efoo(data); } }; //类适配器 class Adapter: public ITarget, protected OldClass{ //多继承 } int main(){ IAdaptee* pAdaptee=new OldClass(); ITarget* pTarget=new Adapter(pAdaptee); pTarget-\u003eprocess(); } class stack{ deqeue container; }; class queue{ deqeue container; }; 结构型模式–桥接模式(Bridge) 桥接模式是一种结构型设计模式， 可将一个大类或一系列紧密相关的类拆分为抽象和实现两个独立的层次结构， 从而能在开发时分别使用。\n由于某些类型的固有的实现逻辑，使得它们具有两个变化的维度， 乃至多个纬度的变化。 如何应对这种“多维度的变化”？如何利用面向对象技术来使得 类型可以轻松地沿着两个乃至多个方向变化，而不引入额外的复杂 度？\n将抽象部分(业务功能)与实现部分(平台实现)分离，使它们 都可以独立地变化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class Messager{ protected: //不向用户开放该接口 //通过组合的形式，把两个类连接起来(桥模式) MessagerImp* m_messagerImp;//... Messager(MessagerImp* messagerImp):m_messagerImp(messagerImp){}; public: virtual void Login(string username, string password)=0; virtual void SendMessage(string message)=0; virtual void SendPicture(Image image)=0; virtual ~Messager(){} }; class MessagerImp{ public: virtual void PlaySound()=0; virtual void DrawShape()=0; virtual void WriteText()=0; virtual void Connect()=0; virtual MessagerImp(){} }; //平台实现 n class PCMessagerImp : public MessagerImp{ public: virtual void PlaySound(){ //********** } virtual void DrawShape(){ //********** } virtual void WriteText(){ //********** } virtual void Connect(){ //********** } }; class MobileMessagerImp : public MessagerImp{ public: virtual void PlaySound(){ //========== } virtual void DrawShape(){ //========== } virtual void WriteText(){ //========== } virtual void Connect(){ //========== } }; //业务抽象 m //类的数目：1+n+m class MessagerLite :public Messager { public: virtual void Login(string username, string password){ messagerImp-\u003eConnect(); //........ } virtual void SendMessage(string message){ messagerImp-\u003eWriteText(); //........ } virtual void SendPicture(Image image){ messagerImp-\u003eDrawShape(); //........ } }; class MessagerPerfect :public Messager { public: virtual void Login(string username, string password){ messagerImp-\u003ePlaySound(); //******** messagerImp-\u003eConnect(); //........ } virtual void SendMessage(string message){ messagerImp-\u003ePlaySound(); //******** messagerImp-\u003eWriteText(); //........ } virtual void SendPicture(Image image){ messagerImp-\u003ePlaySound(); //******** messagerImp-\u003eDrawShape(); //........ } }; void Process(){ //运行时装配 MessagerImp* mImp=new PCMessagerImp(); Messager *m =new Messager(mImp); } 结构型模式–装饰模式(Decorator) 在某些情况下我们可能会“过度地使用继承来扩展对象的功能”， 由于继承为类型引入的静态特质，使得这种扩展方式缺乏灵活性； 并且随着子类的增多（扩展功能的增多），各种子类的组合（扩展 功能的组合）会导致更多子类的膨胀。 如何使“对象功能的扩展”能够根据需要来动态地实现？同时避 免“扩展功能的增多”带来的子类膨胀问题？从而使得任何“功能 扩展变化”所导致的影响将为最低？\n动态（组合）地给一个对象增加一些额外的职责。就增加功 能而言，Decorator模式比生成子类（继承）更为灵活（消 除重复代码 \u0026 减少子类个数）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 //业务操作 class Stream{ public： virtual char Read(int number)=0; virtual void Seek(int position)=0; virtual void Write(char data)=0; virtual ~Stream(){} }; //主体类 class FileStream: public Stream{ public: virtual char Read(int number){ //读文件流 } virtual void Seek(int position){ //定位文件流 } virtual void Write(char data){ //写文件流 } }; class NetworkStream :public Stream{ public: virtual char Read(int number){ //读网络流 } virtual void Seek(int position){ //定位网络流 } virtual void Write(char data){ //写网络流 } }; class MemoryStream :public Stream{ public: virtual char Read(int number){ //读内存流 } virtual void Seek(int position){ //定位内存流 } virtual void Write(char data){ //写内存流 } }; //扩展操作 //装饰流 DecoratorStream: public Stream{ protected: Stream* stream;//... DecoratorStream(Stream * stm):stream(stm){ } }; //加密流 class CryptoStream: public DecoratorStream { public: CryptoStream(Stream* stm):DecoratorStream(stm){ } virtual char Read(int number){ //额外的加密操作... stream-\u003eRead(number);//读文件流 } virtual void Seek(int position){ //额外的加密操作... stream::Seek(position);//定位文件流 //额外的加密操作... } virtual void Write(byte data){ //额外的加密操作... stream::Write(data);//写文件流 //额外的加密操作... } }; class BufferedStream : public DecoratorStream{ // Stream* stream;//... public: BufferedStream(Stream* stm):DecoratorStream(stm){ } //... }; void Process(){ //运行时装配 FileStream* s1=new FileStream(); //实际上是CryptoFileStream CryptoStream* s2=new CryptoStream(s1); //实际上是BufferedFileStream BufferedStream* s3=new BufferedStream(s1); //实际上是CryptoBufferedFileStream BufferedStream* s4=new BufferedStream(s2); } 行为模式 行为模式–观察者模式(Observer) 观察者模式是一种行为设计模式， 允许你定义一种订阅机制， 可在对象事件发生时通知多个 “观察” 该对象的其他对象。\n在软件构建过程中，我们需要为某些对象建立一种“通知依赖关 系” ——一个对象（目标对象）的状态发生改变，所有的依赖对 象（观察者对象）都将得到通知。如果这样的依赖关系过于紧密， 将使软件不能很好地抵御变化。 使用面向对象技术，可以将这种依赖关系弱化，并形成一种稳定 的依赖关系。从而实现软件体系结构的松耦合。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class IProgress{ public: virtual void DoProgress(float value)=0; virtual ~IProgress(){} }; class FileSplitter { string m_filePath; int m_fileNumber; List\u003cIProgress*\u003e m_iprogressList; // 该对象的订阅列表 public: FileSplitter(const string\u0026 filePath, int fileNumber) : m_filePath(filePath), m_fileNumber(fileNumber){ } void split(){ //1.do something... for (int i = 0; i \u003c m_fileNumber; i++){ //... float progressValue = m_fileNumber; progressValue = (i + 1) / progressValue; onProgress(progressValue); } } void addIProgress(IProgress* iprogress){ m_iprogressList.push_back(iprogress); } void removeIProgress(IProgress* iprogress){ m_iprogressList.remove(iprogress); } protected: virtual void onProgress(float value){ List\u003cIProgress*\u003e::iterator itor=m_iprogressList.begin(); while (itor != m_iprogressList.end() ) (*itor)-\u003eDoProgress(value); itor++; } } }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class MainForm : public Form, public IProgress { TextBox* txtFilePath; TextBox* txtFileNumber; ProgressBar* progressBar; public: void Button1_Click(){ string filePath = txtFilePath-\u003egetText(); int number = atoi(txtFileNumber-\u003egetText().c_str()); ConsoleNotifier cn; FileSplitter splitter(filePath, number); splitter.addIProgress(this); //添加订阅 splitter.addIProgress(\u0026cn); //添加订阅 //split事件发生后， 通知多个 “观察” 该对象的其他对象。(即各种IProgress对象) splitter.split(); splitter.removeIProgress(this); //取消订阅 } virtual void DoProgress(float value){ progressBar-\u003esetValue(value); } }; class ConsoleNotifier : public IProgress { public: virtual void DoProgress(float value){ cout \u003c\u003c \".\"; } }; 行为模式–策略模式(Strategy) 策略模式是一种行为设计模式， 它能让你定义一系列算法， 并将每种算法分别放入独立的类中， 以使算法的对象能够相互替换。\n在软件构建过程中，某些对象使用的算法可能多种多样，经常改 变，如果将这些算法都编码到对象中，将会使对象变得异常复杂； 而且有时候支持不使用的算法也是一个性能负担。 如何在运行时根据需要透明地更改对象的算法？将算法与对象本 身解耦，从而避免上述问题？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class TaxStrategy{ public: virtual double Calculate(const Context\u0026 context)=0; virtual ~TaxStrategy(){} }; class CNTax : public TaxStrategy{ public: virtual double Calculate(const Context\u0026 context){ //*********** } }; class USTax : public TaxStrategy{ public: virtual double Calculate(const Context\u0026 context){ //*********** } }; class DETax : public TaxStrategy{ public: virtual double Calculate(const Context\u0026 context){ //*********** } }; //新增一个税 //********************************* class FRTax : public TaxStrategy{ public: virtual double Calculate(const Context\u0026 context){ //......... } }; class SalesOrder{ private: TaxStrategy* strategy; public: SalesOrder(StrategyFactory* strategyFactory){ this-\u003estrategy = strategyFactory-\u003eNewStrategy(); } ~SalesOrder(){ delete this-\u003estrategy; } public double CalculateTax(){ //... Context context(); double val = strategy-\u003eCalculate(context); //实例化的子类的计税方法 //... } }; ","title":"C++ 设计模式","uri":"/contents/c++/c++-designpattern/"},{"categories":["contents"],"content":"\nTransformer的基本结构 Attention机制 Attention 机制不是谷歌首先提出来的，但在在谷歌17年的论文《Attention is All You Need》中被很好地总结。这里贴出一篇很优秀的论文解读《Attention is All You Need》浅读（简介+代码），感兴趣的可以进一步去了解。\n在NLP的任务中,如词性识别，语音识别，文字翻译等，可以将文字通过文字嵌入(word embedding)的方式转化为向量的集合，或者将语音序列通过窗口裁切的方式转化为向量的集合， 然后通过RNN 或者 CNN 的方式完成序列的编码。RNN是一个马尔科夫决策过程，当前输入会依赖上一次的输出，CNN用局部窗口能捕捉到一定范围的结构信息。这两者要获得全局的信息必须得逐步进行(如RNN逐步递归，CNN通过多层卷积扩大感受野)， 而Attention是一种一次计算可以得到全局信息的方法。\nSelf-attention 是 Attention中被用的最多的一种类型，下面简要介绍下Self-attention 。\nAttention 为每一个输入的向量与其他向量计算相关度，最后输出的向量是每个向量根据相关度的加权总和。\n在 Attention 里有三个符号定义需要特别注意：\nq: query k: key v: value 上面的小写 $q, k, v$ 指的是某个向量，可以认为 $q$ 是当前的输入向量，$k$ 是要与 $q$ 计算相关度的向量， $v$ 是最后要根据相关度加权求和对应的向量，它们的矩阵形式(即多个向量的集合) 用$Q, K, V$ 表示。\n举个例子，上图的更具体的表示形式如下：\n对于输入的每一个向量都可以表示成上述的计算过程，很明显上述的过程对于多个输入向量是可以并行计算，这意味着可以表示成矩阵的形式。\n用矩阵的形式表示 $Q, K, V$ 的计算过程如下：\n在上图中，\n$$ \\boldsymbol{I} \\in\\mathbb{R}^{d_i\\times n} $$\n$$ \\boldsymbol{Q}\\in\\mathbb{R}^{d_k\\times n}, \\boldsymbol{K}\\in\\mathbb{R}^{d_k\\times m}, \\boldsymbol{V}\\in\\mathbb{R}^{d_v\\times m} $$\n$$ \\boldsymbol{O} = V \\times k^T \\times Q \\in\\mathbb{R}^{d_v\\times n} $$\n更规范的公式表达是：\n$$ \\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^{\\top}}{\\sqrt{d_k}}\\right)\\boldsymbol{V}\\end{equation} $$\n其中 除以维度的平方根 $\\sqrt{d_k}$ ，目的是使得$K, V$内积的结果不至于太大。\nAttention可以分为Self-attention 和 Cross-attention, 在Self-attention中，$Q$ 和 $K, V$ 属于同一个向量序列， 在Cross-attention中，$Q$ 和 $K, V$ 属于不同向量序列。\nMulti-head Self-attention Multi-head Self-attention 就是把 $Q,K,V$ 通过参数矩阵映射一下，然后再做 Attention ，把这个过程重复做n次，结果拼接起来,再经过一层线性变换做维度变换， 用于表示不同类型的相关性。\n上面是对 Attention 结构的简单介绍，严谨的符号定义还是得看原论文。\nAttention与CNN， GNN的联系 Attention 用过计算相关性来加权融合不同的向量得到新的向量， 可以看作是更加灵活的CNN。CNN可以当做是Attention的子集，在论文On the Relationship between Self-Attention and Convolutional Layers有具体的论证。\n可以注意到上面计算的相关性矩阵和GNN中的节点邻接矩阵很相似，向量集合$V$根据$Q, K$算出来的相关性矩阵加权求和的过程和图节点根据邻接矩阵做特征汇聚也是相似的，感兴趣的可以去进一步了解 Attention 和 GNN 的联系。\n下面是对 Transformer 的编码层和解码层的介绍。\nTransformer的编码层 我们来看Transformer的编码层是怎么实现的：\n有几个需要注意的细节：\nInput Embedding Input Embedding 是将输入转化为向量集合的方式，对于不同的任务处理方法不同。Positional Encoding 的作用是使得到的嵌入向量带有位置信息，这对于一些位置相关的任务是至关重要的。 Positional Encoding 可以用手工的方式设计：\n$$ PE_{2i}(p)=\\sin\\Big(p/10000^{2i/{d_{pos}}}\\Big) $$\n$$ PE_{2i+1}(p)=\\cos\\Big(p/10000^{2i/{d_{pos}}}\\Big) $$\n这里的意思是将$id$为$p$的位置映射为一个$d_{pos}$维的位置向量，这个向量的第$i$个元素的数值就是$PE_i(p)$。\n这里采用正余弦函数的设计是为了让位置编码能反应相对的位置信息。比如已知$p$的位置信息， $p+d$的位置信息，由正余弦公式展开：\n$$ \\sin(p+d)=\\sin p \\cos d + \\cos p \\sin d $$ 和 $$ \\cos(p+d)=\\cos p \\cos d - \\sin p \\sin d $$ 已知绝对位置$p$和相对位置$d$， 可以得到 $p+d$ 的绝对信息表示。\n当然还有其他的位置编码方法，比如设计网络从数据中学习得到编码信息。\nMuiti-Head Attention 指的是上文说过的做 Muiti-Head Self-Attention 的过程\nAdd add Norm Add 指的是 residual 结构，这在很多经典网络架构中都涉及到。Norm 指的是 Layer Norm ，对每一个向量求元素的均值和方差做归一化。这个和 Batch Norm 有所区别：Batch Norm 指的是在一个批次的数据中，对相同维度的元素做归一化。\nFeed Forward 经过多层全连接层构成前馈网络， 做维度的变换，进一步提高网络的表示能力。\n上述的 Muiti-Head Attention–\u003eAdd add Norm–\u003eFeed Forward–\u003eAdd add Norm会重复 $N$次。\nTransformer的解码层 解码层，首先输入一个标志位，结合编码器的编码信息输出第一个解码信息，联合标志位和第一个解码信息作为输入，结合结合编码器的编码信息输出第二个解码信息…以此类推。\n值得注意的几点：\nMasked-Multi-Head Attention 带有Mask的 Multi-Head Self-Attention, 因为在解码阶段的输入不可能预先知道解码后的输出是什么，所以需要设置一个 Mask (下三角矩阵)， 把为输出的解码位信息遮盖。\n第二个 Multi-Head Attention 这里其实是 Multi-Head Cross-Attention ，其中 $Q$ 来自解码器输入， $K, V$ 来自编码器最后一层的输出。\n解码过程什么时候停止 可以在训练的时候，训练标签中当解码器的输入属于最后一个输入，则输出一个结束符，让网络在数据中自行学习什么时候停止。这属于AT Decoder,(AT 指的是auto termination), 也有一些 NAT Decoder 的方法，一次性并行输入$N$个标志位， 一次性输出解码信息。 更多Transformer的扩展 Transformer的代码实现 代码参考3W字长文带你轻松入门视觉transformer\npositional encoding 1 2 3 4 5 6 7 8 9 10 def get_position_angle_vec(position): # hid_j是0-511,d_hid是512，position表示单词位置0～N-1 return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] # 每个单词位置0～N-1都可以编码得到512长度的向量 sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)]) # 偶数列进行sin, 间隔是2 sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i # 奇数列进行cos， 间隔是2 sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1 Multi-head Self-attention 可以看作是一个输入$X$ 分别经过多个Self-attention后再级联起来，最后通过线性变换改变输出维度。\n单个 Self-attention 实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class ScaledDotProductAttention(nn.Module): ''' Scaled Dot-Product Attention ''' def __init__(self, temperature, attn_dropout=0.1): super().__init__() self.temperature = temperature self.dropout = nn.Dropout(attn_dropout) def forward(self, q, k, v, mask=None): # self.temperature是论文中的d_k ** 0.5，防止梯度过大 # QxK/sqrt(dk) attn = torch.matmul(q / self.temperature, k.transpose(2, 3)) if mask is not None: # 屏蔽不想要的输出 attn = attn.masked_fill(mask == 0, -1e9) # softmax+dropout attn = self.dropout(F.softmax(attn, dim=-1)) # 概率分布xV output = torch.matmul(attn, v) return output, attn 下面的代码实现 Multi-head Self-attention + Add and Norm的过程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class MultiHeadAttention(nn.Module): ''' Multi-Head Attention module ''' # n_head头的个数，默认是8 # d_model编码向量长度，例如本文说的512 # d_k, d_v的值一般会设置为 n_head * d_k=d_model， # 此时concat后正好和原始输入一样，当然不相同也可以，因为后面有fc层 # 相当于将可学习矩阵分成独立的n_head份 def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1): super().__init__() # 假设n_head=8，d_k=64 self.n_head = n_head self.d_k = d_k self.d_v = d_v # d_model输入向量，n_head * d_k输出向量 # 可学习W^Q，W^K,W^V矩阵参数初始化 self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False) self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False) self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False) # 最后的输出维度变换操作 self.fc = nn.Linear(n_head * d_v, d_model, bias=False) # 单头自注意力 self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5) self.dropout = nn.Dropout(dropout) # 层归一化 self.layer_norm = nn.LayerNorm(d_model, eps=1e-6) def forward(self, q, k, v, mask=None): # 假设qkv输入是(b,100,512),100是训练每个样本最大单词个数 # 一般qkv相等，即自注意力 residual = q # 将输入x和可学习矩阵相乘，得到(b,100,512)输出 # 其中512的含义其实是8x64，8个head，每个head的可学习矩阵为64维度 # q的输出是(b,100,8,64),kv也是一样 q = self.w_qs(q).view(sz_b, len_q, n_head, d_k) k = self.w_ks(k).view(sz_b, len_k, n_head, d_k) v = self.w_vs(v).view(sz_b, len_v, n_head, d_v) # 变成(b,8,100,64)，方便后面计算，也就是8个头单独计算 q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2) if mask is not None: mask = mask.unsqueeze(1) # For head axis broadcasting. # 输出q是(b,8,100,64),维持不变,内部计算流程是： # q*k转置，除以d_k ** 0.5，输出维度是b,8,100,100即单词和单词直接的相似性 # 对最后一个维度进行softmax操作得到b,8,100,100 # 最后乘上V，得到b,8,100,64输出 q, attn = self.attention(q, k, v, mask=mask) # b,100,8,64--\u003eb,100,512 q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1) # 线性变换 q = self.dropout(self.fc(q)) # Add: 残差计算 q += residual # Norm: 层归一化，在512维度计算均值和方差，进行层归一化 q = self.layer_norm(q) return q, attn FeedForward层 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class PositionwiseFeedForward(nn.Module): ''' A two-feed-forward-layer module ''' def __init__(self, d_in, d_hid, dropout=0.1): super().__init__() # 两个fc层，对最后的512维度进行变换 self.w_1 = nn.Linear(d_in, d_hid) # position-wise self.w_2 = nn.Linear(d_hid, d_in) # position-wise self.layer_norm = nn.LayerNorm(d_in, eps=1e-6) self.dropout = nn.Dropout(dropout) def forward(self, x): residual = x x = self.w_2(F.relu(self.w_1(x))) x = self.dropout(x) x += residual x = self.layer_norm(x) return x Encoder层 Transformer 的编码层是上述的结构的反复堆叠，单层编码层如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 class EncoderLayer(nn.Module): def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1): super(EncoderLayer, self).__init__() self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout) self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout) def forward(self, enc_input, slf_attn_mask=None): # Q K V是同一个，自注意力 # enc_input来自源单词嵌入向量或者前一个编码器输出 enc_output, enc_slf_attn = self.slf_attn( enc_input, enc_input, enc_input, mask=slf_attn_mask) enc_output = self.pos_ffn(enc_output) return enc_output, enc_slf_attn 注意的是：除了第一个模块输入是单词嵌入向量与位置编码的和外，其余编码层输入是上一个编码器输出即后面的编码器输入不需要位置编码向量。\n完整的n个编码器如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Encoder(nn.Module): def __init__( self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v, d_model, d_inner, pad_idx, dropout=0.1, n_position=200): # nlp领域的词嵌入向量生成过程(单词在词表里面的索引idx--\u003ed_word_vec长度的向量) self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx) # 位置编码 self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position) self.dropout = nn.Dropout(p=dropout) # n个编码器层 self.layer_stack = nn.ModuleList([ EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers)]) # 层归一化 self.layer_norm = nn.LayerNorm(d_model, eps=1e-6) def forward(self, src_seq, src_mask, return_attns=False): # 对输入序列进行词嵌入，加上位置编码 enc_output = self.dropout(self.position_enc(self.src_word_emb(src_seq))) enc_output = self.layer_norm(enc_output) # 作为编码器层输入 for enc_layer in self.layer_stack: enc_output, _ = enc_layer(enc_output, slf_attn_mask=src_mask) return enc_output Decoder层 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class DecoderLayer(nn.Module): ''' Compose with three layers ''' def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1): super(DecoderLayer, self).__init__() self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout) self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout) self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout) def forward( self, dec_input, enc_output, slf_attn_mask=None, dec_enc_attn_mask=None): # 标准的自注意力，QKV=dec_input来自目标单词嵌入或者前一个解码器输出 dec_output, dec_slf_attn = self.slf_attn( dec_input, dec_input, dec_input, mask=slf_attn_mask) # KV来自最后一个编码层输出enc_output，Q来自带有mask的self.slf_attn输出 dec_output, dec_enc_attn = self.enc_attn( dec_output, enc_output, enc_output, mask=dec_enc_attn_mask) dec_output = self.pos_ffn(dec_output) return dec_output, dec_slf_attn, dec_enc_attn n个Decoder的堆叠如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Decoder(nn.Module): def __init__( self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v, d_model, d_inner, pad_idx, n_position=200, dropout=0.1): # 目标单词嵌入 self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx) # 位置嵌入向量 self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position) self.dropout = nn.Dropout(p=dropout) # n个解码器 self.layer_stack = nn.ModuleList([ DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers)]) # 层归一化 self.layer_norm = nn.LayerNorm(d_model, eps=1e-6) def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False): # 目标单词嵌入+位置编码 dec_output = self.dropout(self.position_enc(self.trg_word_emb(trg_seq))) dec_output = self.layer_norm(dec_output) # 遍历每个解码器 for dec_layer in self.layer_stack: # 需要输入3个信息：目标单词嵌入+位置编码、最后一个编码器输出enc_output # 和dec_enc_attn_mask，解码时候不能看到未来单词信息 dec_output, dec_slf_attn, dec_enc_attn = dec_layer( dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask) return dec_output 在第一层的 Decoder 仍然有位置编码层， 嵌入特征向量要右移一个单位，则第一个位置的输入是一个特殊符号表示解码开始。\n最后的分类层 比如分类问题： Linear + softmax：\n1 2 3 self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False) dec_output, *_ = self.model.decoder(trg_seq, trg_mask, enc_output, src_mask) return F.softmax(self.model.trg_word_prj(dec_output), dim=-1) 应用–视觉Transformer LOFTR LOFTR的处理流程如下：\n用局部特征提取网络从图像$I_A$以及$I_B$中提取粗略特征图 $\\tilde {F} ^ {A}$和$ \\tilde {F} ^ {B}$，以及精细的特征图$ \\hat {F} ^ {A} $和$ \\hat {F} ^ {B} $。\n将粗略特征图展平为一维向量，并添加位置编码；然后经过LoFTR 模块进行处理，则经过$N_c$个self-attention 和 cross-attention层的编码。\n将可微分匹配层(softmax 或者匈牙利算法)用于匹配上述变换后的特征，最终得到置信矩阵 。根据置信度阈值和相互邻近标准选择匹配项，得到粗略的匹配预测 。\n对于每个选定的粗略预测$(\\tilde {i},\\tilde {j})$ ，我们会从精细特征图中裁剪出具有大小为 $w \\times w$的局部窗口。粗匹配将在此局部窗口内进行细化为并达到亚像素匹配级别，作为最终的匹配预测。\n下面是LOFTR中与Transformer相关的模块代码：\n位置编码 由于是处理图像信息，位置编码是在$x, y$两个维度都进行位置编码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class PositionEncodingSine(nn.Module): \"\"\" This is a sinusoidal position encoding that generalized to 2-dimensional images \"\"\" def __init__(self, d_model, max_shape=(256, 256)): \"\"\" Args: max_shape (tuple): for 1/8 featmap, the max length of 256 corresponds to 2048 pixels \"\"\" super().__init__() pe = torch.zeros((d_model, *max_shape)) y_position = torch.ones(max_shape).cumsum(0).float().unsqueeze(0) x_position = torch.ones(max_shape).cumsum(1).float().unsqueeze(0) div_term = torch.exp(torch.arange(0, d_model//2, 2).float() * (-math.log(10000.0) / d_model//2)) div_term = div_term[:, None, None] # [C//4, 1, 1] pe[0::4, :, :] = torch.sin(x_position * div_term) pe[1::4, :, :] = torch.cos(x_position * div_term) pe[2::4, :, :] = torch.sin(y_position * div_term) pe[3::4, :, :] = torch.cos(y_position * div_term) # self.register_buffer('pe', pe.unsqueeze(0), persistent=False) # [1, C, H, W] self.pe = torch.unsqueeze(pe,0).cuda() def forward(self, x): \"\"\" Args: x: [N, C, H, W] \"\"\" return x + self.pe[:, :, :x.size(2), :x.size(3)] LOFTR模块 self-transformer和cross-transformer交叠使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class LoFTREncoderLayer(nn.Module): def __init__(self, d_model, nhead, attention='linear'): super(LoFTREncoderLayer, self).__init__() self.dim = d_model // nhead self.nhead = nhead # multi-head attention self.q_proj = nn.Linear(d_model, d_model, bias=False) self.k_proj = nn.Linear(d_model, d_model, bias=False) self.v_proj = nn.Linear(d_model, d_model, bias=False) self.attention = LinearAttention() if attention == 'linear' else FullAttention() # multi-head输出的线性融合 self.merge = nn.Linear(d_model, d_model, bias=False) # feed-forward network self.mlp = nn.Sequential( nn.Linear(d_model*2, d_model*2, bias=False), nn.ReLU(True), nn.Linear(d_model*2, d_model, bias=False), ) # norm and dropout self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) def forward(self, x, source, x_mask:Optional[torch.Tensor], source_mask:Optional[torch.Tensor]) -\u003e torch.Tensor: \"\"\" Args: x (torch.Tensor): [N, L, C] source (torch.Tensor): [N, S, C] x_mask (torch.Tensor): [N, L] (optional) source_mask (torch.Tensor): [N, S] (optional) \"\"\" bs = x.size(0) query, key, value = x, source, source # multi-head attention query = self.q_proj(query).view(bs, -1, self.nhead, self.dim) # [N, L, (H, D)] key = self.k_proj(key).view(bs, -1, self.nhead, self.dim) # [N, S, (H, D)] value = self.v_proj(value).view(bs, -1, self.nhead, self.dim) message = self.attention(query, key, value, q_mask=x_mask, kv_mask=source_mask) # [N, L, (H, D)] message = self.merge(message.view(bs, -1, self.nhead*self.dim)) # [N, L, C] message = self.norm1(message) # feed-forward network message = self.mlp(torch.cat([x, message], dim=2)) message = self.norm2(message) # 这里稍微和原本的transformer不一样 return x + message class LocalFeatureTransformer(nn.Module): \"\"\"A Local Feature Transformer (LoFTR) module.\"\"\" def __init__(self, config): super(LocalFeatureTransformer, self).__init__() self.config = config self.d_model = config['d_model'] self.nhead = config['nhead'] self.layer_names = config['layer_names'] encoder_layer = LoFTREncoderLayer(config['d_model'], config['nhead'], config['attention']) self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(len(self.layer_names))]) self._reset_parameters() def _reset_parameters(self): for p in self.parameters(): if p.dim() \u003e 1: nn.init.xavier_uniform_(p) def forward(self, feat0, feat1, mask0:Optional[torch.Tensor], mask1:Optional[torch.Tensor])-\u003eTuple[torch.Tensor, torch.Tensor]: \"\"\" Args: feat0 (torch.Tensor): [N, L, C] feat1 (torch.Tensor): [N, S, C] mask0 (torch.Tensor): [N, L] (optional) mask1 (torch.Tensor): [N, S] (optional) \"\"\" assert self.d_model == feat0.size(2), \"the feature number of src and transformer must be equal\" for idx, v in enumerate(self.layers): name = self.layer_names[idx]; layer = v; if name == 'self': feat0 = layer(feat0, feat0, mask0, mask0) feat1 = layer(feat1, feat1, mask1, mask1) elif name == 'cross': feat0 = layer(feat0, feat1, mask0, mask1) feat1 = layer(feat1, feat0, mask1, mask0) else: raise KeyError return feat0, feat1 ViT 和 DETR ViT 和 DETR在3W字长文带你轻松入门视觉transformer中有详细介绍和代码注释。\nViT将图像切块构建嵌入向量，并只用Transformer中的编码器模块实现图像分类， 在 ViT中，作者在嵌入向量集合的最开始添加了一个属于可学习参数的嵌入向量， 并将该输入对应的输出向量用于图像分类(如果不加入这个向量，用剩下其他的输出向量中的哪个都说不过去，不过我觉得可以将输出经过一个线性变换后，再用于分类)。\n而 DETR 是将 Transformer 用于目标检测。它的每一层编码器和解码器都添加位置编码信息，而且解码器的输入是前面说的 NAT 方式，一次性输入初始化全0的(100,b,256)的输出嵌入向量，经过多层解码器后，将最后一个解码器输出输入到分类和回归head中，得到100个无序集合，再后处理得到提取前景类别和对应的bbox坐标。\n视觉Transformer的更多拓展 最近出了两篇视觉Transformer的综述文章，感兴趣的可以进一步去了解：\n华为等提出视觉Transformer：全面调研 又一篇视觉Transformer综述来了！ ","title":"Transformer简介","uri":"/contents/attention/att_intro/"},{"categories":["contents"],"content":"本文对应的代码可以在百度云链接-密码: t6pu找到。\n利用TorchScript和libtorch可以快速将PyTorch提供的基于Python的模型迁移到到C ++加载和执行的序列化表示形式，而无需依赖Python。\n部署c++模型分为两部：\n将pytorch模型转为TorchScript模型文件并序列化到磁盘文件中。 利用libtorch反序列化TorchScript模型文件，并用libtorch提供的API进行推断。 将已有模型转为Torchscript TorchScript可以视为PyTorch模型的一种中间表示，TorchScript表示的PyTorch模型可以直接在C++中进行读取。PyTorch在1.0版本之后都可以使用TorchScript的方式来构建序列化的模型。TorchScript提供了Tracing和Annotation两种应用方式。\n通过 Tracing 的形式转换成Torchscript模型 一个例子是：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import torch import cv2 from src.loftr import LoFTR, default_cfg # 1. 加载模型 matcher = LoFTR(config=default_cfg) model = torch.load(\"/home/jiajie/3d_reco/LoFTR_2021/LoFTR-master/weights/indoor_ds.ckpt\")['state_dict'] matcher.load_state_dict(model, strict = True) matcher = matcher.eval().cuda() # 2. 加载数据 img0_pth = \"/home/jiajie/3d_reco/LoFTR_pretrained_cpp/models_script/test_imgs/indoor0.JPG\" img1_pth = \"/home/jiajie/3d_reco/LoFTR_pretrained_cpp/models_script/test_imgs/indoor1.JPG\" img0_raw = cv2.imread(img0_pth, cv2.IMREAD_GRAYSCALE) img1_raw = cv2.imread(img1_pth, cv2.IMREAD_GRAYSCALE) #indoor img0_raw = cv2.resize(img0_raw, (640, 480)) img1_raw = cv2.resize(img1_raw, (640, 480)) img0 = torch.from_numpy(img0_raw)[None][None].cuda() / 255. img1 = torch.from_numpy(img1_raw)[None][None].cuda() / 255. batch = {'image0': img0, 'image1': img1} # 3. 推断 # Inference with LoFTR and get prediction with torch.no_grad(): # matcher(batch) traced_script_module = torch.jit.trace(matcher, batch, strict=False) # 4. 保存参数 traced_script_module.save('/home/jiajie/3d_reco/LoFTR_pretrained_cpp/models_script/weights/indoor_ds.pt') 上面的流程和基于python的推断过程不同的地方仅在于：\n1 matcher(batch) 改为\n1 traced_script_module = torch.jit.trace(matcher, batch, strict=False) 其中matcher是定义的pytorch模型，batch是输入数据， strict=False允许模型的输出以字典的形式存在。\ntorch.jit.trace的使用是依赖于输入的数据，它只会记录由输入数据决定的网络的分支。所以当网络模型因为输入数据的不同出现多分支时，不应该使用torch.jit.trace的形式序列化已有模型。\n由于torch.jit.trace高度依赖于输入数据，实际部署使用中，输入数据的维度必须和转换模型时的数据维度一致，比如上述的输入图像尺寸是 $640 \\times 480$, 当在c++代码中改变输入图像的尺寸：\n1 2 3 4 cv::Mat img0 = cv::imread(image_name_vec[0], cv::IMREAD_GRAYSCALE); cv::Mat img1 = cv::imread(image_name_vec[1], cv::IMREAD_GRAYSCALE); cv::resize(img0, img0, cv::Size(800,800)); cv::resize(img1, img1, cv::Size(800,800)); 会出现以下报错：\n1 RuntimeError: shape '[1, 60, 80, 60, 80]' is invalid for input of size 100000000 需要固定输入尺寸并不利于实际部署，比如在面对不同输入尺寸的图像，或者需要扩大输入尺寸来充分利用GPU的计算能力。这时候可以通过另一种方式生成Torchscript模型文件。\n通过 Annotation 的形式转换成Torchscript模型 Annotation允许完全保存网络的分支， 并且支持数据动态维度输入(当然这得看具体网络结构)。\nAnnotation 通过Torch Script 脚本语言中重新定义模型， 将原来转换为ScriptModule并保存。\n官方的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class MyModule(torch.nn.Module): def __init__(self, N, M): super(MyModule, self).__init__() self.weight = torch.nn.Parameter(torch.rand(N, M)) def forward(self, input): if input.sum() \u003e 0: output = self.weight.mv(input) else: output = self.weight + input return output my_module = MyModule(10,20) sm = torch.jit.script(my_module) 上面的例子其实可以认为是Torch Script语言编写的而不是python，但由于Torch Script是python语言的子集，上面的例子没看出区别，但一些python的数据类型在Torch Script中是不支持的，有一些的数据类型的使用方法在两个脚本语言中的使用方法也有所不同。具体信息可以查看jit_language_reference这个链接。\n通过Annotation方式转换模型需要注意几点： 确保整个pytorch模型流程完全有python语言和pytorch操作算子组成，避免出现用numpy数据处理再转为tensor这些操作。 在Torch Script中，默认的数据类型是torch.Tensor，如果数据是其他类型，需要显式地标注，比如在一个类模块中： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class LocalFeatureTransformer(nn.Module): \"\"\"A Local Feature Transformer (LoFTR) module.\"\"\" def __init__(self, config): super(LocalFeatureTransformer, self).__init__() self.config = config self.d_model = config['d_model'] self.nhead = config['nhead'] self.layer_names = config['layer_names'] encoder_layer = LoFTREncoderLayer(config['d_model'], config['nhead'], config['attention']) self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(len(self.layer_names))]) self._reset_parameters() def _reset_parameters(self): for p in self.parameters(): if p.dim() \u003e 1: nn.init.xavier_uniform_(p) def forward(self, feat0, feat1, mask0=None, mask1=None): \"\"\" Args: feat0 (torch.Tensor): [N, L, C] feat1 (torch.Tensor): [N, S, C] mask0 (torch.Tensor): [N, L] (optional) mask1 (torch.Tensor): [N, S] (optional) \"\"\" # assert self.d_model == feat0.size(2), \"the feature number of src and transformer must be equal\" for layer, name in zip(self.layers, self.layer_names): if name == 'self': feat0 = layer(feat0, feat0, mask0, mask0) feat1 = layer(feat1, feat1, mask1, mask1) elif name == 'cross': feat0 = layer(feat0, feat1, mask0, mask1) feat1 = layer(feat1, feat0, mask1, mask0) else: raise KeyError return feat0, feat1 用Torch Script标注的形式是这样的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class LocalFeatureTransformer(nn.Module): \"\"\"A Local Feature Transformer (LoFTR) module.\"\"\" def __init__(self, config): super(LocalFeatureTransformer, self).__init__() self.config = config self.d_model = config['d_model'] self.nhead = config['nhead'] self.layer_names = config['layer_names'] encoder_layer = LoFTREncoderLayer(config['d_model'], config['nhead'], config['attention']) self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(len(self.layer_names))]) self._reset_parameters() def _reset_parameters(self): for p in self.parameters(): if p.dim() \u003e 1: nn.init.xavier_uniform_(p) def forward(self, feat0, feat1, mask0:Optional[torch.Tensor], mask1:Optional[torch.Tensor])-\u003eTuple[torch.Tensor, torch.Tensor]: \"\"\" Args: feat0 (torch.Tensor): [N, L, C] feat1 (torch.Tensor): [N, S, C] mask0 (torch.Tensor): [N, L] (optional) mask1 (torch.Tensor): [N, S] (optional) \"\"\" assert self.d_model == feat0.size(2), \"the feature number of src and transformer must be equal\" for idx, v in enumerate(self.layers): name = self.layer_names[idx]; layer = v; if name == 'self': feat0 = layer(feat0, feat0, mask0, mask0) feat1 = layer(feat1, feat1, mask1, mask1) elif name == 'cross': feat0 = layer(feat0, feat1, mask0, mask1) feat1 = layer(feat1, feat0, mask1, mask0) else: raise KeyError return feat0, feat1 注意在def forward()函数中的数据类型，不是属于torch.Tensor的数据都被显式地标注了类型，包括返回类型。\nTorch Script不支持位置参数（*）关键词参数（**）, 具体解决方法得看实际问题，比如：在rearrange函数中 1 2 def rearrange(tensor, pattern, **axes_lengths): ... 含有关键词参数axes_lengths，用来支持自定义维度扩展，这使得Torch Script不支持该函数，修改方法：\n1 feat_c0 = rearrange(self.pos_encoding(feat_c0), 'n c h w -\u003e n (h w) c') 等价于：\n1 2 3 4 feat_c0 = self.pos_encoding(feat_c0) feat_c0 = torch.transpose(feat_c0,1,3) feat_c0 = torch.transpose(feat_c0,1,2) feat_c0 = torch.reshape(feat_c0, (feat_c0.shape[0], feat_c0.shape[1]*feat_c0.shape[2], feat_c0.shape[3])) Torch Script对 iteration支持也不够好： 1 2 3 4 5 6 7 8 9 for layer, name in zip(self.layers, self.layer_names): if name == 'self': feat0 = layer(feat0, feat0, mask0, mask0) feat1 = layer(feat1, feat1, mask1, mask1) elif name == 'cross': feat0 = layer(feat0, feat1, mask0, mask1) feat1 = layer(feat1, feat0, mask1, mask0) else: raise KeyError 改为：\n1 2 3 4 5 6 7 8 9 10 11 for idx, v in enumerate(self.layers): name = self.layer_names[idx]; layer = v; if name == 'self': feat0 = layer(feat0, feat0, mask0, mask0) feat1 = layer(feat1, feat1, mask1, mask1) elif name == 'cross': feat0 = layer(feat0, feat1, mask0, mask1) feat1 = layer(feat1, feat0, mask1, mask0) else: raise KeyError 其他一些错误，和Torch Script不支持的类型，见一位老哥的博客，写的很详细。\nTracing 中使用 Script 由于Tracing不适用于多分支网络的情况，可以在多分支的地方通过显式调用@torch.jit.script可以记录所有分支，官方的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch @torch.jit.script def foo(x, y): if x.max() \u003e y.max(): r = x else: r = y return r def bar(x, y, z): return foo(x, y) + z traced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3))) Script 中使用 Tracing 反过来，另一种情况是在script module中用tracing生成子模块，对于一些存在script module不支持的python feature的layer，就可以把相关layer封装起来，用trace记录相关layer流，其他layer不用修改。使用示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch import torchvision class MyScriptModule(torch.nn.Module): def __init__(self): super(MyScriptModule, self).__init__() self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68]) .resize_(1, 3, 1, 1)) self.resnet = torch.jit.trace(torchvision.models.resnet18(), torch.rand(1, 3, 224, 224)) def forward(self, input): return self.resnet(input - self.means) my_script_module = torch.jit.script(MyScriptModule()) 定义完TorchScript模型，保存模型：\n1 torch.jit.save(sm, torch_script_path) libtorch加载序列化模型 CMakeLists:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 cmake_minimum_required(VERSION 3.0 FATAL_ERROR) project(loftr) if (NOT CMAKE_BUILD_TYPE) message(STATUS \"No build type selected, default to Release\") set(CMAKE_BUILD_TYPE \"Release\") endif() set(CMAKE_PREFIX_PATH ${CMAKE_CURRENT_SOURCE_DIR}/dependences/libtorch-shared-with-deps-1.8.1+cu102/libtorch) find_package(Torch REQUIRED) find_package(OpenCV REQUIRED) add_executable(loftr loftr.cpp) target_link_libraries(loftr ${OpenCV_LIBS} ${TORCH_LIBRARIES}) include_directories(${OpenCV_INCLUDE_DIRS}) file(COPY ${CMAKE_CURRENT_SOURCE_DIR}/models_script/weights/indoor_ds.pt DESTINATION ${CMAKE_BINARY_DIR} ) 一个例子是：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 void infer(std::vector\u003ccv::String\u003e image_name_vec, std::string executable_dir) { std::cout\u003c\u003c\"start loftr match...\"\u003c\u003cstd::endl; torch::manual_seed(1); torch::autograd::GradMode::set_enabled(false); torch::Device device(torch::kCPU); if (torch::cuda::is_available()) { std::cout \u003c\u003c \"CUDA is available! Training on GPU.\" \u003c\u003c std::endl; device = torch::Device(torch::kCUDA); } //加载模型 auto module_path = executable_dir + \"/\" + \"indoor_ds.pt\"; torch::jit::script::Module loftr = torch::jit::load(module_path); loftr.eval(); loftr.to(device); //加载数据 //read images cv::Mat img0 = cv::imread(image_name_vec[0], cv::IMREAD_GRAYSCALE); cv::Mat img1 = cv::imread(image_name_vec[1], cv::IMREAD_GRAYSCALE); //convert to tensor torch::Tensor image0 = mat2tensor(img0).to(device); torch::Tensor image1 = mat2tensor(img1).to(device); //dump to loftr network torch::Dict\u003cstd::string, Tensor\u003e output; torch::Dict\u003cstd::string, Tensor\u003e input; input.insert(\"image0\", image0); input.insert(\"image1\", image1); //模型推断 output = toTensorDict(loftr.forward({input})); at::Tensor mkpts0_f = output.at(\"mkpts0_f\"); //N*2 at::Tensor mkpts1_f = output.at(\"mkpts1_f\"); //N*2 at::Tensor mconf = output.at(\"mconf\"); //N*1 匹配的置信度 } 具体代码实现见文章最上面的百度云链接。\n","title":"利用TorchScript+libtorch部署c++模型","uri":"/contents/dl/deploy_torch_script/"},{"categories":["contents"],"content":"GIN $$ \\mathbf{x}^{\\prime}_i = h_{\\mathbf{\\Theta}} \\left( (1 + \\epsilon) \\cdot \\mathbf{x}_i + \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{x}_j \\right) $$\nor\n$$ \\mathbf{X}^{\\prime} = h_{\\mathbf{\\Theta}} \\left( \\left( \\mathbf{A} + (1 + \\epsilon) \\cdot \\mathbf{I} \\right) \\cdot \\mathbf{X} \\right), $$\nhere :math: $h_{\\mathbf{\\Theta}}$ denotes a neural network, .i.e. an MLP.\n需要使用MLP取拟合一个单射函数，单层感知机不可以。\n详细见链接\n","title":"GIN","uri":"/contents/gnn/gin/"},{"categories":["contents"],"content":"在人类感知的尺度上，平面是人类社会环境中最常见的结构之一，并且具有强大的约束能力，约束着大量的点/线及其所携带的信息。各种曲面都可以用平面进行近似，根据精度要求选择拟合的平面数量。实际应用中，许多计算机视觉任务都需要平面信息，比如：机器人领域中，识别地面、墙面等平面可用于路径规划、视觉导航，识别桌面、书架等平面可辅助机械手抓取和放置物品；增强现实、混合现实中，利用平面信息放置物品，或者更换桌面、地板、墙面的纹理可以进行快速展示；三维场景重建中，用平面而非点云可以实现对一个城市大规模、简洁的重建。\n整个流程主要分成三个模块：\n平面提取 提取平面特征构建特征描述子 利用图匹配的方法匹配平面 平面提取 基于单张彩色图输入的平面提取任务旨在从图像中分割出所拍摄场景中的平面结构，并同时估计相机到平面的深度信息。\n基于单张彩色图输入的平面提取首先是从图片中提取特征。传统方法关注的是几何基元的提取，比如点、线段等；也会使用纹理信息，比如颜色、形状等。神经网络强大的特征提取能力可以从像素点中提取信息，然后聚合成平面；也有研究是用神经网络提取图片中点、线之类的几何基元来构成平面。\n几何方法 几何方法提取平面使用一种自下而上的方法，即先寻找单张彩色图片中的几何基 元来 恢 复 三 维 信 息，从 而 进 一 步 提 取 平面。为了提高平面提取精度，大部分几何方法使用了一定的场景约束，最常用的是曼哈顿世界假设（曼哈顿世界假设是指场景中不同朝向的平面应相互正交，即分别对应三维笛卡尔坐标系中的ｘｙ面、ｘｚ面和ｙｚ 面。），这使得它们的应用场景也受到了极大得约束。\n神经网络提取平面 神经网络提取平面的主要流程是利用现有的网络作为编码器对图片信息进行提取，然后通过几个分支将这些信息解码成平面/非平面掩膜、平面实例分割掩膜和深度图，最终整合为完整的3Ｄ 平面模型，流程图如下：\n大部分文献通过聚合像素点来提取平面，主要思路是将平面提取问题转化为语义分割问题或实例 分割问题。也有文献在曼哈顿世界假设下，提取图片中的３Ｄ线框（wireframe）然后提取平面。此方法用直线和连接点组成平面，因此提取出的平面边缘整齐平滑，而聚合像素的方法提取出的平面边缘相对粗糙。但该方法相应地对边缘为曲线的平面提取效果较差。\n本文采纳上图的思想，利用单张RGB图像估计平面/非平面掩膜、平面实例分割掩膜和深度图。由于在本文中，平面提取模块主要用于提取平面区域对应的特征构建平面特征描述子，故不涉及对平面三维信息的准确估计。(为提高平面特征描述子的表达能力，后面也尝试做了归一化深度的估计，但是否对描述子表达能力有促进作用还待实验证明)\n关于平面提取还可以改善的地方 无论是何种方法，提取的平面边缘都存在一定问题：使用像素点聚合提取的平面边缘粗糙不平 滑，而通过直线段构成平面的方法对边缘为曲线的平面提取效果较差。基于单张彩色图输入的平面提取的未来工作主要是进一步提升平面分割和深度估计的精度。除此之外，还有很多拓展研究，比如：\n平面边缘的优化。如果平面由像素点构成，可对平面边缘进行平滑处理，或使用直线/曲线拟 合，或从图片中提取边缘信息后与平面的边缘结合。如果平面由线条构成，可以针对性地使用直线/曲线构成平面。\n遮挡推理。单张彩色图包含的信息有限，从中提取的平面会因遮挡而部分缺失，比如被桌子遮 挡的墙面。使用图片中已有的纹理对平面缺失处进行填充，可极大提高平面重建的完整性和观赏性。\n绝对深度。由于缺失深度的维度，根据单张图片重建出的三维模型与真实世界相差一个因子，即尺度。可以通过图片内特定物体的尺寸来确定绝对深度。\n几何方法和神经网络方法的结合。两类方法各有优缺点，后续工作可通过取长补短的方式结合 两类方法，有以下两种思路可供参考：一是，传统方法难以有效建模图像中的不规则线段等几何信息，因此通过神经网络预测图像中的灭点、线段的位置等能更有效的提取图片中的几何信息，进而用传统几何方法根据已知的几何结构实现更高效的平面提取；二 是，利用神经网络提取的特征点 （例 如SuperPoint）替代传统人工特征点，提取更具有描述性和重复性的特征，进行平面位置以及平面边缘的定位。\n用实例分割提取平面 为了得到平面的特征表示，在本实验将神经网络(骨干网络选择Res101)作为编码器对图片信息进行提取，然后通过几个分支将这些信息解码成平面/非平面掩膜、平面实例分割掩膜和深度图。为了让网络推理速度更快，我没有采用如Mask-rcnn的两阶段法（即先回归锚框位置再实例分割），而是采用论文 1中的单阶段方法，首先通过编码器提取输入图像的高维信息，然后用三个解码器分别学习到平面/非平面掩膜，平面嵌入特征(Plane embeddings)以及深度信息。利用其中的平面嵌入特征和平面/非平面掩膜通过均值漂移(Mean Shift)聚类算法得到平面的分割实例。\nMean Shift聚类算法 由于在实际场景中的平面数不固定，一些受初始聚类中心数目影响的算法(例如K-Means，K-Means++)并不适用于处理聚类个数未知的情形，对此，有一些改进的算法如Mean Shift算法被提出。与K-Means算法一样，Mean Shift算法是基于聚类中心的聚类算法，不同的是，Mean Shift算法不需要事先制定类别个数k。\n对于给定的 $d$ 维空间 $R^d$ 中的 $n$ 个样本点 $x_i,i=1,⋯,n$，对于其中的点 $x$， 定义 Mean Shift向量的基本形式为：\n$$ M_h(x)=\\frac{1}{k}\\sum_{x_i\\in S_h}(x_i-x) $$\n其中，$S_h$ 指的是一个半径为h的高维球区域，如上图中的圆形区域。$S_h$ 的定义为：\n$$ S_h(x)=(y \\mid (y-x)( y-x)^T \\leqslant h^2) $$\n为了使得随着样本与被偏移点的距离不同，其偏移量对均值偏移向量的贡献也不同，需要在Mean Shift算法中引入核函数，此时改进的Mean Shift向量形式：\n$$ M_h(x)=\\frac{\\sum_{i=1}^{n}G(\\frac{x_i-x}{h_i})(x_i-x)}{\\sum_{i=1}^{n}G(\\frac{x_i-x}{h_i})} $$\n其中， $$G(\\frac{x_i-x}{h_i})$$ 为核函数(常见的核函数如高斯核函数，余弦函数等)。\n计算 $M_h$ 时考虑距离的影响，同时也可以认为在所有的样本点 $X$ 中,重要性并不一样,因此对每个样本还引入一个权重系数。如此以来就可以把Mean Shift形式扩展为：\n$$ M_h (x)=\\frac{\\sum_{i=1}^{n}G(\\frac{x_i-x}{h_i})w(x_i)(x_i-x)}{\\sum_{i=1}^{n}G(\\frac{x_i-x}{h_i})w(x_i)} $$\n其中，$w(x_i)$ 是一个赋给采样点的权重。\n对于Mean Shift算法，它是一个迭代的步骤，即先算出当前点的偏移均值，将该点移动到此偏移均值，然后以此为新的起始点，继续移动，直到满足最终的条件。\nMean-Shift 聚类就是对于集合中的每一个元素，对它执行下面的操作：把该元素移动到它邻域中所有元素的特征值的均值的位置，不断重复直到收敛。准确的说，不是真正移动元素，而是把该元素与它的收敛位置的元素标记为同一类。\n一般的均值漂移聚类会在每次迭代中对每个像素有 $N$ 次核函数的计算( $N$ 为像素总数)，计算复杂度是 $O(N^2)$ 。为了提高均值漂移聚类的效率，论文1中生成 $k^d$ 个锚点，其中 $k$ 是每个维度的锚点数，$d$ 是嵌入特征的维度，然后仅仅计算 锚点与每个像素的核函数，计算复杂度是 $O(k^d \\times N)$, 当 $k^d « N$ 时，算法执行效率大大提高。聚类完成后，论文1再将小于设定阈值的锚点合并作为平面实例的聚类中心。\n文章链接\n损失函数 平面/非平面掩膜的损失函数采用平衡交叉熵损失： $$ L_{S}=-(1-w) \\sum_{i \\in \\mathcal{F}} \\log p_{i}-w \\sum_{i \\in \\mathcal{B}} \\log \\left(1-p_{i}\\right) $$\n其中 $\\mathcal{F}$ 和 $\\mathcal{B}$ 是 前景和背景像素的集合， $p_i$ 是第 $i$ 个像素属于前景(平面)的概率， $w$ 是前景和背景的比率，是一个超参数。\n嵌入特征的损失函数采用判别损失(discriminative loss), 该损失包括两部分：pull loss 和 push loss, pull loss将高维嵌入特征向聚类中心靠拢， push loss 使 聚类中心相互远离。\n$$ L_{E}=L_{p u l l}+L_{p u s h} $$\n其中：\n$$ L_{\\text {pull }}=\\frac{1}{C} \\sum_{c=1}^{C} \\frac{1}{N_{c}} \\sum_{i=1}^{N_{c}} \\max \\left(\\left|\\mu_{c}-x_{i}\\right|-\\delta_{\\mathrm{v}}, 0\\right) $$\n$$ L_{\\text {push }}=\\frac{1}{C(C-1)} \\sum_{c_{A}=1}^{C} \\sum_{c_{B}=1}^{C} \\max \\left(\\delta_{\\mathrm{d}}-\\left|\\mu_{c_{A}}-\\mu_{c_{B}}\\right|,0\\right). c_{A} \\neq c_{B} $$\n其中， $C$ 是聚类中心的个数（平面个数）， $N_c$ 是聚类区域 $c$ 中的元素数， $x_i$ 是 嵌入特征， $\\mu_c$ 是聚类区域 $c$ 中的平均嵌入特征， $\\delta_{\\mathrm{v}}$ 和 $\\delta_{\\mathrm{d}}$ 是超参数。\n深度图回归采用 Huber loss $$ L_D = \\operatorname{loss}(x, y)=\\frac{1}{n} \\sum_{i} z_{2} $$\n其中\n$$ z_{\\imath}=0.5\\left(x_{2}-y_{2}\\right)^{2}, \\text { if }\\left|x_{2}-y_{i}\\right|\u003c1 $$\n$$ z_{\\imath}= \\left|x_{\\imath}-y_{\\imath}\\right|-0.5, \\text { otherwise } $$\n实验结果 第一二三四行依次为：平面分割结果， 平面分割和RGB图像混合结果， 平面/非平面掩模， 深度图。第一二三列依次为：原图， 网络预测结果， 真值。\n提取平面特征构建特征描述子 构建图节点 在前面已经得到平面实例分割的前提下，对于每一个平面实例，选用一个矩形包围框，该包围框恰好能包含对应平面实例。然后用该包围框裁取前面特征提取网络编码器输出的深度嵌入特征区域以及对应的深度图对应区域，将两部分区域重采样到 $128 \\times 128 \\times N$， 其中 $N$ 是通道维数，对于深度嵌入特征，$N=64$， 对于深度图 $N=1$。\n为了减低后续的网络的计算复杂度，提取的深度嵌入特征和深度图经过两次 max pooling 和一次 mean pooling， max pooling 同时还具有一定的抗平面旋转/平移的作用。\n深度嵌入特征和深度图经过多层感知机(MLP)统一特征维度后级联在一起当做图网络的节点特征。\n利用图匹配的方法匹配平面 两阶段图匹配算法 论文2 中利用图内卷积和跨图卷积汇聚节点邻域特征，然后通过最优传输理论得到两个图之间的节点匹配关系，该方法由于将节点特征学习和全局匹配分开对待，因此网络运算效率不高；此外， 由于仅考虑局部嵌入，该方法可能倾向于不一致地匹配图之间的邻域。(邻域一致性保证邻域的节点不会匹配到待匹配图的不同区域)\n论文3 提出了一种完全可微分两阶段图匹配的方法，旨在由数据驱动解决 邻域一致的图节点匹配问题，而无需在网络推理阶段解决任何优化问题。\n二阶段图匹配网络结构图如下所示：\n给定源图 $\\mathcal{G}_{s}$ 和 目标图 $\\mathcal{G}_{t}$, 首先通过GNN特征汇聚节点邻域特征, 即 $\\boldsymbol{H}_{s}=\\boldsymbol{\\Psi}_{\\theta_{1}}\\left(\\boldsymbol{X}_{s}, \\boldsymbol{A}_{s}, \\boldsymbol{E}_{s}\\right) \\in \\mathbb{R}^{\\left|\\mathcal{V}_{s}\\right| \\times .}$ 和 $\\boldsymbol{H}_{t}=\\boldsymbol{\\Psi}_{\\theta_{1}}\\left(\\boldsymbol{X}_{t}, \\boldsymbol{A}_{t}, \\boldsymbol{E}_{t}\\right) \\in \\mathbb{R}^{\\left|\\mathcal{V}_{t}\\right| \\times .}$, 其中 ${\\Psi}_{\\theta_{1}}$ 选用：\n$$ \\vec{h}_{i}^{(t+1)}= {\\Psi}_{\\theta_{1}}(\\vec{h}_{i}^{(t)}) = \\sigma\\left(\\boldsymbol{W}^{(t+1)} \\vec{h}_{i}^{(t)}+\\sum_{j \\rightarrow i} \\Phi_{\\theta}^{(t+1)}\\left(\\vec{e}_{j, i}\\right) \\cdot \\vec{h}_{j}^{(t)}\\right) $$\n这样，通过特征汇聚得到 $\\boldsymbol{H}_{s}$ 和 $\\boldsymbol{H}_{t}$ 后，通过矩阵行间的softmax 正则化后，计算源图和目标图的节点相似度，选取相似度最高的当做匹配节点，得到初始的匹配矩阵 $\\boldsymbol{S}^{(0)}$。\n损失函数：\n$$ \\mathcal{L}^{\\text {(initial) }}=-\\sum_{i \\in \\mathcal{V}_{s}} \\log \\left(S_{i, \\pi_{\\mathrm{gr}}(i)}^{(0)}\\right) $$\n其中 $\\pi_{\\mathrm{gr}}$ 是监督标签的目标图中与源图节点 $i$ 匹配的节点。\n由于初始的匹配仅基于局部的匹配，网络有可能会让节点特征相似但不满足局部一致性的节点匹配上，这需要引入二阶段的匹配一致性学习。\n对于二阶段的学习， 给定第 $l$ 次迭代时网络的匹配矩阵是 $S^{(l)}$, 通过共享权重的图网络 $\\Psi_{\\theta_{2}}$ 执行同步消息传递：\n$$ \\boldsymbol{O}_{s}=\\boldsymbol{\\Psi}_{\\theta_{2}}\\left(\\boldsymbol{I}_{\\left|\\mathcal{V}_{s}\\right|}, \\boldsymbol{A}_{s}, \\boldsymbol{E}_{s}\\right) \\quad \\text { and } \\quad \\boldsymbol{O}_{t}=\\boldsymbol{\\Psi}_{\\theta_{2}}\\left(\\boldsymbol{S}_{(l)}^{\\top} \\boldsymbol{I}_{\\left|\\mathcal{V}_{s}\\right|}, \\boldsymbol{A}_{t}, \\boldsymbol{E}_{t}\\right) $$\n上面的 $\\Phi_{\\theta_{1}}$ 和 $\\Phi_{\\theta_{2}}$ 都采用相同的网络结构：\n$$ \\vec{h}_{i}^{(t+1)}=\\sigma\\left(\\boldsymbol{W}^{(t+1)} \\vec{h}_{i}^{(t)}+\\sum_{j \\rightarrow i} \\Phi_{\\theta}^{(t+1)}\\left(\\vec{e}_{j, i}\\right) \\cdot \\vec{h}_{j}^{(t)}\\right) $$\n其中 $\\Phi_{\\theta}(.)$ 是关于边特征的可学习参数。\n通过计算差值向量 $\\vec{d}_{i, j}=\\vec{o}_{i}^{(s)}-\\vec{o}_{j}^{(t)}$ , 更新 匹配矩阵：\n$$ S_{i, j}^{(l+1)}=\\operatorname{softmax}\\left(\\hat{\\boldsymbol{S}}^{(l+1)}\\right)_{i, j} \\text { with } \\hat{S}_{i, j}^{(l+1)}=\\hat{S}_{i, j}^{(l)}+\\Phi_{\\theta_{3}}\\left(\\vec{d}_{j, i}\\right) $$\n其中 $\\Phi_{\\theta_{3}}$ 是多层感知机。\n网络的最终损失由特征匹配损失和邻域一致性损失构成：$\\mathcal{L}=\\mathcal{L}^{(\\text {initial) }}+\\mathcal{L}^{\\text {(refined) }}$ with $\\mathcal{L}^{\\text {(refined) }}=-\\sum_{i \\in \\mathcal{V}_{s}} \\log \\left(S_{i, \\pi_{\\mathrm{m}}(i)}^{(L)}\\right)$ 。\n实验结果 每张图第一第二行是匹配真值，第三四行是网络预测匹配结果。\n参考文献 Yu Z, Zheng J, Lian D, et al. Single-image piece-wise planar 3d reconstruction via associative embedding[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 1029-1037. ↩︎ ↩︎ ↩︎\nR. Wang, J. Yan, and X. Yang. Learning combinatorial embedding networks for deep graph matching. In ICCV, 2019b. ↩︎\nFey M, Lenssen J E, Morris C, et al. Deep graph matching consensus[J]. arXiv preprint arXiv:2001.09621, 2020. ↩︎\n","title":"基于图匹配的平面匹配算法","uri":"/contents/dl/plane_match/"},{"categories":["contents"],"content":"代码主要参考：PlaneNet\n3D数据平面拟合 思路： 为3D数据中每一个已标注和未标注(代码中把未标注的顶点当做一类)的实例拟合平面，然后合并符合指定要求的平面。\n阅读代码前注意的一些概念：\ngroup 代码里面的 group 指的是一个物体实例，如上图中用不同颜色区分开的点云集合。\nsegment 标签相同的点云集合，一个group 里面可能含有多个segment ，一个segment 里可能含有多个平面。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 import xml.etree.ElementTree as ET import numpy as np import cv2 import sys import os from plyfile import PlyData, PlyElement import json import zipfile import glob #ROOT_FOLDER = '/mnt/vision/ScanNet/data/' ROOT_FOLDER = '/home/jiajie/planar_match/PlaneNet/data_preparation/ScanNet/data/' class ColorPalette: def __init__(self, numColors): np.random.seed(2) #self.colorMap = np.random.randint(255, size = (numColors, 3)) #self.colorMap[0] = 0 self.colorMap = np.array([[255, 0, 0], [0, 255, 0], [0, 0, 255], [80, 128, 255], [255, 230, 180], [255, 0, 255], [0, 255, 255], [100, 0, 0], [0, 100, 0], [255, 255, 0], [50, 150, 0], [200, 255, 255], [255, 200, 255], [128, 128, 80], [0, 50, 128], [0, 100, 100], [0, 255, 128], [0, 128, 255], [255, 0, 128], [128, 0, 255], [255, 128, 0], [128, 255, 0], ]) if numColors \u003e self.colorMap.shape[0]: self.colorMap = np.concatenate([self.colorMap, np.random.randint(255, size = (numColors - self.colorMap.shape[0], 3))], axis=0) pass return def getColorMap(self): return self.colorMap def getColor(self, index): if index \u003e= colorMap.shape[0]: print(\"index out of range...\") return np.random.randint(255, size = (3)) else: print(\"get color in colormap\") return self.colorMap[index] pass def writePointCloudFace(filename, points, faces): with open(filename, 'w') as f: header = \"\"\"ply format ascii 1.0 element vertex \"\"\" header += str(len(points)) header += \"\"\" property float x property float y property float z property uchar red { start of vertex color } property uchar green property uchar blue property uchar alpha element face \"\"\" header += str(len(faces)) header += \"\"\" property list uchar int vertex_index end_header \"\"\" f.write(header) for point in points: for value in point[:3]: f.write(str(value) + ' ') continue for value in point[3:]: f.write(str(int(value)) + ' ') continue f.write(str(int(255)) + ' ') f.write('\\n') continue for face in faces: f.write('3 ' + str(face[0]) + ' ' + str(face[1]) + ' ' + str(face[2]) + '\\n') continue f.close() pass return def loadClassMap(): classMap = {} classLabelMap = {} with open(ROOT_FOLDER + 'class_label/scannetv2-labels.combined.tsv') as info_file: line_index = 0 for line in info_file: if line_index \u003e 0: line = line.split('\\t') key = line[1].strip() classMap[key] = line[7].strip() classMap[key + 's'] = line[7].strip() if line[4].strip() != '': label = int(line[4].strip()) else: label = -1 pass classLabelMap[key] = label classLabelMap[key + 's'] = label pass line_index += 1 continue pass return classMap, classLabelMap def fitPlane(points): if points.shape[0] == points.shape[1]: return np.linalg.solve(points, np.ones(points.shape[0])) else: return np.linalg.lstsq(points, np.ones(points.shape[0]), rcond=None)[0] def mergePlanesNew(points, planes, planePointIndices, planeSegments, segmentNeighbors, numPlanes, planeDiffThreshold = 0.05, planeAngleThreshold = 30, inlierThreshold = 0.9, planeAreaThreshold = 10, orthogonalThreshold = np.cos(np.deg2rad(60)), parallelThreshold = np.cos(np.deg2rad(30)), debug=False): fittingErrorThreshold = planeDiffThreshold planeFittingErrors = [] for plane, pointIndices in zip(planes, planePointIndices): XYZ = points[pointIndices] planeNorm = np.linalg.norm(plane) if planeNorm == 0: planeFittingErrors.append(fittingErrorThreshold) continue diff = np.abs(np.matmul(XYZ, plane) - np.ones(XYZ.shape[0])) / planeNorm planeFittingErrors.append(diff.mean()) continue planeList = zip(planes, planePointIndices, planeSegments, planeFittingErrors) planeList = sorted(planeList, key=lambda x:x[3]) ## Merge two planes if they are neighbors and the merged plane has small fitting error while len(planeList) \u003e 0: hasChange = False planeIndex = 0 if debug: for index, planeInfo in enumerate(sorted(planeList, key=lambda x:-len(x[1]))): print(index, planeInfo[0] / np.linalg.norm(planeInfo[0]), planeInfo[2], planeInfo[3]) continue pass while planeIndex \u003c len(planeList): plane, pointIndices, segments, fittingError = planeList[planeIndex] if fittingError \u003e fittingErrorThreshold: break neighborSegments = [] for segment in segments: if segment in segmentNeighbors: neighborSegments += segmentNeighbors[segment] pass continue neighborSegments += list(segments) neighborSegments = set(neighborSegments) bestNeighborPlane = (fittingErrorThreshold, -1, None) for neighborPlaneIndex, neighborPlane in enumerate(planeList): if neighborPlaneIndex \u003c= planeIndex: continue if not bool(neighborSegments \u0026 neighborPlane[2]): continue dotProduct = np.abs(np.dot(neighborPlane[0], plane) / np.maximum(np.linalg.norm(neighborPlane[0]) * np.linalg.norm(plane), 1e-4)) newPointIndices = np.concatenate([neighborPlane[1], pointIndices], axis=0) XYZ = points[newPointIndices] if dotProduct \u003e parallelThreshold and len(neighborPlane[1]) \u003e len(pointIndices) * 0.5: newPlane = fitPlane(XYZ) else: newPlane = plane pass #newPlane = plane diff = np.abs(np.matmul(XYZ, newPlane) - np.ones(XYZ.shape[0])) / np.linalg.norm(newPlane) newFittingError = diff.mean() if debug: print(len(planeList), planeIndex, neighborPlaneIndex, newFittingError, plane / np.linalg.norm(plane), neighborPlane[0] / np.linalg.norm(neighborPlane[0]), dotProduct, orthogonalThreshold) pass if dotProduct \u003c orthogonalThreshold: continue if newFittingError \u003c bestNeighborPlane[0]: newPlaneInfo = [newPlane, newPointIndices, segments.union(neighborPlane[2]), newFittingError] bestNeighborPlane = (newFittingError, neighborPlaneIndex, newPlaneInfo) pass continue if bestNeighborPlane[1] != -1: newPlaneList = planeList[:planeIndex] + planeList[planeIndex + 1:bestNeighborPlane[1]] + planeList[bestNeighborPlane[1] + 1:] newFittingError, newPlaneIndex, newPlane = bestNeighborPlane for newPlaneIndex in range(len(newPlaneList)): if (newPlaneIndex == 0 and newPlaneList[newPlaneIndex][3] \u003e newFittingError) \\ or newPlaneIndex == len(newPlaneList) - 1 \\ or (newPlaneList[newPlaneIndex][3] \u003c newFittingError and newPlaneList[newPlaneIndex + 1][3] \u003e newFittingError): newPlaneList.insert(newPlaneIndex, newPlane) break continue if len(newPlaneList) == 0: newPlaneList = [newPlane] pass planeList = newPlaneList hasChange = True else: planeIndex += 1 pass continue if not hasChange: break continue planeList = sorted(planeList, key=lambda x:-len(x[1])) minNumPlanes, maxNumPlanes = numPlanes if minNumPlanes == 1 and len(planeList) == 0: if debug: print('at least one plane') pass elif len(planeList) \u003e maxNumPlanes: if debug: print('too many planes', len(planeList), maxNumPlanes) pass planeList = planeList[:maxNumPlanes] pass groupedPlanes, groupedPlanePointIndices, groupedPlaneSegments, groupedPlaneFittingErrors = zip(*planeList) groupNeighbors = [] for planeIndex, planeSegments in enumerate(groupedPlaneSegments): neighborSegments = [] for segment in planeSegments: if segment in segmentNeighbors: neighborSegments += segmentNeighbors[segment] pass continue neighborSegments += list(planeSegments) neighborSegments = set(neighborSegments) neighborPlaneIndices = [] for neighborPlaneIndex, neighborPlaneSegments in enumerate(groupedPlaneSegments): if neighborPlaneIndex == planeIndex: continue if bool(neighborSegments \u0026 neighborPlaneSegments): plane = groupedPlanes[planeIndex] neighborPlane = groupedPlanes[neighborPlaneIndex] if np.linalg.norm(plane) * np.linalg.norm(neighborPlane) \u003c 1e-4: continue dotProduct = np.abs(np.dot(plane, neighborPlane) / np.maximum(np.linalg.norm(plane) * np.linalg.norm(neighborPlane), 1e-4)) if dotProduct \u003c orthogonalThreshold: neighborPlaneIndices.append(neighborPlaneIndex) pass pass continue groupNeighbors.append(neighborPlaneIndices) continue if debug and len(groupedPlanes) \u003e 1: print('merging result', [len(pointIndices) for pointIndices in groupedPlanePointIndices], groupedPlaneFittingErrors, groupNeighbors) pass planeList = zip(groupedPlanes, groupedPlanePointIndices, groupNeighbors) return planeList def readMesh(scene_id): filename = ROOT_FOLDER + scene_id + '/' + scene_id + '.aggregation.json' data = json.load(open(filename, 'r')) aggregation = np.array(data['segGroups']) high_res = False if high_res: filename = ROOT_FOLDER + scene_id + '/' + scene_id + '_vh_clean.labels.ply' else: filename = ROOT_FOLDER + scene_id + '/' + scene_id + '_vh_clean_2.labels.ply' pass plydata = PlyData.read(filename) vertices = plydata['vertex'] points = np.stack([vertices['x'], vertices['y'], vertices['z']], axis=1) faces = np.array(plydata['face']['vertex_indices']) semanticSegmentation = vertices['label'] if high_res: filename = ROOT_FOLDER + scene_id + '/' + scene_id + '_vh_clean.segs.json' else: filename = ROOT_FOLDER + scene_id + '/' + scene_id + '_vh_clean_2.0.010000.segs.json' pass data = json.load(open(filename, 'r')) segmentation = np.array(data['segIndices']) groupSegments = [] groupLabels = [] for segmentIndex in range(len(aggregation)): groupSegments.append(aggregation[segmentIndex]['segments']) groupLabels.append(aggregation[segmentIndex]['label']) continue segmentation = segmentation.astype(np.int32) uniqueSegments = np.unique(segmentation).tolist() numSegments = 0 for segments in groupSegments: for segmentIndex in segments: if segmentIndex in uniqueSegments: uniqueSegments.remove(segmentIndex) pass continue numSegments += len(segments) continue for segment in uniqueSegments: groupSegments.append([segment, ]) groupLabels.append('unannotated') continue numGroups = len(groupSegments) numPoints = segmentation.shape[0] numPlanes = 1000 ## Segment connections for plane merging later segmentEdges = [] for faceIndex in range(faces.shape[0]): face = faces[faceIndex] segment_1 = segmentation[face[0]] segment_2 = segmentation[face[1]] segment_3 = segmentation[face[2]] if segment_1 != segment_2 or segment_1 != segment_3: if segment_1 != segment_2 and segment_1 != -1 and segment_2 != -1: segmentEdges.append((min(segment_1, segment_2), max(segment_1, segment_2))) pass if segment_1 != segment_3 and segment_1 != -1 and segment_3 != -1: segmentEdges.append((min(segment_1, segment_3), max(segment_1, segment_3))) pass if segment_2 != segment_3 and segment_2 != -1 and segment_3 != -1: segmentEdges.append((min(segment_2, segment_3), max(segment_2, segment_3))) pass pass continue segmentEdges = list(set(segmentEdges)) numPlanes = 1000 numPlanesPerSegment = 2 segmentRatio = 0.1 planeAreaThreshold = 10 numIterations = 100 numIterationsPair = 1000 planeDiffThreshold = 0.05 fittingErrorThreshold = planeDiffThreshold ## Specify the minimum and maximum number of planes for each object labelNumPlanes = {'wall': [1, 3], 'floor': [1, 1], 'cabinet': [1, 5], 'bed': [1, 5], 'chair': [1, 2], 'sofa': [1, 10], 'table': [1, 5], 'door': [1, 2], 'window': [1, 2], 'bookshelf': [1, 5], 'picture': [1, 1], 'counter': [1, 10], 'blinds': [0, 0], 'desk': [1, 10], 'shelf': [1, 5], 'shelves': [1, 5], 'curtain': [0, 0], 'dresser': [1, 5], 'pillow': [0, 0], 'mirror': [0, 0], 'entrance': [1, 1], 'floor mat': [1, 1], 'clothes': [0, 0], 'ceiling': [1, 5], 'book': [0, 1], 'books': [0, 1], 'refridgerator': [1, 5], 'television': [1, 1], 'paper': [0, 1], 'towel': [0, 1], 'shower curtain': [0, 1], 'box': [1, 5], 'whiteboard': [1, 5], 'person': [0, 0], 'night stand': [1, 5], 'toilet': [0, 5], 'sink': [0, 5], 'lamp': [0, 1], 'bathtub': [0, 5], 'bag': [0, 1], 'otherprop': [0, 5], 'otherstructure': [0, 5], 'otherfurniture': [0, 5], 'unannotated': [0, 5], '': [0, 0], } nonPlanarGroupLabels = ['bicycle', 'bottle', 'water bottle'] nonPlanarGroupLabels = {label: True for label in nonPlanarGroupLabels} verticalLabels = ['wall', 'door', 'cabinet'] classMap, classLabelMap = loadClassMap() allXYZ = points.reshape(-1, 3) segmentNeighbors = {} for segmentEdge in segmentEdges: if segmentEdge[0] not in segmentNeighbors: segmentNeighbors[segmentEdge[0]] = [] pass segmentNeighbors[segmentEdge[0]].append(segmentEdge[1]) if segmentEdge[1] not in segmentNeighbors: segmentNeighbors[segmentEdge[1]] = [] pass segmentNeighbors[segmentEdge[1]].append(segmentEdge[0]) continue planeGroups = [] print('num groups', len(groupSegments)) debug = False debugIndex = -1 ## A group corresponds to an instance in the ScanNet annotation for groupIndex, group in enumerate(groupSegments): if debugIndex != -1 and groupIndex != debugIndex: continue if groupLabels[groupIndex] in nonPlanarGroupLabels: groupLabel = groupLabels[groupIndex] minNumPlanes, maxNumPlanes = 0, 0 elif groupLabels[groupIndex] == 'unannotated': groupLabel = 'unannotated' minNumPlanes, maxNumPlanes = labelNumPlanes[groupLabel] elif groupLabels[groupIndex] in classMap: groupLabel = classMap[groupLabels[groupIndex]] minNumPlanes, maxNumPlanes = labelNumPlanes[groupLabel] else: minNumPlanes, maxNumPlanes = 0, 0 groupLabel = '' pass if maxNumPlanes == 0: pointMasks = [] for segmentIndex in group: pointMasks.append(segmentation == segmentIndex) continue pointIndices = np.any(np.stack(pointMasks, 0), 0).nonzero()[0] groupPlanes = [[np.zeros(3), pointIndices, []]] planeGroups.append(groupPlanes) continue groupPlanes = [] groupPlanePointIndices = [] groupPlaneSegments = [] ## A group contains multiple segments and we run RANSAC for each segment for segmentIndex in group: segmentMask = segmentation == segmentIndex segmentIndices = segmentMask.nonzero()[0] XYZ = allXYZ[segmentMask.reshape(-1)] numPoints = XYZ.shape[0] segmentPlanes = [] segmentPlanePointIndices = [] for c in range(2): if c == 0: ## First try to fit one plane to see if the entire segment is one plane plane = fitPlane(XYZ) diff = np.abs(np.matmul(XYZ, plane) - np.ones(XYZ.shape[0])) / np.linalg.norm(plane) if diff.mean() \u003c fittingErrorThreshold: segmentPlanes.append(plane) segmentPlanePointIndices.append(segmentIndices) break else: ## Run ransac for planeIndex in range(numPlanesPerSegment): if len(XYZ) \u003c planeAreaThreshold: continue bestPlaneInfo = [None, 0, None] for iteration in range(min(XYZ.shape[0], numIterations)): sampledPoints = XYZ[np.random.choice(np.arange(XYZ.shape[0]), size=(3), replace=False)] try: plane = fitPlane(sampledPoints) pass except: continue diff = np.abs(np.matmul(XYZ, plane) - np.ones(XYZ.shape[0])) / np.linalg.norm(plane) inlierMask = diff \u003c planeDiffThreshold numInliers = inlierMask.sum() if numInliers \u003e bestPlaneInfo[1]: bestPlaneInfo = [plane, numInliers, inlierMask] pass continue if bestPlaneInfo[1] \u003c planeAreaThreshold: break pointIndices = segmentIndices[bestPlaneInfo[2]] #bestPlane = bestPlaneInfo[0] bestPlane = fitPlane(XYZ[bestPlaneInfo[2]]) segmentPlanes.append(bestPlane) segmentPlanePointIndices.append(pointIndices) outlierMask = np.logical_not(bestPlaneInfo[2]) segmentIndices = segmentIndices[outlierMask] XYZ = XYZ[outlierMask] continue pass continue if sum([len(indices) for indices in segmentPlanePointIndices]) \u003c numPoints * 0.5: print('not enough fitted points') if len(segmentIndices) \u003e= planeAreaThreshold: groupPlanes.append(np.zeros(3)) groupPlanePointIndices.append(segmentIndices) groupPlaneSegments.append(set([segmentIndex])) pass else: groupPlanes += segmentPlanes groupPlanePointIndices += segmentPlanePointIndices for _ in range(len(segmentPlanes)): groupPlaneSegments.append(set([segmentIndex])) continue pass continue if len(groupPlanes) \u003e 0: ## Merge planes of each instance groupPlanes = mergePlanesNew(points, groupPlanes, groupPlanePointIndices, groupPlaneSegments, segmentNeighbors, numPlanes=(minNumPlanes, maxNumPlanes), planeDiffThreshold=planeDiffThreshold, planeAreaThreshold=planeAreaThreshold, debug=debugIndex != -1) pass if debug: print('group', groupIndex, groupLabels[groupIndex], groupLabel, len(groupPlanes)) pass planeGroups.append(groupPlanes) continue if debug: #colorMap = np.random.randint(255, size=(segmentation.max() + 2, 3)) colorMap = ColorPalette(segmentation.max() + 2).getColorMap() colorMap[-1] = 0 colorMap[-2] = 255 annotationFolder = 'test/' #colorMap = np.tile(np.expand_dims(np.arange(256), -1), [1, 3]) else: #colorMap = ColorPalette(segmentation.max() + 2).getColorMap() # numPlanes = sum([len(group) for group in planeGroups]) #print('num planes', numPlanes) #exit(1) # segmentationColor = (np.arange(numPlanes) + 1) * 100 # colorMap = np.stack([segmentationColor / (256 * 256), segmentationColor / 256 % 256, segmentationColor % 256], axis=1) colorMap = ColorPalette(numPlanes).getColorMap() # print('colorMap: ',colorMap) # colorMap[-1] = 255 annotationFolder = ROOT_FOLDER + scene_id + '/annotation/' pass if debug: colors = colorMap[segmentation] writePointCloudFace(annotationFolder + '/segments.ply', np.concatenate([points, colors], axis=-1), faces) groupedSegmentation = np.full(segmentation.shape, fill_value=-1) for segmentIndex in range(len(aggregation)): indices = aggregation[segmentIndex]['segments'] for index in indices: groupedSegmentation[segmentation == index] = segmentIndex continue continue groupedSegmentation = groupedSegmentation.astype(np.int32) colors = colorMap[groupedSegmentation] writePointCloudFace(annotationFolder + '/groups.ply', np.concatenate([points, colors], axis=-1), faces) pass planes = [] planePointIndices = [] for index, group in enumerate(planeGroups): groupPlanes, groupPlanePointIndices, groupNeighbors = zip(*group) planes += groupPlanes planePointIndices += groupPlanePointIndices continue planeSegmentation = np.full(segmentation.shape, fill_value=-1, dtype=np.int32) for planeIndex, planePoints in enumerate(planePointIndices): if np.linalg.norm(planes[planeIndex]) \u003c 1e-4: planeSegmentation[planePoints] = -2 else: planeSegmentation[planePoints] = planeIndex pass continue if debug: groupSegmentation = np.full(segmentation.shape, fill_value=-1, dtype=np.int32) structureSegmentation = np.full(segmentation.shape, fill_value=-1, dtype=np.int32) typeSegmentation = np.full(segmentation.shape, fill_value=-1, dtype=np.int32) for planeIndex, planePoints in enumerate(planePointIndices): if len(planeInfo[planeIndex]) \u003e 1: structureSegmentation[planePoints] = planeInfo[planeIndex][1][0] typeSegmentation[planePoints] = np.maximum(typeSegmentation[planePoints], planeInfo[planeIndex][1][1] - 2) pass groupSegmentation[planePoints] = planeInfo[planeIndex][0][0] continue colors = colorMap[groupSegmentation] writePointCloudFace(annotationFolder + '/group.ply', np.concatenate([points, colors], axis=-1), faces) colors = colorMap[structureSegmentation] writePointCloudFace(annotationFolder + '/structure.ply', np.concatenate([points, colors], axis=-1), faces) colors = colorMap[typeSegmentation] writePointCloudFace(annotationFolder + '/type.ply', np.concatenate([points, colors], axis=-1), faces) pass planes = np.array(planes) print('number of planes: ', planes.shape[0]) planesD = 1.0 / np.maximum(np.linalg.norm(planes, axis=-1, keepdims=True), 1e-4) planes *= pow(planesD, 2) ## Remove boundary faces for rendering purpose removeIndices = [] for faceIndex in range(faces.shape[0]): face = faces[faceIndex] segment_1 = planeSegmentation[face[0]] segment_2 = planeSegmentation[face[1]] segment_3 = planeSegmentation[face[2]] if segment_1 != segment_2 or segment_1 != segment_3: removeIndices.append(faceIndex) pass continue faces = np.delete(faces, removeIndices) colors = colorMap[planeSegmentation] writePointCloudFace(annotationFolder + '/planes.ply', np.concatenate([points, colors], axis=-1), faces) if debug: exit(1) pass np.save(annotationFolder + '/planes.npy', planes) return if __name__=='__main__': scene_ids = os.listdir(ROOT_FOLDER) scene_ids = scene_ids for scene_id in scene_ids: if scene_id[:5] != 'scene': continue if not os.path.exists(ROOT_FOLDER + '/' + scene_id + '/annotation'): os.system('mkdir -p ' + ROOT_FOLDER + '/' + scene_id + '/annotation') pass if not os.path.exists(ROOT_FOLDER + '/' + scene_id + '/annotation/segmentation'): os.system('mkdir -p ' + ROOT_FOLDER + '/' + scene_id + '/annotation/segmentation') pass print(scene_id) ## Download if not exists if not os.path.exists(ROOT_FOLDER + '/' + scene_id + '/' + scene_id + '.aggregation.json'): print('file not download') pass print('plane fitting', scene_id) if not os.path.exists(ROOT_FOLDER + '/' + scene_id + '/annotation/planes.ply'): readMesh(scene_id) pass # ## Use a C++ program built upon OpenGL to render the 3D plane fitting results to each view # if len(glob.glob(ROOT_FOLDER + '/' + scene_id + '/annotation/segmentation/*.png')) \u003c len(glob.glob(ROOT_FOLDER + '/' + scene_id + '/pose/*.txt')): # cmd = '/home/jiajie/planar_match/PlaneNet/data_preparation/Renderer/build/Renderer --scene_id=' + scene_id + ' --root_folder=' + ROOT_FOLDER # os.system(cmd) # pass # continue 二维平面分割以及平面匹配 首先用opengl渲染网格到RGB图 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 import os import sys import cv2 import argparse import numpy as np import trimesh import pyrender import matplotlib.pyplot as plt from tqdm import tqdm,trange sys.path.append('../') from utils.functions import get_filelist,get_suffix def parse_args(): parser = argparse.ArgumentParser(description='Render mesh to images in different poses') parser.add_argument('--root_folder', help='Working Folder', default= \"/home/jiajie/planar_match/PlaneNet/data_preparation/ScanNet/data\", type=str) parser.add_argument('--seg_imgs_folder', help='Working Folder', default= \"/home/jiajie/planar_match/graph_plane_matching/data_process/seg_images\", type=str) # parser.add_argument('--seg_masks_folder', # help='Working Folder', # default= \"/home/jiajie/planar_match/graph_plane_matching/data_process/seg_masks\", # type=str) args = parser.parse_args() return args def get_pose_gl(opencv_camera_pose_inv): \"\"\" input: the inverse of camera pose(you can use cv2.solvePnPRansac to produce the input): _,rvec,tvec,inliners = cv2.solvePnPRansac(pt13, pt2, K, None) R = cv2.Rodrigues(rvec)[0] T = tvec #inverse R = R.transpose() T = -R.dot(T) # the pose offered by ScanNet datasets is opencv_camera_pose_inv, not the opencv_camera_pose(solvePnPRansac) return: camera_pose: camera_pose \"\"\" #matx solve the different coordinate between opencv and opengl matx = np.array([[1, 0, 0,0 ],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]]) R = opencv_camera_pose_inv[0:3,0:3] T = opencv_camera_pose_inv[0:3,3:4] camera_pose = np.eye(4) camera_pose[0:3,0:3] = R camera_pose[0:3,3:4] = T camera_pose = camera_pose.dot(matx) return camera_pose def read_pose_in_txt(pose_dir): opencv_camera_pose_inv=np.loadtxt(pose_dir,dtype=np.float64) return opencv_camera_pose_inv def gl_renderer(mesh_path, camera_pose_path, save_dir, is_visualize = False): \"\"\" input: path of model, camera_pose(opencv_camera_pose_inv) output: .png file rendered by the mesh model and different camera views(gl) \"\"\" fuze_trimesh = trimesh.load(mesh_path) # print('raw color: ',fuze_trimesh.visual.vertex_colors) # fuze_trimesh.visual.vertex_colors = np.random.uniform(size=fuze_trimesh.vertices.shape) # fuze_trimesh.visual.face_colors = np.random.uniform(size=fuze_trimesh.faces.shape) mesh = pyrender.Mesh.from_trimesh(fuze_trimesh) pose_path_lists = [] get_filelist(camera_pose_path,pose_path_lists) for i in trange(len(pose_path_lists)): pose_path = pose_path_lists[i] scene = pyrender.Scene() #add mesh room_node = scene.add(mesh,pose=np.eye(4)) #add camera camera = pyrender.IntrinsicsCamera(fx=1169.62,fy=1167.11,cx=646.295,cy=489.927,znear=0.05, zfar=100.0) light = pyrender.DirectionalLight(color=np.ones(3), intensity=1) # light = pyrender.SpotLight(color=np.ones(3), intensity=10.0, # innerConeAngle=np.pi/16, outerConeAngle=np.pi/6) # light = pyrender.PointLight(color=np.ones(3), intensity=10.0) scene.add(light, pose=np.eye(4)) # pyrender.Viewer(scene, use_raymond_lighting=True) camera_pose_inv = read_pose_in_txt(pose_path) camera_pose = get_pose_gl(camera_pose_inv) scene.add(camera, pose=camera_pose) # scene.set_pose(room_node,pose=camera_pose) r = pyrender.OffscreenRenderer(viewport_width=1296, viewport_height=968, point_size=1.0) \"\"\"Render the color buffer flat, with no lighting computations.\"\"\" flags = pyrender.RenderFlags.FLAT color, _ = r.render(scene,flags) suffix = get_suffix(pose_path) color_name = pose_path.split(\"/\")[-1].replace(f'{suffix}',\"\") cv2.imwrite(f'{save_dir}/{color_name}.jpg',color) # print('render color: ',color) r.delete() if(is_visualize): plt.figure(figsize=(8,8)) plt.imshow(color) plt.show() def main(): args = parse_args() ROOT_FOLDER = args.root_folder scene_ids = os.listdir(ROOT_FOLDER) for i in trange(len(scene_ids)): scene_id = scene_ids[i] if scene_id[:5] != 'scene': continue if not os.path.exists(ROOT_FOLDER + '/' + scene_id + '/annotation'): # os.system('mkdir -p ' + ROOT_FOLDER + '/' + scene_id + '/annotation') os.makedirs(f'{ROOT_FOLDER}/{scene_id}/annotation') mesh_path = f'{ROOT_FOLDER}/{scene_id}/annotation/planes.ply' mesh_mask_path = f'{ROOT_FOLDER}/{scene_id}/annotation/planes_mask.ply' pose_path = f'{ROOT_FOLDER}/{scene_id}/pose' seg_path = f'{args.seg_imgs_folder}/{scene_id}' # seg_mask_path = f'{args.seg_masks_folder}/{scene_id}' if not os.path.exists(seg_path): # os.system('mkdir -p ' + seg_path) os.makedirs(seg_path) # if not os.path.exists(seg_mask_path): # # os.system('mkdir -p ' + seg_path) # os.makedirs(seg_mask_path) gl_renderer(mesh_path, pose_path, seg_path, False) # gl_renderer(mesh_mask_path, pose_path, seg_mask_path, False) if __name__=='__main__': main() 为每张图像保存对应的数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 import os import sys import cv2 import math import argparse import numpy as np from tqdm import tqdm,trange sys.path.append('../') from utils.functions import get_filelist,get_suffix,plot_cv_image def parse_args(): parser = argparse.ArgumentParser(description='Render mesh to images in different poses') parser.add_argument('--seg_imgs_folder', help='segmentation images folder', default= \"/home/jiajie/planar_match/graph_plane_matching/data_process/seg_images\", type=str) parser.add_argument('--raw_imgs_folder', help='ScanNet raw images folder', default= \"/home/jiajie/planar_match/PlaneNet/data_preparation/ScanNet/data\", type=str) parser.add_argument('--npz_save_folder', help='npz files save folder', default= \"/home/jiajie/planar_match/graph_plane_matching/data_process/npz_datas\", type=str) parser.add_argument( '--resize', type=int, nargs='+', default=[640, 480], help='resize the input image') parser.add_argument( '--min_nums_in_plane', type=int, default=400, help='min num of pixel in one plane') args = parser.parse_args() return args def save_npz_file(raw_img_path, seg_img_path, camera_pose_path, npz_save_path): #read img raw_img = cv2.imread(raw_img_path, flags = 1) seg_img = cv2.imread(seg_img_path, flags = 2) #resize size = args.resize resized_raw = cv2.resize(raw_img, tuple(size), interpolation = cv2.INTER_AREA) resized_seg = cv2.resize(seg_img, tuple(size), interpolation = cv2.INTER_AREA) #bgr 2 rgb rgb_array = resized_raw[..., ::-1] # seg_array = resized_seg[..., ::-1] # plot_image(resized_seg) rgb_array = np.asarray(rgb_array).astype(np.uint8) seg_array = np.asarray(resized_seg).astype(np.uint8) planar_nonplanar_array = np.asarray(resized_seg).astype(np.uint8) unique_seg = np.unique(seg_array).tolist() min_nums = args.min_nums_in_plane for us in unique_seg: if(seg_array[np.where(seg_array == us)].shape[0]\u003cmin_nums): seg_array[np.where(seg_array == us)] = 0 seg_array[np.where(seg_array == 255)] = 0 #plane area or not in plane area planar_nonplanar_array[np.where(seg_array == 0)] = 0 planar_nonplanar_array[np.where(seg_array != 0)] = 255 kernel = np.ones((4,4),np.uint8) #开运算，则先腐蚀再膨胀，用于去噪以及物体边缘二值化 seg_array = cv2.morphologyEx(seg_array, cv2.MORPH_OPEN, kernel) planar_nonplanar_array = cv2.morphologyEx(planar_nonplanar_array, cv2.MORPH_OPEN, kernel) # plot_cv_image(planar_nonplanar_array) # plot_cv_image(seg_array) seg_mask_value = np.unique(seg_array) segmentation = [] for smv in seg_mask_value.tolist(): segmentation.append(np.where(seg_array == smv)) seg_array = np.array(segmentation) data_info_dict = {} data_info_dict['num_planes'] = seg_mask_value.shape[0] data_info_dict['image_name'] = raw_img_path.split(\"/\")[-1] camera_pose = np.loadtxt(camera_pose_path) data_info_dict['camera_pose'] = camera_pose #save npz file # image:rgb format data # seg_mask_value: values of different seg_mask # segmentation: pixel index of differnet seg_mask np.savez(npz_save_path, image=rgb_array, seg_mask_value=seg_mask_value, segmentation=seg_array,planar_non_planar_mask = planar_nonplanar_array, data_info=data_info_dict) def main(): seg_folder = args.seg_imgs_folder scene_ids = os.listdir(seg_folder) for n in trange(len(scene_ids)): scene_id = scene_ids[n] if scene_id[:5] != 'scene': continue seg_imgs_path = f'{seg_folder}/{scene_id}' seg_imgs_lists = [] get_filelist(seg_imgs_path,seg_imgs_lists) raw_imgs_path = f'{args.raw_imgs_folder}/{scene_id}/color' camera_pose_path = f'{args.raw_imgs_folder}/{scene_id}/pose' npz_save_folder = f'{args.npz_save_folder}/{scene_id}' print(npz_save_folder) if not os.path.exists(npz_save_folder): os.makedirs(npz_save_folder) print(npz_save_folder) for i in trange(len(seg_imgs_lists)): seg_img = seg_imgs_lists[i] raw_img = f'{raw_imgs_path}/{seg_img.split(\"/\")[-1]}' suffix = get_suffix(raw_img) camera_pose_name = seg_img.split('/')[-1].replace(suffix,'.txt') camera_pose = f\"{camera_pose_path}/{camera_pose_name}\" npz_save_path = f'{npz_save_folder}/'+ seg_img.split('/')[-1].replace(suffix,'.npz') save_npz_file(raw_img, seg_img, camera_pose, npz_save_path) if __name__ == \"__main__\": args = parse_args() main() 根据分割的具体像素值以及相机位姿寻找匹配平面 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 import sys import cv2 import os import math import numpy as np import argparse from tqdm import tqdm,trange from matplotlib import pyplot as plt sys.path.append('../') from utils.functions import get_filelist,get_suffix,plot_image def parse_args(): parser = argparse.ArgumentParser(description='Render mesh to images in different poses') parser.add_argument('--npz_files_folder', help='npz files folder', default= \"/home/jiajie/planar_match/graph_plane_matching/data_process/npz_datas\", type=str) parser.add_argument('--min_value', type=float, nargs='+', default=[0.98, 1], help='min cos angle value threshold of two cameras') parser.add_argument('--match_planes_folder', help='files folder store which images have neighbor relationsive', default= \"/home/jiajie/planar_match/graph_plane_matching/data_process/match_planes\", type=str) args = parser.parse_args() return args def find_match_plane(npz_files_floder, match_planes_folder): npz_files_list = [] get_filelist(npz_files_floder,npz_files_list) for i in trange(len(npz_files_list)): data_s = np.load(npz_files_list[i],allow_pickle=True) camera_pose_s = data_s['data_info'].item()['camera_pose'] img_id_s = data_s['data_info'].item()['image_name'] suffix = get_suffix(img_id_s) img_id_s = img_id_s.replace(f\"{suffix}\",\"\") nunm_palnes_s = data_s['data_info'].item()['num_planes'] # rotation vector R_matrix_s = camera_pose_s[0:3,0:3] r_vec_s = cv2.Rodrigues(R_matrix_s)[0] r_vec_s = np.swapaxes(r_vec_s,0,1) t_vec_s = camera_pose_s[0:3,3:4] # translation vector for j in range(i,len(npz_files_list)): if npz_files_list[i].split(\"/\")[-1] == npz_files_list[j].split(\"/\")[-1]: continue data_t = np.load(npz_files_list[j],allow_pickle=True) camera_pose_t = data_t['data_info'].item()['camera_pose'] # rotation vector R_matrix_t = camera_pose_t[0:3,0:3] r_vec_t = cv2.Rodrigues(R_matrix_t)[0] t_vec_t = camera_pose_t[0:3,3:4] #caculate cos angle: cos=ab/|a|*|b| cos_value = np.dot(r_vec_s, r_vec_t)/(np.linalg.norm(r_vec_s) * np.linalg.norm(r_vec_t)) t_vec_diff = np.linalg.norm(t_vec_t-t_vec_s) if cos_value \u003e args.min_value[0] and t_vec_diff \u003c args.min_value[1]: img_id_t = data_t['data_info'].item()['image_name'] # plot_image(name=img_id_t,img=data_t['image']) img_id_t = data_t['data_info'].item()['image_name'] suffix = get_suffix(img_id_t) img_id_t = img_id_t.replace(f\"{suffix}\",\"\") #generate plane match relationsive nunm_palnes_t = data_t['data_info'].item()['num_planes'] Max_num_planes = nunm_palnes_s if nunm_palnes_s \u003e nunm_palnes_t else nunm_palnes_t match_plane_list = [] for k in range(nunm_palnes_s): #get seg mask value seg_mask_value_s = data_s['seg_mask_value'][k] target_index = np.where(data_t['seg_mask_value']==seg_mask_value_s) if target_index[0].shape[0] != 0: match_plane_list.append([k,target_index[0][0]]) else: match_plane_list.append([k,-1]) #save txt file match_name = f'{img_id_s}_{img_id_t}.txt' np.set_printoptions(suppress=True) np.savetxt(f'{match_planes_folder}/{match_name}',np.array(match_plane_list)) #save visualize imgs source_img = np.asarray(data_s['planar_non_planar_mask']).astype(np.uint8) for num in range(data_s['segmentation'].shape[0]): seg = tuple(data_s['segmentation'][num]) source_img[seg] = data_s['seg_mask_value'][num] target_img = np.asarray(data_t['planar_non_planar_mask']).astype(np.uint8) for num in range(data_t['segmentation'].shape[0]): seg = tuple(data_t['segmentation'][num]) target_img[seg] = data_t['seg_mask_value'][num] fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), sharex=False, sharey=False) ax[0].set_title('image_s') ax[0].imshow(source_img,cmap=\"bone\") ax[1].set_title('image_t') ax[1].imshow(target_img,cmap=\"bone\") match_plane_name = f'{img_id_s}_{img_id_t}.jpg' # plt.show() plt.savefig(f'{match_planes_folder}/{match_plane_name}') def main(): scene_ids = os.listdir(args.npz_files_folder) for scene_id in scene_ids: match_planes_folder = f'{args.match_planes_folder}/{scene_id}' npz_files_floder = f'{args.npz_files_folder}/{scene_id}' if not os.path.exists(match_planes_folder): os.makedirs(match_planes_folder) find_match_plane(npz_files_floder, match_planes_folder) if __name__ == \"__main__\": args = parse_args() main() ","title":"ScanNet 数据集平面拟合和平面匹配","uri":"/contents/datasets/scannet_process1/"},{"categories":["contents"],"content":"\nDown by the salley gardens\n园柳深深\nMy love and I did meet\n伊人相遇\nShe passed the salley gardens\n如玉纤足\nWith little snow white feet\n越阡步细\nShe bid me take love easy\n嘱予素心\nAs the leaves grow on the tree\n如柳见绿\nBut I being young and foolish\n少不经事\nWith her would not agree\n不解其义\nIn a field by the river\n在水之湄\nMy love and I did stand\n伊人并立\nAnd on my leaning shoulder\n肩头微移\nShe laid her snow-white hand\n皓腕于侧\nShe bid me take life easy\n嘱予素心\nAs the grass grows on the weirs\n盈堤凝碧\nBut I was young and foolish\n少不经事\nAnd now am full of tears\n泪满襟衣\n","title":"Down by the salley gardens","uri":"/contents/folk/down_by_the_salley_gardens/"},{"categories":["contents"],"content":"scanNet数据集下载\n下载脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 #!/usr/bin/env python # Downloads ScanNet public data release # Run with ./download-scannet.py (or python download-scannet.py on Windows) # -*- coding: utf-8 -*- import argparse import os #import urllib.request (for python3) import urllib import tempfile BASE_URL = 'http://kaldir.vc.in.tum.de/scannet/' TOS_URL = BASE_URL + 'ScanNet_TOS.pdf' FILETYPES = ['.aggregation.json', '.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.0.010000.segs.json', '_vh_clean_2.ply', '_vh_clean.segs.json', '_vh_clean.aggregation.json', '_vh_clean_2.labels.ply', '_2d-instance.zip', '_2d-instance-filt.zip', '_2d-label.zip', '_2d-label-filt.zip'] FILETYPES_TEST = ['.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.ply'] PREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB'] TEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB'] LABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv'] RELEASES = ['v2/scans', 'v1/scans'] RELEASES_TASKS = ['v2/tasks', 'v1/tasks'] RELEASES_NAMES = ['v2', 'v1'] RELEASE = RELEASES[0] RELEASE_TASKS = RELEASES_TASKS[0] RELEASE_NAME = RELEASES_NAMES[0] LABEL_MAP_FILE = LABEL_MAP_FILES[0] RELEASE_SIZE = '1.2TB' V1_IDX = 1 def get_release_scans(release_file): #scan_lines = urllib.request.urlopen(release_file) scan_lines = urllib.urlopen(release_file) scans = [] for scan_line in scan_lines: scan_id = scan_line.decode('utf8').rstrip('\\n') scans.append(scan_id) return scans def download_release(release_scans, out_dir, file_types, use_v1_sens): if len(release_scans) == 0: return print('Downloading ScanNet ' + RELEASE_NAME + ' release to ' + out_dir + '...') for scan_id in release_scans: scan_out_dir = os.path.join(out_dir, scan_id) download_scan(scan_id, scan_out_dir, file_types, use_v1_sens) print('Downloaded ScanNet ' + RELEASE_NAME + ' release.') def download_file(url, out_file): out_dir = os.path.dirname(out_file) if not os.path.isdir(out_dir): os.makedirs(out_dir) if not os.path.isfile(out_file): print('\\t' + url + ' \u003e ' + out_file) fh, out_file_tmp = tempfile.mkstemp(dir=out_dir) f = os.fdopen(fh, 'w') f.close() #urllib.request.urlretrieve(url, out_file_tmp) urllib.urlretrieve(url, out_file_tmp) os.rename(out_file_tmp, out_file) else: print('WARNING: skipping download of existing file ' + out_file) def download_scan(scan_id, out_dir, file_types, use_v1_sens): print('Downloading ScanNet ' + RELEASE_NAME + ' scan ' + scan_id + ' ...') if not os.path.isdir(out_dir): os.makedirs(out_dir) for ft in file_types: v1_sens = use_v1_sens and ft == '.sens' url = BASE_URL + RELEASE + '/' + scan_id + '/' + scan_id + ft if not v1_sens else BASE_URL + RELEASES[V1_IDX] + '/' + scan_id + '/' + scan_id + ft out_file = out_dir + '/' + scan_id + ft download_file(url, out_file) print('Downloaded scan ' + scan_id) def download_task_data(out_dir): print('Downloading ScanNet v1 task data...') files = [ LABEL_MAP_FILES[V1_IDX], 'obj_classification/data.zip', 'obj_classification/trained_models.zip', 'voxel_labeling/data.zip', 'voxel_labeling/trained_models.zip' ] for file in files: url = BASE_URL + RELEASES_TASKS[V1_IDX] + '/' + file localpath = os.path.join(out_dir, file) localdir = os.path.dirname(localpath) if not os.path.isdir(localdir): os.makedirs(localdir) download_file(url, localpath) print('Downloaded task data.') def download_label_map(out_dir): print('Downloading ScanNet ' + RELEASE_NAME + ' label mapping file...') files = [ LABEL_MAP_FILE ] for file in files: url = BASE_URL + RELEASE_TASKS + '/' + file localpath = os.path.join(out_dir, file) localdir = os.path.dirname(localpath) if not os.path.isdir(localdir): os.makedirs(localdir) download_file(url, localpath) print('Downloaded ScanNet ' + RELEASE_NAME + ' label mapping file.') def main(): parser = argparse.ArgumentParser(description='Downloads ScanNet public data release.') parser.add_argument('-o', '--out_dir', required=True, help='directory in which to download') parser.add_argument('--task_data', action='store_true', help='download task data (v1)') parser.add_argument('--label_map', action='store_true', help='download label map file') parser.add_argument('--v1', action='store_true', help='download ScanNet v1 instead of v2') parser.add_argument('--id', help='specific scan id to download') parser.add_argument('--preprocessed_frames', action='store_true', help='download preprocessed subset of ScanNet frames (' + PREPROCESSED_FRAMES_FILE[1] + ')') parser.add_argument('--test_frames_2d', action='store_true', help='download 2D test frames (' + TEST_FRAMES_FILE[1] + '; also included with whole dataset download)') parser.add_argument('--type', help='specific file type to download (.aggregation.json, .sens, .txt, _vh_clean.ply, _vh_clean_2.0.010000.segs.json, _vh_clean_2.ply, _vh_clean.segs.json, _vh_clean.aggregation.json, _vh_clean_2.labels.ply, _2d-instance.zip, _2d-instance-filt.zip, _2d-label.zip, _2d-label-filt.zip)') args = parser.parse_args() print('By pressing any key to continue you confirm that you have agreed to the ScanNet terms of use as described at:') print(TOS_URL) print('***') print('Press any key to continue, or CTRL-C to exit.') key = raw_input('') if args.v1: global RELEASE global RELEASE_TASKS global RELEASE_NAME global LABEL_MAP_FILE RELEASE = RELEASES[V1_IDX] RELEASE_TASKS = RELEASES_TASKS[V1_IDX] RELEASE_NAME = RELEASES_NAMES[V1_IDX] LABEL_MAP_FILE = LABEL_MAP_FILES[V1_IDX] release_file = BASE_URL + RELEASE + '.txt' release_scans = get_release_scans(release_file) file_types = FILETYPES; release_test_file = BASE_URL + RELEASE + '_test.txt' release_test_scans = get_release_scans(release_test_file) file_types_test = FILETYPES_TEST; out_dir_scans = os.path.join(args.out_dir, 'scans') out_dir_test_scans = os.path.join(args.out_dir, 'scans_test') out_dir_tasks = os.path.join(args.out_dir, 'tasks') if args.type: # download file type file_type = args.type if file_type not in FILETYPES: print('ERROR: Invalid file type: ' + file_type) return file_types = [file_type] if file_type in FILETYPES_TEST: file_types_test = [file_type] else: file_types_test = [] if args.task_data: # download task data download_task_data(out_dir_tasks) elif args.label_map: # download label map file download_label_map(args.out_dir) elif args.preprocessed_frames: # download preprocessed scannet_frames_25k.zip file if args.v1: print('ERROR: Preprocessed frames only available for ScanNet v2') print('You are downloading the preprocessed subset of frames ' + PREPROCESSED_FRAMES_FILE[0] + ' which requires ' + PREPROCESSED_FRAMES_FILE[1] + ' of space.') download_file(os.path.join(BASE_URL, RELEASE_TASKS, PREPROCESSED_FRAMES_FILE[0]), os.path.join(out_dir_tasks, PREPROCESSED_FRAMES_FILE[0])) elif args.test_frames_2d: # download test scannet_frames_test.zip file if args.v1: print('ERROR: 2D test frames only available for ScanNet v2') print('You are downloading the 2D test set ' + TEST_FRAMES_FILE[0] + ' which requires ' + TEST_FRAMES_FILE[1] + ' of space.') download_file(os.path.join(BASE_URL, RELEASE_TASKS, TEST_FRAMES_FILE[0]), os.path.join(out_dir_tasks, TEST_FRAMES_FILE[0])) elif args.id: # download single scan scan_id = args.id is_test_scan = scan_id in release_test_scans if scan_id not in release_scans and (not is_test_scan or args.v1): print('ERROR: Invalid scan id: ' + scan_id) else: out_dir = os.path.join(out_dir_scans, scan_id) if not is_test_scan else os.path.join(out_dir_test_scans, scan_id) scan_file_types = file_types if not is_test_scan else file_types_test use_v1_sens = not is_test_scan if not is_test_scan and not args.v1 and '.sens' in scan_file_types: print('Note: ScanNet v2 uses the same .sens files as ScanNet v1: Press \\'n\\' to exclude downloading .sens files for each scan') key = raw_input('') if key.strip().lower() == 'n': scan_file_types.remove('.sens') download_scan(scan_id, out_dir, scan_file_types, use_v1_sens) else: # download entire release if len(file_types) == len(FILETYPES): print('WARNING: You are downloading the entire ScanNet ' + RELEASE_NAME + ' release which requires ' + RELEASE_SIZE + ' of space.') else: print('WARNING: You are downloading all ScanNet ' + RELEASE_NAME + ' scans of type ' + file_types[0]) print('Note that existing scan directories will be skipped. Delete partially downloaded directories to re-download.') print('***') print('Press any key to continue, or CTRL-C to exit.') key = raw_input('') if not args.v1 and '.sens' in file_types: print('Note: ScanNet v2 uses the same .sens files as ScanNet v1: Press \\'n\\' to exclude downloading .sens files for each scan') key = raw_input('') if key.strip().lower() == 'n': file_types.remove('.sens') download_release(release_scans, out_dir_scans, file_types, use_v1_sens=True) if not args.v1: download_label_map(args.out_dir) download_release(release_test_scans, out_dir_test_scans, file_types_test, use_v1_sens=False) download_file(os.path.join(BASE_URL, RELEASE_TASKS, TEST_FRAMES_FILE[0]), os.path.join(out_dir_tasks, TEST_FRAMES_FILE[0])) if __name__ == \"__main__\": main() ","title":"ScanNet 数据集","uri":"/contents/datasets/scannet/"},{"categories":["contents"],"content":"pytorch 搭建 ResNet网络结构 网络结构图 实现代码 参考 1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 import torch.nn as nn from torch.nn import functional as F class ResNetModel(nn.Module): \"\"\" 实现通用的ResNet模块，可根据需要定义 \"\"\" def __init__(self, num_classes=1000, layer_num=[],bottleneck = False): super(ResNetModel, self).__init__() #conv1 self.pre = nn.Sequential( #in 224*224*3 nn.Conv2d(3,64,7,2,3,bias=False), #输入通道3，输出通道64，卷积核7*7*64，步长2,根据以上计算出padding=3 #out 112*112*64 nn.BatchNorm2d(64), #输入通道C = 64 nn.ReLU(inplace=True), #inplace=True, 进行覆盖操作 # out 112*112*64 nn.MaxPool2d(3,2,1), #池化核3*3，步长2,计算得出padding=1; # out 56*56*64 ) if bottleneck: #resnet50以上使用BottleNeckBlock self.residualBlocks1 = self.add_layers(64, 256, layer_num[0], 64, bottleneck=bottleneck) self.residualBlocks2 = self.add_layers(128, 512, layer_num[1], 256, 2,bottleneck) self.residualBlocks3 = self.add_layers(256, 1024, layer_num[2], 512, 2,bottleneck) self.residualBlocks4 = self.add_layers(512, 2048, layer_num[3], 1024, 2,bottleneck) self.fc = nn.Linear(2048, num_classes) else: #resnet34使用普通ResidualBlock self.residualBlocks1 = self.add_layers(64,64,layer_num[0]) self.residualBlocks2 = self.add_layers(64,128,layer_num[1]) self.residualBlocks3 = self.add_layers(128,256,layer_num[2]) self.residualBlocks4 = self.add_layers(256,512,layer_num[3]) self.fc = nn.Linear(512, num_classes) def add_layers(self, inchannel, outchannel, nums, pre_channel=64, stride=1, bottleneck=False): layers = [] if bottleneck is False: #添加大模块首层, 首层需要判断inchannel == outchannel ? #跨维度需要stride=2，shortcut也需要1*1卷积扩维 layers.append(ResidualBlock(inchannel,outchannel)) #添加剩余nums-1层 for i in range(1,nums): layers.append(ResidualBlock(outchannel,outchannel)) return nn.Sequential(*layers) else: #resnet50使用bottleneck #传递每个block的shortcut，shortcut可以根据是否传递pre_channel进行推断 #添加首层,首层需要传递上一批blocks的channel layers.append(BottleNeckBlock(inchannel,outchannel,pre_channel,stride)) for i in range(1,nums): #添加n-1个剩余blocks，正常通道转换，不传递pre_channel layers.append(BottleNeckBlock(inchannel,outchannel)) return nn.Sequential(*layers) def forward(self, x): x = self.pre(x) x = self.residualBlocks1(x) x = self.residualBlocks2(x) x = self.residualBlocks3(x) x = self.residualBlocks4(x) x = F.avg_pool2d(x, 7) x = x.view(x.size(0), -1) return self.fc(x) class ResidualBlock(nn.Module): ''' 定义普通残差模块 resnet34为普通残差块，resnet50为瓶颈结构 ''' def __init__(self, inchannel, outchannel, stride=1, padding=1, shortcut=None): super(ResidualBlock, self).__init__() #resblock的首层，首层如果跨维度，卷积stride=2，shortcut需要1*1卷积扩维 if inchannel != outchannel: stride= 2 shortcut=nn.Sequential( nn.Conv2d(inchannel,outchannel,1,stride,bias=False), nn.BatchNorm2d(outchannel) ) # 定义残差块的左部分 self.left = nn.Sequential( nn.Conv2d(inchannel, outchannel, 3, stride, padding, bias=False), nn.BatchNorm2d(outchannel), nn.ReLU(inplace=True), nn.Conv2d(outchannel, outchannel, 3, 1, padding, bias=False), nn.BatchNorm2d(outchannel), ) #定义右部分 self.right = shortcut def forward(self, x): out = self.left(x) residual = x if self.right is None else self.right(x) out = out + residual return F.relu(out) class BottleNeckBlock(nn.Module): ''' 定义resnet50的瓶颈结构 ''' def __init__(self,inchannel,outchannel, pre_channel=None, stride=1,shortcut=None): super(BottleNeckBlock, self).__init__() #首个bottleneck需要承接上一批blocks的输出channel if pre_channel is None: #为空则表示不是首个bottleneck， pre_channel = outchannel #正常通道转换 else: # 传递了pre_channel,表示为首个block，需要shortcut shortcut = nn.Sequential( nn.Conv2d(pre_channel,outchannel,1,stride,0,bias=False), nn.BatchNorm2d(outchannel) ) self.left = nn.Sequential( #1*1,inchannel nn.Conv2d(pre_channel, inchannel, 1, stride, 0, bias=False), nn.BatchNorm2d(inchannel), nn.ReLU(inplace=True), #3*3,inchannel nn.Conv2d(inchannel,inchannel,3,1,1,bias=False), nn.BatchNorm2d(inchannel), nn.ReLU(inplace=True), #1*1,outchannel nn.Conv2d(inchannel,outchannel,1,1,0,bias=False), nn.BatchNorm2d(outchannel), nn.ReLU(inplace=True), ) self.right = shortcut def forward(self,x): out = self.left(x) residual = x if self.right is None else self.right(x) return F.relu(out+residual) if __name__ == '__main__': # channel_nums = [64,128,256,512,1024,2048] num_classes = 6 #layers = 18, 34, 50, 101, 152 layer_nums = [[2,2,2,2],[3,4,6,3],[3,4,6,3],[3,4,23,3],[3,8,36,3]] #选择resnet版本， # resnet18 ——0；resnet34——1,resnet-50——2,resnet-101——3,resnet-152——4 i = 3; bottleneck = i \u003e= 2 #i\u003c2, false,使用普通的ResidualBlock; i\u003e=2，true,使用BottleNeckBlock model = ResNetModel(num_classes,layer_nums[i],bottleneck) print(model) ResNet结构改写 看下图的结构，多模态的数据首先采用ResNet的conv1-11作特征提取，concat后再经过conv12-50，最后是一层全连接：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def conv3x3(in_planes, out_planes, stride=1): \"3x3 convolution with padding\" return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False) class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out class Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None): super(Bottleneck, self).__init__() self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False) self.bn3 = nn.BatchNorm2d(planes * 4) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out class ResNetMI(nn.Module): def __init__(self, block, layers, num_classes=256): super(ResNetMI, self).__init__() self.inplanes = 64 self.inplanes_rgbdnm = 4 super(ResNetMI, self).__init__() self.conv1 = nn.Conv2d(3, 4, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(4) self.conv2 = nn.Conv2d(1, 4, kernel_size=7, stride=2, padding=3, bias=False) self.bn2 = nn.BatchNorm2d(4) self.conv3 = nn.Conv2d(3, 4, kernel_size=7, stride=2, padding=3, bias=False) self.bn3 = nn.BatchNorm2d(4) self.conv4 = nn.Conv2d(1, 4, kernel_size=7, stride=2, padding=3, bias=False) self.bn4 = nn.BatchNorm2d(4) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1_color = self._make_layer_rgbdnm(block, 4, layers[0]) self.layer1_depth = self._make_layer_rgbdnm(block, 4, layers[0]) self.layer1_normal = self._make_layer_rgbdnm(block, 4, layers[0]) self.layer1_mask = self._make_layer_rgbdnm(block, 4, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2) self.layer3 = self._make_layer(block, 256, layers[2], stride=2) self.layer4 = self._make_layer(block, 512, layers[3], stride=2) self.avgpool = nn.AvgPool2d(7) self.fc = nn.Linear(512 * block.expansion, num_classes) self.fcms = nn.Linear(1 * num_classes, num_classes) self.fcms_gl = nn.Linear(2 * num_classes, num_classes) #权重初始化 for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() def _make_layer_rgbdnm(self, block, planes, blocks, stride=1): downsample = None # 残差块通过1x1卷积提升通道维度 if stride != 1 or self.inplanes_rgbdnm != planes * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.inplanes_rgbdnm, planes * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes_rgbdnm, planes, stride, downsample)) self.inplanes_rgbdnm = planes * block.expansion for i in range(1, blocks): layers.append(block(self.inplanes_rgbdnm, planes)) self.inplanes_rgbdnm = 4 return nn.Sequential(*layers) def _make_layer(self, block, planes, blocks, stride=1): downsample = None #添加大模块首层, 首层需要判断inchannel == outchannel ? #跨维度需要stride=2，shortcut也需要1*1卷积扩维 if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample)) self.inplanes = planes * block.expansion for i in range(1, blocks): layers.append(block(self.inplanes, planes)) return nn.Sequential(*layers) def forward(self, x1, x2, x3, x4, x5, x6, x7, x8): x2 = x2[:,None, :, :] x4 = x4[:,None, :, :] x6 = x6[:,None, :, :] x8 = x8[:,None, :, :] # global tower # conv1-11 x1 = self.conv1(x1) x1 = self.bn1(x1) x1 = self.relu(x1) x1 = self.maxpool(x1) x1 = self.layer1_color(x1) x2 = self.conv2(x2) x2 = self.bn2(x2) x2 = self.relu(x2) x2 = self.maxpool(x2) x2 = self.layer1_depth(x2) x3 = self.conv3(x3) x3 = self.bn3(x3) x3 = self.relu(x3) x3 = self.maxpool(x3) x3 = self.layer1_normal(x3) x4 = self.conv4(x4) x4 = self.bn4(x4) x4 = self.relu(x4) x4 = self.maxpool(x4) x4 = self.layer1_mask(x4) x1 = torch.cat((x1, x2, x3, x4), 1) # conv12-50 x1 = self.layer2(x1) x1 = self.layer3(x1) x1 = self.layer4(x1) x1 = self.avgpool(x1) x1 = x1.view(x1.size(0), -1) x1 = self.fc(x1) xms1 = self.fcms(x1) # local tower # conv1-11 x5 = self.conv1(x5) x5 = self.bn1(x5) x5 = self.relu(x5) x5 = self.maxpool(x5) x5 = self.layer1_color(x5) x6 = self.conv2(x6) x6 = self.bn2(x6) x6 = self.relu(x6) x6 = self.maxpool(x6) x6 = self.layer1_depth(x6) x7 = self.conv3(x7) x7 = self.bn3(x7) x7 = self.relu(x7) x7 = self.maxpool(x7) x7 = self.layer1_normal(x7) x8 = self.conv4(x8) x8 = self.bn4(x8) x8 = self.relu(x8) x8 = self.maxpool(x8) x8 = self.layer1_mask(x8) x5 = torch.cat((x5, x6, x7, x8), 1) # conv12-50 x5 = self.layer2(x5) x5 = self.layer3(x5) x5 = self.layer4(x5) x5 = self.avgpool(x5) x5 = x5.view(x5.size(0), -1) x5 = self.fc(x5) xms2 = self.fcms(x5) # concat global \u0026 local xms = torch.cat((xms1, xms2), 1) xms = self.fcms_gl(xms) return xms Pytorch实现ResNet50网络结构，包含ResNet18，ResNet34，ResNet50，ResNet101，ResNet152 ↩︎\n","title":"pytorch 搭建网络结构","uri":"/contents/dl/basic_network_struct/"},{"categories":["contents"],"content":" How long before I get in\n在我抵达之前已过了多久\nBefore it starts before I begin\n在开启旅途之前 在我启程之前\nHow long before you decide or\n在你做决定之前已过了多久\nBefore I know what it feels like\n在我体会到这种远行的滋味之前\nWhere to, where do I go?\n朝着何方 我要去何方\nIf you never try then you’ll never know\n如果你从不去尝试 你将永远无法知晓\nHow long do I have to climb\n我还得往上攀爬多久呢\nUp on the side of this mountain of mine\n一直沿着这座山向上爬\n参考文献 1\n图片链接 图片并列\n表格\na b Ford v Ferrari ↩︎\n","title":"markdown写作样式记录","uri":"/contents/timestamp/md_type/"},{"categories":["contents"],"content":"HRNet 是一个很实用的骨干网络，在对目标位置敏感，需要输出高分辨率特征图的计算机视觉任务中表现很好，比如目标检测，语义分割，人体姿态估计， facial landmark estimate 等任务。\n不同于以往先用从高分辨率到低分辨率网络编码低分辨率特征图结构(ResNet,VGGNet),再解码为高分辨率特征图的网络.文章从高分辨率子网络(high-resolution subnetwork)作为第一阶段开始，逐步增加高分辨率到低分辨率的子网，形成更多的阶段，并将多分辨率子网并行连接。通过进行多次多尺度融合 multi-scale fusions ，使得每一个高分辨率到低分辨率的表征都从其他并行表示中反复接收信息，从而得到丰富的高分辨率表征。\nHRNet 的各类应用见 github官网。\n网络结构和代码分析 【论文阅读笔记】HRNet–从代码来看论文这篇文章讲的很详细，不重复造轮子了，在这里我补充上我看代码时觉得注意的一些点。\nmodels.py里面的参数定义：\n1 2 3 POSE_HIGH_RESOLUTION_NET.STAGE2.NUM_MODULES = 1 POSE_HIGH_RESOLUTION_NET.STAGE2.NUM_BRANCHES = 2 POSE_HIGH_RESOLUTION_NET.STAGE2.NUM_BLOCKS = [4, 4] 这里分清楚 STAGE,MODULES,BRANCHES,BLOCKS分别代表什么含义，上图的网络结构图中整个代表一个MODULES，该MODULES横向分为四个STAGE：\nSTAGE1没有分支BRANCHES；\nSTAGE2有两个分支BRANCHES，每个BRANCHES有四个BLOCKS；\nSTAGE3有三个分支BRANCHES，每个BRANCHES有四个BLOCKS；\nSTAGE4有四个分支BRANCHES，每个BRANCHES有四个BLOCKS；\n代码中还有两个重要的概念：\n_make_fuse_layers 在\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class HighResolutionModule(nn.Module): ... ... ... def _make_fuse_layers(self): if self.num_branches == 1: return None num_branches = self.num_branches num_inchannels = self.num_inchannels fuse_layers = [] for i in range(num_branches if self.multi_scale_output else 1): fuse_layer = [] for j in range(num_branches): # 1x1卷积，再上采样 if j \u003e i: fuse_layer.append(nn.Sequential( nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False), nn.BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM), nn.Upsample(scale_factor=2**(j-i), mode='nearest'))) elif j == i: fuse_layer.append(None) else: #降采样，采样数和隔的层数相关，比如相邻层采样率为2**1，隔一层采样率为2**2,最后一层输出不用relu conv3x3s = [] for k in range(i-j): if k == i - j - 1: num_outchannels_conv3x3 = num_inchannels[i] conv3x3s.append(nn.Sequential( nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False), nn.BatchNorm2d(num_outchannels_conv3x3, momentum=BN_MOMENTUM))) else: num_outchannels_conv3x3 = num_inchannels[j] conv3x3s.append(nn.Sequential( nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False), nn.BatchNorm2d(num_outchannels_conv3x3, momentum=BN_MOMENTUM), nn.ReLU(False))) fuse_layer.append(nn.Sequential(*conv3x3s)) fuse_layers.append(nn.ModuleList(fuse_layer)) return nn.ModuleList(fuse_layers) 中的\n1 for i in range(num_branches if self.multi_scale_output else 1): 当self.multi_scale_output == True时会执行特征融合，而在代码里设置每个STAGE都会执行该操作，上面代码的特征融合规则是，对于待融合分支BRANCHES的特征图，当与之融合的特征图分辨率更大，需要通过卷积降低分辨率，相同BRANCHES的特征图不执行操作（则后续直接相加），当与之融合的特征图分辨率更小，需要先进行1X1卷积让两特征图的通道数一致，再通过上采样操作加大分辨率，特征融合规则是直接相加。\n_make_transition_layer 注意代码：\n1 2 3 for i in range(num_branches if self.multi_scale_output else 1): fuse_layer = [] for j in range(num_branches): 进行融合的BRANCHES数是一致的，新多出来的BRANCHE的特征融合方式，不同于论文的模型图，代码里是直接从最近BRANCHE的融合特征图通过卷积降采样得到。\n详细代码见：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def _make_transition_layer( self, num_channels_pre_layer, num_channels_cur_layer): num_branches_cur = len(num_channels_cur_layer) num_branches_pre = len(num_channels_pre_layer) transition_layers = [] for i in range(num_branches_cur): #卷积实现升采样 if i \u003c num_branches_pre: if num_channels_cur_layer[i] != num_channels_pre_layer[i]: transition_layers.append(nn.Sequential( nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False), nn.BatchNorm2d( num_channels_cur_layer[i], momentum=BN_MOMENTUM), nn.ReLU(inplace=True))) else: transition_layers.append(None) else: #卷积实现降采样、或者通道不变 conv3x3s = [] for j in range(i+1-num_branches_pre): inchannels = num_channels_pre_layer[-1] outchannels = num_channels_cur_layer[i] \\ if j == i-num_branches_pre else inchannels conv3x3s.append(nn.Sequential( nn.Conv2d( inchannels, outchannels, 3, 2, 1, bias=False), nn.BatchNorm2d(outchannels, momentum=BN_MOMENTUM), nn.ReLU(inplace=True))) transition_layers.append(nn.Sequential(*conv3x3s)) return nn.ModuleList(transition_layers) ","title":"HRNet 网络结构以及代码分析","uri":"/contents/dl/hrnet/"},{"categories":["contents"],"content":"本文主要记录在学习《GPU并行计算与CUDA编程》过程中的知识要点，并且把跑过的代码统一放在cmake工程里面。\n关于cuda更具体的介绍，一个博客专题介绍的很好： CUDA从入门到入门\n对应的代码在： https://github.com/Tony-Tan/CUDA_Freshman\n基础 关于 cuda 的基础知识参考之前的文章：\ncuda学习笔记1 cuda学习笔记2 cuda学习笔记3 cuda学习笔记4 cuda学习笔记5 最详细最新的内容，还看官网：CUDA C++ Programming Guide。\n在CMake编译cuda程序，和编译一般的cpp程序几乎相同：\nCMakeLists.txt编写如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cmake_minimum_required(VERSION 3.8) project(CUDA_MAT_MUL LANGUAGES CXX CUDA) aux_source_directory(cuda_src CUDA_SRC_LIST) aux_source_directory(cuda_head CUDA_INCLUDE_LIST) include_directories(cuda_head) add_library(cudaPractice ${CUDA_SRC_LIST} ${CUDA_INCLUDE_LIST}) target_compile_features(cudaPractice PUBLIC cxx_std_11) #opencv find_package(OpenCV 3.4.3 REQUIRED) include_directories( ${OpenCV_INCLUDE_DIRS} ) add_executable(main main.cc) target_link_libraries(main cudaPractice) target_link_libraries(main ${OpenCV_LIBS}) 目录结构如下所示：\nadd_library把cuda的源文件和头文件打包到静态库里面，然后target_compile_features和target_link_libraries调用打包的静态库。\n要点记录 gpu的硬件模式 GPU，SM(流处理器)，Kernel(核)，thread block(线程块)，线程 CPU的ALU, Cache和Control单元的特点 ALU：CPU有强大的ALU（算术运算单元）,它可以在很少的时钟周期内完成算术计算。 当今的CPU可以达到64bit 双精度。执行双精度浮点源算的加法和乘法只需要1～3个时钟周期。 CPU的时钟周期的频率是非常高的，达到1.532～3gigahertz(千兆HZ, 10的9次方). Cache：大的缓存也可以降低延时。保存很多的数据放在缓存里面，当需要访问的这些数据，只要在之前访问过的，如今直接在缓存里面取即可。 Control：复杂的逻辑控制单元。 当程序含有多个分支的时候，它通过提供分支预测的能力来降低延时。 数据转发。 当一些指令依赖前面的指令结果时，数据转发的逻辑控制单元决定这些指令在pipeline中的位置并且尽可能快的转发一个指令的结果给后续的指令。这些动作需要很多的对比电路单元和转发电路单元。\nGPU的ALU, Cache和Control单元的特点 ALU，Cache：GPU的特点是有很多的ALU和很少的cache. 缓存的目的不是保存后面需要访问的数据的，这点和CPU不同，而是为thread提高服务的。如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问dram（因为需要访问的数据保存在dram中而不是cache里面），获取数据后cache会转发这个数据给对应的线程，这个时候是数据转发的角色。但是由于需要访问dram，自然会带来延时的问题。 Control：控制单元（左边黄色区域块）可以把多个的访问合并成少的访问。 GPU的虽然有dram延时，却有非常多的ALU和非常多的thread. 为了平衡内存延时的问题，我们可以中充分利用多的ALU的特性达到一个非常大的吞吐量的效果。尽可能多的分配多的Threads.通常来看GPU ALU会有非常重的pipeline就是因为这样。 CPU擅长逻辑控制，串行的运算。和通用类型数据运算不同，GPU擅长的是大规模并发计算，这也正是密码破解等所需要的。所以GPU除了图像处理，也越来越多的参与到计算当中来。\ncuda常用库 CUDA函数库 CUDA提供了几个较为成熟的高效函数库,程序员可以 直接调用这些库函数进行计算,因而大大简化了程序员 的工作量。其中最常用的包括: CUFFT （利用CUDA进行傅里叶变换的函数库 ） CUBLAS （利用CUDA进行加速版本的完 整标准矩阵与向量的运算库 ） CUDPP （常用的并行操作函数库） CUDNN （利用CUDA进行深度卷积神经网络，深度学习常用） 全部库：https://developer.nvidia.com/gpu-accelerated-libraries\ncuda中的标识符 __global__, __device__用来标识某个函数是设备代码而不是主机代码，而__host__表示主机代码(可以不写)。 主机/设备上的变量，在设备/主机上只可以访问而不能修改，不能在主机代码中用主机指针访问设备内存，反过来也一样，不能用设备指针访问主机内存。 cuda中threadIdx、blockIdx、blockDim和gridDim的使用\nthreadIdx是一个uint3类型，表示一个线程的索引。调用方法：(a.x, a.y, a.z)\nblockIdx是一个uint3类型，表示一个线程块的索引，一个线程块中通常有多个线程。\nblockDim是一个dim3类型，表示线程块的大小。\ngridDim是一个dim3类型，表示网格的大小，一个网格中通常有多个线程块。\n一维线程的使用：\n1 2 3 4 5 6 7 8 __global__ void add_kernel(double *a, double *b, double *c) { //block id int tid = blockIdx.x; if (tid \u003c N) { c[tid] = a[tid] + b[tid]; }\t} 1 2 3 4 5 6 7 8 9 10 11 err1 = cudaMalloc((void**)\u0026dev_a, N * sizeof(double)); err2 = cudaMalloc((void**)\u0026dev_b, N * sizeof(double)); err3 = cudaMalloc((void**)\u0026dev_c, N * sizeof(double)); //表示 N 个block， 每个block分配 1个 thread add_kernel \u003c\u003c \u003cN, 1 \u003e\u003e \u003e (dev_a, dev_b, dev_c);////在GPU上相加操作 ////用完设备指针要释放 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c); 二维block的使用：\n1 2 3 4 5 6 __global__ void kernel(unsigned char *ptr) { int x = blockIdx.x; int y = blockIdx.y; int offset = x + y * gridDim.x; //... } 1 2 3 4 5 6 7 8 unsigned char *dev_bitmap; HANDLE_ERROR(cudaMalloc((void**)\u0026dev_bitmap, bitmap.image_size())); dim3 grid(DIM1, DIM2); ////实际上是DIM1*DIM2*1的三维线程格 //三维grid, 1个thread kernel \u003c\u003c \u003cgrid, 1 \u003e\u003e \u003e (dev_bitmap); HANDLE_ERROR(cudaFree(dev_bitmap)); 更多自由搭配：(1/2/3维度block)*(1/2/3维度thread):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 //thread 1D __global__ void testThread1(int *c, const int *a, const int *b) { int i = threadIdx.x; c[i] = b[i] - a[i]; } //thread 2D __global__ void testThread2(int *c, const int *a, const int *b) { int i = threadIdx.x + threadIdx.y*blockDim.x; c[i] = b[i] - a[i]; } //thread 3D __global__ void testThread3(int *c, const int *a, const int *b) { int i = threadIdx.x + threadIdx.y*blockDim.x + threadIdx.z*blockDim.x*blockDim.y; c[i] = b[i] - a[i]; } //block 1D __global__ void testBlock1(int *c, const int *a, const int *b) { int i = blockIdx.x; c[i] = b[i] - a[i]; } //block 2D __global__ void testBlock2(int *c, const int *a, const int *b) { int i = blockIdx.x + blockIdx.y*gridDim.x; c[i] = b[i] - a[i]; } //block 3D __global__ void testBlock3(int *c, const int *a, const int *b) { int i = blockIdx.x + blockIdx.y*gridDim.x + blockIdx.z*gridDim.x*gridDim.y; c[i] = b[i] - a[i]; } //block-thread 1D-1D __global__ void testBlockThread1(int *c, const int *a, const int *b) { int i = threadIdx.x + blockDim.x*blockIdx.x; c[i] = b[i] - a[i]; } //block-thread 1D-2D __global__ void testBlockThread2(int *c, const int *a, const int *b) { int threadId_2D = threadIdx.x + threadIdx.y*blockDim.x; int i = threadId_2D+ (blockDim.x*blockDim.y)*blockIdx.x; c[i] = b[i] - a[i]; } //block-thread 1D-3D __global__ void testBlockThread3(int *c, const int *a, const int *b) { int threadId_3D = threadIdx.x + threadIdx.y*blockDim.x + threadIdx.z*blockDim.x*blockDim.y; int i = threadId_3D + (blockDim.x*blockDim.y*blockDim.z)*blockIdx.x; c[i] = b[i] - a[i]; } //block-thread 2D-1D __global__ void testBlockThread4(int *c, const int *a, const int *b) { int blockId_2D = blockIdx.x + blockIdx.y*gridDim.x; int i = threadIdx.x + blockDim.x*blockId_2D; c[i] = b[i] - a[i]; } //block-thread 3D-1D __global__ void testBlockThread5(int *c, const int *a, const int *b) { int blockId_3D = blockIdx.x + blockIdx.y*gridDim.x + blockIdx.z*gridDim.x*gridDim.y; int i = threadIdx.x + blockDim.x*blockId_3D; c[i] = b[i] - a[i]; } //block-thread 2D-2D __global__ void testBlockThread6(int *c, const int *a, const int *b) { int threadId_2D = threadIdx.x + threadIdx.y*blockDim.x; int blockId_2D = blockIdx.x + blockIdx.y*gridDim.x; int i = threadId_2D + (blockDim.x*blockDim.y)*blockId_2D; c[i] = b[i] - a[i]; } //block-thread 2D-3D __global__ void testBlockThread7(int *c, const int *a, const int *b) { int threadId_3D = threadIdx.x + threadIdx.y*blockDim.x + threadIdx.z*blockDim.x*blockDim.y; int blockId_2D = blockIdx.x + blockIdx.y*gridDim.x; int i = threadId_3D + (blockDim.x*blockDim.y*blockDim.z)*blockId_2D; c[i] = b[i] - a[i]; } //block-thread 3D-2D __global__ void testBlockThread8(int *c, const int *a, const int *b) { int threadId_2D = threadIdx.x + threadIdx.y*blockDim.x; int blockId_3D = blockIdx.x + blockIdx.y*gridDim.x + blockIdx.z*gridDim.x*gridDim.y; int i = threadId_2D + (blockDim.x*blockDim.y)*blockId_3D; c[i] = b[i] - a[i]; } //block-thread 3D-3D __global__ void testBlockThread9(int *c, const int *a, const int *b) { int threadId_3D = threadIdx.x + threadIdx.y*blockDim.x + threadIdx.z*blockDim.x*blockDim.y; int blockId_3D = blockIdx.x + blockIdx.y*gridDim.x + blockIdx.z*gridDim.x*gridDim.y; int i = threadId_3D + (blockDim.x*blockDim.y*blockDim.z)*blockId_3D; c[i] = b[i] - a[i]; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 //testThread1\u003c\u003c\u003c1, size\u003e\u003e\u003e(dev_c, dev_a, dev_b); //uint3 s;s.x = size/5;s.y = 5;s.z = 1; //testThread2 \u003c\u003c\u003c1,s\u003e\u003e\u003e(dev_c, dev_a, dev_b); //uint3 s; s.x = size / 10; s.y = 5; s.z = 2; //testThread3\u003c\u003c\u003c1, s \u003e\u003e\u003e(dev_c, dev_a, dev_b); //testBlock1\u003c\u003c\u003csize,1 \u003e\u003e\u003e(dev_c, dev_a, dev_b); //uint3 s; s.x = size / 5; s.y = 5; s.z = 1; //testBlock2\u003c\u003c\u003cs, 1 \u003e\u003e\u003e(dev_c, dev_a, dev_b); //uint3 s; s.x = size / 10; s.y = 5; s.z = 2; //testBlock3\u003c\u003c\u003cs, 1 \u003e\u003e\u003e(dev_c, dev_a, dev_b); //testBlockThread1\u003c\u003c\u003csize/10, 10\u003e\u003e\u003e(dev_c, dev_a, dev_b); //uint3 s1; s1.x = size / 100; s1.y = 1; s1.z = 1; //uint3 s2; s2.x = 10; s2.y = 10; s2.z = 1; //testBlockThread2 \u003c\u003c \u003cs1, s2 \u003e\u003e \u003e(dev_c, dev_a, dev_b); //uint3 s1; s1.x = size / 100; s1.y = 1; s1.z = 1; //uint3 s2; s2.x = 10; s2.y = 5; s2.z = 2; //testBlockThread3 \u003c\u003c \u003cs1, s2 \u003e\u003e \u003e(dev_c, dev_a, dev_b); //uint3 s1; s1.x = 10; s1.y = 10; s1.z = 1; //uint3 s2; s2.x = size / 100; s2.y = 1; s2.z = 1; //testBlockThread4 \u003c\u003c \u003cs1, s2 \u003e\u003e \u003e(dev_c, dev_a, dev_b); //uint3 s1; s1.x = 10; s1.y = 5; s1.z = 2; //uint3 s2; s2.x = size / 100; s2.y = 1; s2.z = 1; //testBlockThread5 \u003c\u003c \u003cs1, s2 \u003e\u003e \u003e(dev_c, dev_a, dev_b); //uint3 s1; s1.x = size / 100; s1.y = 10; s1.z = 1; //uint3 s2; s2.x = 5; s2.y = 2; s2.z = 1; //testBlockThread6 \u003c\u003c \u003cs1, s2 \u003e\u003e \u003e(dev_c, dev_a, dev_b); //uint3 s1; s1.x = size / 100; s1.y = 5; s1.z = 1; //uint3 s2; s2.x = 5; s2.y = 2; s2.z = 2; //testBlockThread7 \u003c\u003c \u003cs1, s2 \u003e\u003e \u003e(dev_c, dev_a, dev_b); //uint3 s1; s1.x = 5; s1.y = 2; s1.z = 2; //uint3 s2; s2.x = size / 100; s2.y = 5; s2.z = 1; //testBlockThread8 \u003c\u003c\u003cs1, s2 \u003e\u003e\u003e(dev_c, dev_a, dev_b); uint3 s1; s1.x = 5; s1.y = 2; s1.z = 2; uint3 s2; s2.x = size / 200; s2.y = 5; s2.z = 2; testBlockThread9\u003c\u003c\u003cs1, s2 \u003e\u003e\u003e(dev_c, dev_a, dev_b); //或者： dim3 blocks(DIM/16,DIM/16，1); ////二维线程块 dim3 threads(16,16,1); ////二维线程 func_kernel\u003c\u003c\u003cblocks,threads\u003e\u003e\u003e（参数）； 注意的是 blockDim.x和 gridDim.x确实有一个物理上的最大值，但在使用时的大小是由在代码中的设定决定的，比如下面的：\n1 2 3 4 5 6 7 8 9 10 11 __global__ void add_kernel(double *a, double *b, double *c) { int tid = threadIdx.x + blockIdx.x * blockDim.x; if (tid \u003c N) { c[tid] = a[tid] + b[tid]; tid += blockDim.x * gridDim.x; } } add_kernel \u003c\u003c \u003c 128, 128 \u003e\u003e \u003e (dev_a, dev_b, dev_c);////在GPU上相加操作 blockDim.x和gridDim.x都为128， 128，确实这样是合理的，它让我们可以固定具体使用多少的thread，而不因为数据扩增而导致出现异常。同理地，合理的设置使得blockDim.x和gridDim.x逼近gpu的计算上限，可以充分利用gpu的计算能力。\n在设备代码上定义结构体(类) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 struct cuComplex { float r; float i; __device__ cuComplex(float a, float b) :r(a), i(b) {} __device__ float magnitude2(void) { return r*r + i*i; } ////返回复数的模的平方 __device__ cuComplex operator*(const cuComplex\u0026 a) { return cuComplex(r*a.r - i*a.i, i*a.r + r*a.i); } __device__ cuComplex operator+(const cuComplex\u0026 a) { return cuComplex(r + a.r, i + a.i); } }; 可以看到，和常用的C++代码，区别也只在于函数定义是否有__device__标识符。\ngpu 的内存结构 GPU编程3–GPU内存深入了解这篇文章讲的很清楚。\ncuda中有寄存器内存，局部内存，共享内存，常量内存，纹理内存，全局内存。寄存器内存用于定义线程专属私有变量。当私有变量申请大小溢出时，自动转为局部内存。当在核函数里面申请局部数组时，自动称为局部内存。\n共享内存 共享内存（shared memory，SMEM）是GPU的一个关键部分，物理层面，每个SM都有一个小的内存池，这个线程池被次SM上执行的线程块中的所有线程所共享。共享内存使同一个线程块中可以相互协同，便于片上的内存可以被最大化的利用，降低回到全局内存读取的延迟。 共享内存是被我们用代码控制的，这也是是他称为我们手中最灵活的优化武器。 一级缓存，二级缓存，共享内存，以及只读和常量缓存，他们的关系如下图：\n可以看到， 共享内存(SMEM)， 一级缓存， 只读缓存和常量缓存更接近SM计算核心，有更低的访问延迟和传输带宽。\n将线程块分解为线程的目的，除了物理设备上线程块最大数目的限制，还有一个原因是 CUDA C支持共享内存。对于GPU上的每一个线程块，编译器都为该共享变量创建一个副本，而线程块中的每一个线程共享这块内存。由于共享内存驻留在物理GPU上而不是GPU之外的系统内存中，访问共享内存的延迟要远低于访问普通内缓存区的延迟。\n利用共享内存实现内积(dot):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 __global__ void dot(float *a, float *b, float *c) { __shared__ float cache[threadsPerBlock]; ////共享内存缓存区（驻留在物理GPU上），编译器为每一个线程块生成一个共享变量的副本。同一线程块中的每个线程共享这块内存。 int tid = threadIdx.x + blockIdx.x * blockDim.x; ////索引偏置 int cacheIndex = threadIdx.x; ////由于每一个线程块都具有一个共享内存的副本，故共享内存的索引就是该线程块上的线程索引。 float temp = 0; while (tid \u003c N) { temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } // set the cache values cache[cacheIndex] = temp; ////每个线程处理的数据，相加放在对应的共享内存区域中。 // synchronize threads in this block __syncthreads(); ////线程同步，当所有的线程都执行完以上操作时，才能继续执行下一步。 // for reductions, threadsPerBlock must be a power of 2 // because of the following code ////归约，将共享内存区域每一个储存的值相加起来，由于规约每次迭代数量减半，要求 threadsPerBlock 是2 的指数倍。 int i = blockDim.x / 2; while (i != 0) { if (cacheIndex \u003c i) cache[cacheIndex] += cache[cacheIndex + i]; __syncthreads(); ////线程同步。同样地，执行下一次规约迭代前，必须确保所有线程都已经执行完上面相加的操作。 i /= 2; } if (cacheIndex == 0) c[blockIdx.x] = cache[0]; ////把一个线程块中的最后计算得到的相加值返还给全局变量。 } 若申请blocksPerGrid个block, 则会在gpu上创建blocksPerGrid个共享内存副本。\n共享内存和block同生命周期。\n1 2 dot \u003c\u003c \u003cblocksPerGrid, threadsPerBlock \u003e\u003e \u003e(dev_a, dev_b, dev_partial_c); 常量内存 常量内存用于保存在核函数执行期间不会发生变化的数据，由于GPU的性能瓶颈通常不在于芯片的数学吞吐能力，而在于芯片的内存带宽，合理利用常量内存能有效减小内存的带宽的消耗。常量内存存在于核函数之外，在kernel函数外声明，即常量内存存在于内存中，并不在片上，常量内容的访问速度也是很快的，这是因为每个SM都有专用的常量内存缓存，会把片外的常量读取到缓存中；对所有的核函数都可见，在Host端进行初始化后，核函数不能再修改。\n写法：\n1 __constant__ Sphere s[num] 对于常量内存，不需要再用 cudaMalloc() 或者 cudaFree() 来申请或释放内存空间，编译器会自动为这个数组提交一个固定的大小。\ncudaMemcpy() 会将主机内存复制到全局内存，而cudaMemcpyToSymbol() 会将主机内存复制到常量内存。\n常量内存为什么有效：\n对常量内存的单次操作可以广播到其他临近线程，范围为半个线程束（Wrap）。\n常量内存的数据将缓存起来，因此对相同地址的连续读操作不会产生额外的内存通信量。\n在CUDA架构中，线程束是指包含32个线程的集合，这个线程集合被“编织”在一起并且以“步调一致”的形式执行，在程序的每一行，线程束中的每个线程都在不同的数据中执行相同的操作。当这半个线程束读取常量内存相同地址时，才可以大幅度提升性能，否则，这半个线程束的请求会被串行化，在这个情况下性能反而会降低。\n常量内存有两个特性，一个是高速缓存，另一个是它支持将单个值广播到线程束中的每个线程。但要注意的是，对于那些数据不太集中或者数据重用率不高的内存访问，尽量不要使用常量内存。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #define SPHERES 20 __constant__ Sphere s[SPHERES]; int main(void) { // allocate temp memory, initialize it, copy to constant // memory on the GPU, then free our temp memory Sphere *temp_s = (Sphere*)malloc(sizeof(Sphere) * SPHERES); for (int i = 0; i\u003cSPHERES; i++) { temp_s[i].r = rnd(1.0f); temp_s[i].g = rnd(1.0f); temp_s[i].b = rnd(1.0f); temp_s[i].x = rnd(1000.0f) - 500; temp_s[i].y = rnd(1000.0f) - 500; temp_s[i].z = rnd(1000.0f) - 500; temp_s[i].radius = rnd(100.0f) + 20; } //用cudaMemcpyToSymbol拷贝到常量内存 HANDLE_ERROR(cudaMemcpyToSymbol(s, temp_s, sizeof(Sphere) * SPHERES)); free(temp_s); } 纹理内存 同常量内存一样，纹理内存（Texture Memory）也是一种只读内存。 之所以称之为 “纹理”，是因为最初是为图形应用设计的。 当程序中存在大量局部空间操作时，纹理内存可以提高性能。 纹理内存的优势： 1.它们是被缓存的,如果它们在texture fetch 中将提供更高的带宽 2.它们不会像全局或常驻内存读取时受内存访问模式的约束 3.寻址计算时的延迟更低,从而提高随机访问数据时的性能 4.在一个操作中,包装的数据可以通过广播到不同的变量中 5.8-bit和16-bit的整型输入数据可以被转换成在范围[0.0,1.0]或[-1.0,1.0]的浮点数\n全局内存 全局内存，就是我们常说的显存，就是GDDR的空间，全局内存中的变量，只要不销毁，生命周期和应用程序是一样的。 在访问全局内存时，要求是对齐的，也就是一次要读取指定大小（32、64、128）整数倍字节的内存，数据对齐就意味着传输效率降低，比如我们想读33个字节，但实际操作中，需要读取64字节的空间。\ncudaMemcpy() 会将主机内存复制到全局内存，而cudaMemcpyToSymbol() 会将主机内存复制到常量内存。\ncuda编程原则 避免线程发散 要避免线程发散的程序：\n若将：\n1 2 3 4 5 6 while (i != 0) { if (cacheIndex \u003c i) cache[cacheIndex] += cache[cacheIndex + i]; __syncthreads(); i /= 2; } 改为：\n1 2 3 4 5 6 7 8 while (i != 0) { if (cacheIndex \u003c i) { cache[cacheIndex] += cache[cacheIndex + i]; __syncthreads(); } i /= 2; } 则是错误的，因为不能保证cacheIndex \u003c i对每个线程都成立。\n尽量也要避免在kernel做条件判断，使得不同线程可能执行不同操作。\n要关注代码性能 使用Even测量代码性能\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cudaEvent_t start, stop; cudaEventCreate(\u0026start); cudaEventCreate(\u0026stop); cudaEventRecord(start, 0); ***************** 在GPU上执行一些工作（包括前后的设备内存复制） ***************** cudaEventRecord(stop, 0); cudaEventSynchronize(stop); float elapsedTime; cudaEventElapsedTime(\u0026elapsedTime,start, stop); printf(\"Time to generate: %3.1f ms\\n\", elapsedTime); cudaEventDestroy(start); cudaEventDestroy(stop); 所有在同一个线程块上的线程必然会在同一时间运行在同一个SM上\n同一个内核的所有线程块必然会全部完成了后，才会运行下一个内核\n线程同步\n不同的线程在共享和全局内存中读写数据需要有先后的控制，所以引入了同步性的概念。\n_syncthreads（）:线程块内线程同步 _threadfence(): 一个线程调用__threadfence后，该线程在该语句前对全局存储器或共享存储器的访问已经全部完成，执行结果对grid中的所有线程可见。 _threadfence_block(): 一个线程调用__threadfence_block后，该线程在该语句前对全局存储器或者共享存储器的访问已经全部完成，执行结果对block中的所有线程可见。 以上两个函数的重要作用是，及时通知其他线程，全局内存或者共享内存内的结果已经读入或写入完成了。\n最大化计算强度 意味着要最大化每个线程的计算量和最小化每个线程的内存读取速度\n原子操作 对于有很多线程需要同时读取或写入相同的内存时，保证同一时间只有一个线程能进行操作。 只支持某些运算(加、减、最小值、异或运算等，不支持求余和求幂等)和数据类型（整型）\n举个例子，假设我们想要用GPU统计“char data_0[32] = {1,0, … ,1}”这个数组中“0”和“1”的个数并写入“int counter[2]”中。\n如果我们不使用原子操作，直接在核函数中这样写：\n1 2 3 4 5 6 7 8 9 10 11 extern \"C\" __global__ void kernel_func(int * counter, char * data_0) { // 计算线程号 unsigned int block_index = blockIdx.x + blockIdx.y * gridDim.x + blockIdx.z * gridDim.x * gridDim.y; unsigned int thread_index = block_index * blockDim.x * blockDim.y * blockDim.z + \\ threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y; // 统计结果 int value = data_0[thread_index]; counter[value] ++; } 我们会发现结果是“counter[2] = {1, 1}”，这显然不是正确的结果。 正确的写法是：\n1 2 3 4 5 6 7 8 9 10 11 extern \"C\" __global__ void kernel_func(int * counter, char * data_0) { // 计算线程号 unsigned int block_index = blockIdx.x + blockIdx.y * gridDim.x + blockIdx.z * gridDim.x * gridDim.y; unsigned int thread_index = block_index * blockDim.x * blockDim.y * blockDim.z + \\ threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y; // 统计结果 int value = data_0[thread_index]; atomicAdd(\u0026counter[value], 1); } CUDA流Streams 用到CUDA的程序一般需要处理海量的数据，内存带宽经常会成为主要的瓶颈。在Stream的帮助下，CUDA程序可以有效地将内存读取和数值运算并行，从而提升数据的吞吐量。\ncudaStreams 有效的原因, 在nvidia gpu中：\n数据拷贝和数值计算可以同时进行。 两个方向的拷贝可以同时进行（GPU到CPU，和CPU到GPU），数据如同行驶在双向快车道。\n这样，使用一个Stream搬移大量数据，由于内存带宽有限，速度可能没有把数据切块，用多个Streams来搬移的速度块。当然，这是在gpu的运算单元没有全被占用的情况下是成立的。\n定义流：cudaStream_t s1; 创建流：cudaStreamCreate(\u0026s1); 销毁流：cudaStreamDestory(s1);\n使用八个Streams来优化：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 uint8_t* bgraBuffer; uint8_t* yuvBuffer; uint8_t* deviceBgraBuffer; uint8_t* deviceYuvBuffer; const int dataSizeBgra = 7680 * 4320 * 4; const int dataSizeYuv = 7680 * 4320 * 3; cudaMallocHost(\u0026bgraBuffer, dataSizeBgra); cudaMallocHost(\u0026yuvBuffer, dataSizeYuv); cudaMalloc(\u0026deviceBgraBuffer, dataSizeBgra); cudaMalloc(\u0026deviceYuvBuffer, dataSizeYuv); //随机生成8K的BGRA图像 GenerateBgra8K(bgraBuffer, dataSizeBgra); //Stream的数量，这里用8个 const int nStreams = 8; //Stream的初始化 cudaStream_t streams[nStreams]; for (int i = 0; i \u003c nStreams; i++) { cudaStreamCreate(\u0026streams[i]); } //计算每个Stream处理的数据量。这里只是简单将数据分成8等分 //这里不会出现不能整除的情况，但实际中要小心 int brgaOffset = 0; int yuvOffset = 0; const int brgaChunkSize = dataSizeBgra / nStreams; const int yuvChunkSize = dataSizeYuv / nStreams; //这个循环依次启动 nStreams 个 Stream for(int i=0; i\u003cnStreams; i++) { brgaOffset = brgaChunkSize*i; yuvOffset = yuvChunkSize*i; //CPU到GPU的数据拷贝（原始数据），Stream i cudaMemcpyAsync( deviceBgraBuffer+brgaOffset, bgraBuffer+brgaOffset, brgaChunkSize, cudaMemcpyHostToDevice, streams[i] ); //数值计算，Stream i convertPixelFormat\u003c\u003c\u003c4096, 1024, 0, streams[i]\u003e\u003e\u003e( deviceBgraBuffer+brgaOffset, deviceYuvBuffer+yuvOffset, brgaChunkSize/4 ); //GPU到CPU的数据拷贝（计算结果），Stream i cudaMemcpyAsync( yuvBuffer+yuvOffset, deviceYuvBuffer+yuvOffset, yuvChunkSize, cudaMemcpyDeviceToHost, streams[i] ); } //等待所有操作完成 cudaDeviceSynchronize(); cudaFreeHost(bgraBuffer); cudaFreeHost(yuvBuffer); cudaFree(deviceBgraBuffer); cudaFree(deviceYuvBuffer); 详细代码看CUDA随笔之Stream的使用， 讲的很详细。\n练习代码 可以参考cuda_practice ，熟悉opencv和cuda联合编程的几个例子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 /****************************1. 并行编程的基础知识***********************************/ //1. hello cuda showhello(); //2. 矩阵相乘 showMatMul(); //3. 求平方 showsquare(); //4. 多数之和 输入参数0：使用全局内存 输入参数1：使用共享内存 showreduce(0); //5. 数组扫描 更新数组，每一个数组的值是前面所有遍历过的数组的值之和 输入参数0：使用全局内存 输入参数1：使用共享内存 showscan(1); //上面两个例子的global memory 都比 share memory 的时间小，不太懂为什么，是跑的数据量比较小吗 //6. 分段直方图统计 输入参数0：错误示范 输入参数1：不分段，适用于bin数比较多的情况 输入参数2：分段，适用于bin数比较少的情况 //分段直方图思想：假设开的线程数是THREADS_COUNT，直方图分组的数是BIN_COUNT； //每个线程不只处理一个数据，而是平均地处理ARRAY_SIZE/THREADS_COUNT个数据。 //这时需要创建一个THREADS_COUNT×BIN_COUNT×sizeof(int)的共享内存，每个线程处理ARRAY_SIZE/THREADS_COUNT个数据，然后把结果放在 //BIN_COUNT×sizeof(int)的共享内存里面；将所有数据处理完后再做一次reduce即可。 //速度比不分段的慢，但开的线程很少，节省资源 showhisto(1); /*****************************2. 图像处理中的并行编程基础**********************************/ //7. 彩色图转灰度图 showrgb2grey(); //8. 均值滤波 showblur(); //9. 图像翻转 showreverse(); ","title":"《GPU并行计算与CUDA编程》学习笔记","uri":"/contents/cuda/cuda_more/"},{"categories":["contents"],"content":" 图的概念回顾 对于一个图$G$，我们一般用点的集合$V$和边的集合$E$来描述。即为$G(V,E)$。其中$V$即为我们数据集里面所有的点$(v1,v2,…vn)$。对于$V$中的任意两个点，可以有边连接，也可以没有边连接。我们定义权重$w_{ij}$为点 $v_i$ 和点 $v_j$ 之间的权重。\n对于有边连接的两个点$v_i$和$v_j$，$w_{ij}\u003e0$,对于没有边连接的两个点$v_i$和$v_j$，$w_{ij}=0$。对于图中的任意一个点$v_i$，它的度$d_i$定义为和它相连的所有边的权重之和，即\n$$ d_i = \\sum\\limits_{j=1}^{n}w_{ij} $$\n利用每个点度的定义，我们可以得到一个$n \\times n$的度矩阵$D$,它是一个对角矩阵，只有主对角线有值，对应第$i$行的第$i$个点的度数$d_i $\nminCT 和 谱聚类 谱聚类是从图论中演化出来的算法，后来在聚类中得到了广泛的应用。它的主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高，从而达到聚类的目的。\n给定一个图$G(V,E)$，K向归一化 minCUT 问题（简称为 minCUT ）通过去除最小权重边将$V$划分为$K$个不相交子集 。 问题等同于最大化：\n$$ \\frac{1}{K} \\sum_{k=1}^{K} \\frac{\\operatorname{links}\\left(\\mathcal{V}_{k}\\right)}{\\operatorname{degree}\\left(\\mathcal{V}_{k}\\right)}=\\frac{1}{K} \\sum_{k=1}^{K} \\frac{\\sum_{i, j \\in \\mathcal{V}_{k}} \\mathcal{E}_{i, j}}{\\sum_{i \\in \\mathcal{V}_{k}, j \\in \\mathcal{V} \\backslash \\mathcal{V}_{k}} \\mathcal{E}_{i, j}} $$\n其中分子计算一个划分内节点的边权重之和，分母计算一个划分内的节点和其划分的补集内节点的连接边的权重之和。\n让 $\\mathbf{C} \\in{0,1}^{N \\times K}$ 为聚类分配矩阵, $\\mathbf{C}_{i, j}=1$代表 $i$ 属于划分 $j,$ 否则为0. minCUT 问题可以表示为:\n$$ \\text { maximize } \\frac{1}{K} \\sum_{k=1}^{K} \\frac{\\mathbf{C}_{k}^{T} \\mathbf{A} \\mathbf{C}_{k}}{\\mathbf{C}_{k}^{T} \\mathbf{D} \\mathbf{C}_{k}} $$\n$$ \\text { s.t. } \\mathbf{C} \\in{0,1}^{N \\times K}, \\mathbf{C 1}_{K}=\\mathbf{1}_{N} $$\n其中 $\\mathbf{C}_{k}$ 代表 $\\mathbf{C}$的第$k$ 列. 该问题是个 NP-hard 的问题, 但它可以松弛到另外一个可以用多项式时间内解决的问题，并保证了接近最佳的解决方案： $$ \\underset{\\mathbf{Q} \\in \\mathbb{R}^{{N} \\times K}}{\\arg \\max } \\frac{1}{K} \\sum_{k=1}^{K} \\mathbf{Q}_{k}^{T} \\mathbf{A} \\mathbf{Q}_{k} $$\n$$ \\text { s.t. } \\mathbf{Q}=\\mathbf{C}\\left(\\mathbf{C}^{T} \\mathbf{D} \\mathbf{C}\\right)^{-\\frac{1}{2}}, \\mathbf{Q}^{T} \\mathbf{Q}=\\mathbf{I} _{K} $$ 虽然上述的问题仍然是非凸的, 它有一个最优的解： $\\mathbf{Q}^{*}=\\mathbf{U}_{K} \\mathbf{O},$ 其中 $\\mathbf{U}_{K} \\in \\mathbb{R}^{N \\times K}$ 是矩阵 $\\mathbf{A}$ 的前 $K$ 个最大特征向量组成的矩阵。$\\mathbf{O} \\in \\mathbb{R}^{{K} \\times {K}}$是正交变换矩阵。\n在此之后对$\\mathbf{Q}^{*}$做一个 k-mean 聚类，至此完成谱聚类的全部流程。\n利用GNN做谱聚类 网络结构 基于这样的假设：同一划分中的节点之间是强连接的，同一划分中的节点间的特征是相似的，利用GNN的消息传播机制，节点的邻域特征得到聚合，强连接的节点的特征趋向于更加相似。在论文1中，聚类分配矩阵用一个多层感知机表示，具体如下：\n让 $\\mathbf{X}$ 代表节点特征矩阵. 文章让$\\mathbf{X}$通过一个或多个消息传播层( MP )得到$\\overline{\\mathbf{X}}$，再通过 MLP 层计算聚类分配矩阵 $\\mathbf{S}$，并且在输出层加上 softmax 层对输出数据归一化:\n$$ \\overline{\\mathbf{X}} =\\operatorname{GNN}\\left(\\mathbf{X}, \\tilde{\\mathbf{A}} ; \\Theta_{\\mathrm{GNN}}\\right) $$\n$$ \\mathbf{S} =\\operatorname{MLP}\\left(\\overline{\\mathbf{X}} ; \\Theta_{\\mathrm{MLP}}\\right) $$ 其中 $\\Theta_{\\text {GNN }}$ and $\\Theta_{\\text {MLP }}$ 是可训练参数. softmax 层保证了$s_{i j} \\in[0,1]$ 并且满足了约束：$\\mathbf{S} \\mathbf{1}_{K}=\\mathbf{1}_{N}$.\n损失函数 网络参数 $\\Theta_{\\text {GNN }}$ 和 $\\Theta_{\\text {MLP }}$ 通过无监督损失函数$\\mathcal{L}_{u}$被联合优化:\n$$ \\mathcal{L}_{u}=\\mathcal{L}_{c}+\\mathcal{L}_{o}=\\underbrace{-\\frac{\\operatorname{Tr}\\left(\\mathbf{S}^{T} \\tilde{\\mathbf{A}} \\mathbf{S}\\right)}{\\operatorname{Tr}\\left(\\mathbf{S}^{T} \\tilde{\\mathbf{D}} \\mathbf{S}\\right)}}_{\\mathcal{L}_{c}}+\\underbrace{\\left|\\frac{\\mathbf{S}^{T} \\mathbf{S}}{\\left|\\mathbf{S}^{T} \\mathbf{S}\\right|_{F}}-\\frac{\\mathbf{I}_{K}}{\\sqrt{K}}\\right|_{F}}_{\\mathcal{L}_{o}} $$ 其中 $|\\cdot|_{F}$ 表示 Frobenius 范数.$\\mathcal{L}_{c}$形式类似于上面的 minCUT 求最优解的问题，$\\mathcal{L}_{o}$是一个正则项，它让学习出来的$\\mathbf{S}$趋向于正交矩阵以及聚类后每一个划分的节点数趋向相似的大小。\n建图 构建特征矩阵$X$ 由于上图的任务是从数据中提取直线，数据中每两个点构成一个直线模型假设，设图中离散点的数目为$N$，生成的总假设模型数为$M$，即在原始数据中随机采样$M$次生成所有的假设模型，然后对于数据中每个离散点，考察该点到$M$个假设模型的距离。这样可以构建一个维度为$N \\times M$的特征矩阵$X$，当第$i$点到假设模型$j$的欧氏距离小于等于给定阈值$T$，$X_(i,j)=1$,否则$X_(i,j)=0$。\n构建邻接矩阵$A$ 构建一个合理的邻接矩阵有助于网络收敛，代码中为每个节点选择K个邻域节点，具体方式如下：\n在步骤1中，每个节点表示为一个$M$维向量，为给每个节点寻找$K$近邻，需要在$N$个$M$维向量中为每个向量寻找相似度最高的$K$个向量，代码中用 faiss 库完成向量相似度检索。\n这样我们就得到了一个N×N维的稀疏矩阵A。\n数据转换保存 保存特征矩阵X，邻接矩阵A以及原始数据坐标，并转换为pytorch geometric需要的数据格式，完成建图。\n网络结构选取 消息传播层( MP )可以是一层或多层 GNN 层的堆叠，在试验中我选用了 GENeralized Aggregation Networks (GEN)2 ,它是 GCN 的一个变体，相较 GCN 文章首先提出可微的广义消息聚合 函数，它定义了一系列置换不变函数，并且进一步介绍了残余连接和消息规范化层的新变体，这有助于节点特征的更有效聚合。\n池化层( Pool )采用的是文章1提出的 MinCutPool 层。\n实验结果 直线聚类结果 二次曲线聚类结果 目前发现该方法不足之处 聚类的划分数需要事先确定，若数据中存在待拟合的模型数未知，网络难以收敛，或者收敛到一个不理想的结果。\n聚类结果好坏依赖于构建的初始邻接矩阵的质量，虽然在试验中发现一些 GNN 层能有效地动态改变图中节点的邻接关系，但对于模型数较多的数据(如 star11 )，当初始邻接矩阵不理想，拟合效果仍然不好。\n参考文献 -[1] Spectral Clustering with Graph Neural Networks for Graph Pooling\n-[2] DeeperGCN: All You Need to Train Deeper GCNs\n","title":"图卷积网络在多模型拟合中的应用","uri":"/contents/gnn/model_fit/"},{"categories":["contents"],"content":"本文中一些重要的符号定义 图卷积神经网络的提出 传统的卷积神经网络只能处理欧氏空间数据 (如图像 、文本和语音 )。以图像数据为例，图像数据规则的栅格形式允许我们在数据空间定义具有全局共享的卷积核，这类型的卷积核和传统的池化操作使得图像数据具有平移不变形。基于此 ,卷积神经网络通过学习在每个像素点共享的卷积核来建模局部连接，进而为图片学习到意义丰富的隐层表示。\n但对于非欧数据，传统的卷积操作难以应用到上面。比如图数据和流形数据，这两类数据有个特点就是，排列不整齐，比较的随意。具体体现在：对于数据中的某个点，难以定义出其邻居节点出来，或者是不同节点的邻居节点的数量是不同的，这个其实是一个特别麻烦的问题，因为这样就意味着难以在这类型的数据上定义出和图像等数据上相同的卷积操作出来。\n图卷积神经网络关注如何在图上构造深度学习模型，图卷积分为谱域图卷积和空域图卷积，谱域图卷积根据图谱理论和卷积定理,将数据由空域转换到谱域做处理。空域图卷积不依靠图谱卷积理论,直接在空间上定义卷积操作。以这两种卷积算子实现的模型称为谱域图卷积模型与空域图卷积模型。基于卷积定理的图卷积神经网络称为谱域图卷积神经网络，基于设计聚合函数来聚合图上每个中心节点和其邻近节点特征的神经网络称为空域图卷积神经网络。\n有关图卷积神经网络的研究，自2019年以来顶会接收的相关文章呈现爆炸性的增长，详情见此github仓库。\n谱域图卷积与空域图卷积的联系 谱域图卷积有比较坚实的理论基础，相较于谱域图卷积，空域图卷积的定义直观，灵活性更高。关于他们的联系，这里先直接摆出结论，便于对谱域图卷积的定义形式到空域图卷积的定义形式的演变有一个整体的把握。\n谱域图卷积的形式表示为： $$ x_{G}^{*} y=\\boldsymbol{U}\\left(\\left(\\boldsymbol{U}^{\\mathrm{T}} x\\right) \\odot\\left(\\boldsymbol{U}^{\\mathrm{T}} y\\right)\\right) $$\n其中$_G^*$表示图卷积算子，$x,y$ 表示图上节点域的信号，$\\odot$ 表示哈达玛乘法，则两个向量的对应元素相乘。\n空域图卷积可以看做特殊形式的一阶切比雪夫卷积: $$ \\mathbf{Y}=\\hat{\\mathbf{A}} \\mathbf{X} \\mathbf{W}, \\quad \\hat{\\mathbf{A}} \\in R^{N \\times N}, \\mathbf{X} \\in R^{N \\times C}, \\mathbf{W} \\in R^{C \\times D} $$\n其中 $\\hat{\\mathbf{A}} =\\tilde{\\mathbf{D}}^{-1 / 2} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-1 / 2}$, $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$,$\\mathbf{A}$指的是邻接矩阵，$\\mathbf{I}$指的是单位阵，$ \\tilde{\\boldsymbol{D}}_{i i}=\\sum_{j} \\tilde{\\boldsymbol{A}}_{i, j} $,$\\hat{\\mathbf{A}}$是对$\\tilde{\\mathbf{A}}$的归一化操作，将它的特征值范围约束到0和1之间。\n谱域图卷积到空域图卷积的演变过程将在下文陈述。\n卷积算子 在信号与处理邻域，由卷积定理，两信号在空域(或者时域)的卷积的傅里叶变换等于这两信号在 谱域（频域）中的傅里叶变换的乘积，即：\n$$ \\mathcal{F}\\left[f_{1}(t) \\star f_{2}(t)\\right]=F_{1}(w) \\cdot F_{2}(w) $$\n此公式等价为：\n$$ f_{1}(t) \\star f_{2}(t)=\\mathcal{F}^{-1}\\left[F_{1}(w) \\cdot F_{2}(w)\\right] $$\n上面公式可以理解为： 两信号在空域的卷积等价于：先将两信号转到 谱域中进行相乘操作，再将相乘的结果转换到空域。\n连续信号的傅里叶变换和反变换：\n$$ F(w)=\\mathcal{F}[f(t)]=\\int_{\\mathbb{R}} f(t) e^{-2 \\pi i w t} d t $$\n$$ f(t)=\\mathcal{F}^{-1}[F(w)]=\\int_{\\mathbb{R}} F(t) e^{2 \\pi i w t} d t $$\n离散信号的傅里叶变换和反变换：\n$$ F(w)=\\sum_{t=1}^{n} f(t) e^{-i \\frac{2 \\pi}{n} w t} $$\n$$ f(t)=\\frac{1}{n} \\sum_{w=1}^{n} F(w) e^{i \\frac{2 \\pi}{n} w t} $$\n不管是连续形式还是离散形式，有空域转为谱域的关键是找到一组正交基，对于$n$维空间，即找到$n$组线性无关的向量。\n经典的傅里叶变换有如下规律：傅里叶变换的基函数是拉普拉斯算子的本征函数。严格的数学证明见引用1。拉普拉斯算子定义为梯度的散度：\n$$ \\Delta f=\\nabla \\cdot(\\nabla f)=\\operatorname{div}(\\operatorname{grad}(f)) $$\n在$n$维欧几里得空间，可以认为拉普拉斯算子是一个二阶微分算子，即在各个维度求二阶导数后求和。\n对于离散情况，譬如对于二维图像，两个变量的离散拉普拉斯算子可以写成：\n$$ \\frac{\\partial f}{\\partial x}=f^{\\prime}(x)=f(x+1)-f(x) $$\n$$ \\frac{\\partial^{2} f}{\\partial x^{2}}=f^{\\prime \\prime}(x)=f^{\\prime}(x+1)-f^{\\prime}(x)=f(x+1)+f(x-1)-2 f(x) $$\n$$ \\Delta f=\\frac{\\partial^{2} f}{\\partial x^{2}}+\\frac{\\partial^{2} f}{\\partial y^{2}}=f(x+1, y)+f(x-1, y)+f(x, y-1)+f(x, y+1)-4 f(x, y) $$\n将上面公式的每一个正项和负项结合，可以理解为欧氏空间内,二维的拉普拉斯算子是中心节点与周围节点的差值,然后求和。\n将该思想拓展到图(graph)上，可以定义图上的拉普拉斯算子：\n$$ \\Delta f_{i}=\\sum_{(i, j) \\in \\mathcal{E}}\\left(f_{i}-f_{j}\\right) $$\n其中 $\\quad f=\\left(f_{1}, f_{2}, \\cdots f_{n}\\right),$ 代表n个节点上每个节点的信号。\n谱域图卷积 要定义图上的拉普拉斯算子，有必要先把图的几个基本概念阐述清楚：\n基本概念 下文中指的图都为无向图 $$ \\quad \\mathcal{G}=(\\mathcal{V}, \\mathcal{E}, A) \\quad|\\mathcal{V}|=n $$\n邻接矩阵表示顶点与顶点之间的连接关系4 $$ A \\in \\mathbb{R}^{n \\times n} $$\n度矩阵\n$$ \\quad D \\in \\mathbb{R}^{n \\times n} \\quad D_{i i}=\\sum_{j} W_{i j} $$\n$D_{ii}$ 表示以顶点 $v_i$为端点的边的数目。\n图上定义的拉普拉斯矩阵：\n$$ L = D - A $$\n上述概念的一个直观的例子： 论文2中提出，拉普拉斯矩阵是定义在图上的一种拉普拉斯算子，前面说过傅里叶变换的基函数是拉普拉斯算子的本征函数，类似的,图傅里叶变换的基函数即为拉普拉斯矩阵的特征向量。通过对 $L$的矩阵特征分解，我们可以为图傅里叶变换找到一组正交基。\n拉普拉斯矩阵由图的邻接矩阵和度决定，给定一个图，拉普拉斯矩阵是确定的，换句话来说，如果一个图的拓扑形式发生改变，需要重新计算拉普拉斯矩阵。\n可以证明，拉普拉斯矩阵$L$是对称半正定矩阵,，对于一个$n$维向量$f$:\n$$ f^{T} L f=f^{T} D f-f^{T} W f=\\sum_{i=1}^{n} d_{i} f_{i}^{2}-\\sum_{i, j=1}^{n} f_{i} f_{j} W_{i j} $$\n$$ =\\frac{1}{2}\\left(\\sum_{i=1}^{n} d_{i} f_{i}^{2}-2 \\sum_{i, j=1}^{n} f_{i} f_{j} W_{i j}+\\sum_{j=1}^{n} d_{j} f_{j}^{2}\\right) $$\n$$ =\\frac{1}{2}\\left(\\sum_{i, j=1}^{n} W_{i j}\\left(f_{i}-f_{j}\\right)^{2}\\right) \\geq 0 $$\n作为对称半正定矩阵,拉普拉斯矩阵有如下性质:\n$n$阶对称矩阵一定有$n$个线性无关的特征向量。\n对称矩阵的不同特征值对应的特征向量相互正交,这些正交的特征向量构成的矩阵为正交矩阵。\n实对称矩阵的特征向量一定是实向量\n半正定矩阵的特征值一定非负。\n对拉普拉斯矩阵特征分解：\n$$ L=U \\Lambda U^{-1} $$\n$$ U=\\left(\\vec{u}_{1}, \\vec{u}_{2}, \\cdots, \\vec{u}_{n}\\right) $$\n$$ \\vec{u}_{i} \\in \\mathbb{R}^{n}, i=1,2, \\cdots n $$\n需要说明的是，拉普拉斯矩阵的特征向量组成的矩阵$U$不仅是正交矩阵，它还是标准正交矩阵。这样，通过计算$U=\\left(\\vec{u}_{1}, \\vec{u}_{2}, \\cdots, \\vec{u}_{n}\\right)$,我们可以使用拉普拉斯的特征向量作为基函数,任意的图上的信号可以表示为:\n$$ x=\\hat{x}\\left(\\lambda_{1}\\right) \\vec{u}_{1}+\\hat{x}\\left(\\lambda_{2}\\right) \\vec{u}_{2}+\\cdots+\\hat{x}\\left(\\lambda_{n}\\right) \\vec{u}_{n} $$\n转为矩阵形式，图傅里叶反变换定义如下： 图傅里叶正变换定义如下：\n图傅里叶正变换将图上任意信号与每个正交基做内积求出各项的傅里叶系数，图傅里叶反变换是说，图上任意信号可以由一组正交基的线性组合来表示。 由此得出谱域图卷积的定义：\n$$ x_{G}^{*} g=\\boldsymbol{U}\\left(\\left(\\boldsymbol{U}^{\\mathrm{T}} x\\right) \\odot\\left(\\boldsymbol{U}^{\\mathrm{T}} g\\right)\\right) $$\n定义 $g_{\\theta} = diag(\\boldsymbol{U}^Tg) $为谱域上的卷积算子，上述公式还可以表示为：\n$$ x ^\\star_{G} g_{\\theta}=\\boldsymbol{U}g_{\\theta}\\boldsymbol{U}^{\\mathrm{T}} x $$\n$g_{\\theta}$是一个$n$维的对角阵。\n有不少关于谱域卷积的工作，SCNN 用可学习的对角矩阵来代替谱域的卷积核；ChebNet 采用切比雪夫多项式来代替谱域的卷积核；GCN 仅考虑1阶切比雪夫多项式，且每个卷积核仅只有一个参数。\n谱域图卷积的不足之处 上面的公式在图数据的应用上存在一定的局限性：\n参与卷积的图上信号$x$是全图的所有节点的信号（$x \\in \\mathbb{R}^{n \\times c}$）,它不是一个局部链接的结构。 由于矩阵$U$是由拉普拉斯矩阵$L$特征分解而来，$L$是一个对称半正定矩阵，这要求处理的图解钩是无向图。 由于进行谱域图卷积需要事先确定$L$，在模型训练过程中，图结构不能变化，而在某些场景下，图结构是可能发生变化的（社交网络数据，交通数据等。 空域图卷积 空域图卷积的定义 前面提到，在\n$$ x ^\\star_{G} g_{\\theta}=\\boldsymbol{U}g_{\\theta}\\boldsymbol{U}^{\\mathrm{T}} x $$\n中，$g_{\\theta}$是一个$n$维的对角阵。一个谱域图卷积网络 ChebyNet3对$g_{\\theta}$参数化：\n$$ \\boldsymbol{g}_{\\theta}=\\sum_{i=0}^{K-1} \\theta_{k} T_{k}(\\hat{\\Lambda}) $$ 其中 $ \\theta_{k}$ 是需要学的系数 ,$\\hat{\\Lambda}=\\frac{2 \\Lambda}{\\lambda_{\\max }}-\\mathbf{I}_{n} $。 切比雪夫多项式是通过递归得到，递归表达式为\n$$ T_{k}(x)=2 x T_{k-1}(x)-T_{k-2}(x) $$\n其中,\n$$ T_{0}(x)=1, T_{1}(x)=x $$\n这样，有：\n$$ x ^\\star_{G} g_{\\theta} =U g_{\\theta} U^{T} x =U \\sum_{k=0}^{K} \\beta_{k} T_{k}(\\hat{\\Lambda}) U^{T}x $$\n$$ = \\sum_{k=0}^{K} \\beta_{k} T_{k}(U \\hat{\\Lambda} U^{T}) x =\\sum_{k=0}^{K} \\beta_{k} T_{k}(\\hat{L}) x $$\n其中$\\hat{L}=\\frac{2}{\\lambda_{\\max }}\\tilde L-I_{n}$, $\\lambda_{\\max } = 2$,$\\tilde L$表示归一化的拉普拉斯矩阵： $\\tilde L = I_n - D^{-1/2}AD^{-1/2}$,故$\\hat{L}$的表达式最终化简为：\n$$ \\hat{L} = - D^{-1/2}AD^{-1/2} $$\n当仅考虑一阶切比雪夫多项式，即$K = 1$时，\n$$ x \\star_{G} g_{\\theta} =\\sum_{k=0}^{K} \\beta_{k} T_{k}(\\hat{L}) x=\\sum_{k=0}^{1} \\beta_{k} T_{k}(\\hat{L}) x $$\n$$ =\\beta_{0} T_{0}(\\hat{L}) x+\\beta_{1} T_{1}(\\hat{L}) x =\\left(\\beta_{0}+\\beta_{1} \\hat{L}\\right) x $$\n令 $\\beta_{1} = -\\beta_{0}$，\n故：\n$$ x \\star_{G} g_{\\theta} = \\left(\\beta_{0}+\\beta_{1} \\hat{L}\\right) x = \\beta_{0}\\left( I_n + D^{-1/2}AD^{-1/2}\\right)x $$\n由于$I_n + D^{-1/2}AD^{-1/2}$具有[0,2]的特征值，如果在深度神经网络模型中使 用该算子,则反复应用该算子会导致数值不稳定(发散)和梯度爆炸/消失, 为了解决该问题, 引入了一个 renormalization trick:\n$$ I_{n}+D^{-1 / 2} A D^{-1 / 2} \\rightarrow \\tilde{D}^{-1 / 2} \\tilde{A} \\tilde{D}^{-1 / 2} $$\n$$ \\tilde{A}=A+I_{n} \\quad \\tilde{D}_{i i}=\\sum \\tilde{A}_{i j} $$\n这其实是谱域GCN（图卷积网络）参数化的卷积函数，但从空域的角度来看，一阶图卷积网络将$\\tilde{D}^{-1 / 2} \\tilde{A} \\tilde{D}^{-1 / 2}$当做聚合函数，令$\\hat{\\mathbf{A}} =\\tilde{\\mathbf{D}}^{-1 / 2} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-1 / 2}$，有：\n$$ x \\star_{G} g_{\\theta} = \\beta_{0}\\hat{\\mathbf{A}}x $$\n从空域的理解上面的式子，它的物理意义是：\n为图上节点添加自连接边 局部空间信息融合 归一化 这里的 $\\beta_{0}$是一个尺度因子，在神经网络模型中会被归一化操作替代，因此不妨设$\\beta_{0} = 1$，为了加强网络的拟合能力，有研究引入一个参数化的权重矩阵$W$对输入的图信号矩阵$x$做仿射变换，于是有：\n$$ \\mathbf{Y} = x \\star_{G} g_{\\theta} = \\hat{\\mathbf{A}}x \\mathbf{W} $$\n$$ \\hat{\\mathbf{A}} \\in R^{N \\times N}, x \\in R^{N \\times C}, \\mathbf{W} \\in R^{C \\times D} $$\n为和文章开始引入的公式对齐，令 $x = \\mathbf{X}$,故单层的空域上的图卷积层可以定义为：\n$$ \\mathbf X’ = \\sigma \\left(\\hat{\\mathbf{A}}\\mathbf{X} \\mathbf{W}\\right) $$\n可以看到空间域图卷积实际上可以由一阶切比雪夫卷积导出，它也可以看做GCN在空域视角的表达形式。\n空域图卷积的几个模型介绍 作为深度学习域图数据结合的代表性方法，GCN的出现带动了将神经网络技术运用到图数据的学习任务中去的一大类方法。从空域角度看GCN，本质上是一个迭代式地聚合邻居的过程，这启发了一大类模型对于这种聚类操作的重新设计，以加强模型对于图数据的适应性，下面将介绍几个空域图卷积的模型。\nGAT GAT4即 GRAPH ATTENTION NETWORKS,其核心思想为将attention引入到图卷积模型中。图注意力网络利用注意力机制定义聚合函数，与以往关心边上信息的模型不同，在图注意力网络中，邻接矩阵仅被用来定义相关节点，而关联权重的计算则是依赖于节点的特征表达。图注意力网络的每一层结构如下图所示： 1.使用注意力机制计算节点之间的关联度\n$$ e_{i j}=\\alpha\\left(\\mathbf{W} \\vec{h}_{i}, \\mathbf{W} \\vec{h}_{j}\\right)=\\vec{a}^{T}\\left[\\mathbf{W} \\vec{h}_{i}|| \\mathbf{W} \\vec{h}_{j}\\right] $$\n其中 $\\vec{h}_{i} \\in \\mathbb{R}^{F}$ 表示i节点的特征, F表示节点特征 的通道数。$W \\in \\mathbb{R}^{F \\times F^{\\prime}}$ 是可学习的线性变换参数。 $\\alpha(\\cdot): \\mathbb{R}^{F^{\\prime}} \\times \\mathbb{R}^{F^{\\prime}} \\rightarrow \\mathbb{R}$ 表示注意力机制, 其输出 $e_{i j}$ 为注意力系数。 在实验中 $\\alpha(\\cdot)$ 的实现是一个单层前馈神经网络。 ||表示拼接。 $\\vec{a}^{T} \\in \\mathbb{R}^{2 F^{\\prime}}$ 为可学习参数。\n用softmax 归一化注意力系数： $$ \\alpha_{i j}=\\operatorname{softmax}_{j}\\left(e_{i j}\\right)=\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(e_{i k}\\right)} $$ 若将$\\alpha_{i j}$和$\\mathbf{W} $结合起来看做一个系数,实际上GAT对邻域的每个节点隐式地(implicitly)分配了不同的卷积核参数。\n从图注意力网络开始 ,节点之间的权重计算开始从依赖于网络的结构信息转移到依赖于节点的 特征表达 ,但是以上模型在处理时需要加载整个网络的节点特征 ,这阻碍了模型在大规模网络上的应用。\nGraphSAGE GraphSAGE5不同于以往模型考虑所有邻近节点 ,图采样聚合网络对邻近节点做随机采样 ,使得每个节点的邻近节点都小于给定的采样个数.\nGraphSAGE认为卷积可以理解为采样和信息聚合两个步骤的结合，邻域的节点不需要进行排序，设计的聚合函数需要与输入顺序无关。\nGraphSAGE 的聚合函数有多种形式，分别是基于最大值的聚合，基于均值的聚合和长短时记忆力网络(LSTM)。最大值聚合和均值聚合分别指取相关节点的最大值和均值作为聚合结果。长短时记忆网络指将相关节点输入LSTM，并把输出作为聚合结果。 图采样聚合网络提出用分批量处理数据的方法训练模型 ,在每个批量输入数据下只需要加载对应节点的局部结构 ,避免了整张网络的加载，这使得在大规模数据集上搭建图卷积神经网络成为可能。\nPGC PGC6将卷积理解为特定的采样函数与特定的权重函数相乘后求和。与GraphSAGE认为邻域不需要排序，并且使用均值采样确定邻域不同，PGC采用更加泛化的方式，为邻域节点根据某种度量分类，不同的类别分配不同的权重函数。 池化算子 在传统的卷积神经网络中，卷积会和池化相结合 ,池化算子一方面能够减少学习的参数 ,另外一方面能反应输入数据的层次结构 ,而在图卷积神经网络中 ,在解决节点级别的任务如节点分类，链接预测时，池化算子并非必要。但在图层面的学习任务中，如图分类问题需要关注图数据的全局信息，既包含图的结构信息，也包含各个节点的属性信息，模型的重点在于如何学习一个优秀的全图表示向量，在这个层面上，如何实现层次化池化机制是GNN需要解决的基础问题。\n层次化池化机制的几种实现思路 基于图坍塌的池化机制 图坍塌是将图划分为不同的子图，然后将子图视为超级节点，从而形成一个坍塌的图。\n基于TopK 的池化机制 对图中每个节点学习出一个分数，基于这个分数的排序丢弃一些低分数的节点，这类方法借鉴了CNN中最大池化操作的思路：将更重要的信息筛选出来。所不同的是，图数据难以实现局部滑窗操作，因此需要依据分数进行全局筛选。\n基于边收缩的池化操作 边收缩是指并行地将图中的边移除，并将移除边的两个节点合并，同时保持被移除节点的连接关系，该思路通过归并操作来逐步学习图的全局信息的方法。\n图卷积神经网络的过平滑现象以及缓解方案 GCN的过平滑现象 论文7在图卷积神经网络的协同训练展开详细分析 ,指出 GCN 的本质是拉普拉斯平滑。\n对图做拉普拉斯平滑,其形式如下： $$ \\left(\\boldsymbol{I}-\\gamma \\hat{\\boldsymbol{D}}^{-1} \\hat{\\boldsymbol{L}}\\right) X $$ 其中 $, \\hat{L}=\\hat{D}-\\hat{A}, 0\u003c\\gamma\u003c1$ 是超参数，用以调节源节点 和周围节点的权重.令 $\\gamma=1$,则拉普拉斯平滑形式为 $\\hat{\\mathbf{D}}^{-1} \\hat{\\boldsymbol{A}} X,$ 此时如果用对称标准化的拉普拉斯算子 $\\hat{\\boldsymbol{D}}^{-\\frac{1}{2}} \\hat{\\boldsymbol{L}} \\hat{\\boldsymbol{D}}^{-\\frac{1}{2}}$ 替代 $\\hat{\\boldsymbol{D}}^{-1} \\hat{\\boldsymbol{L}},$ 则上式等价为 $\\hat{\\boldsymbol{D}}^{-\\frac{1}{2}} \\hat{\\boldsymbol{A}} \\hat{\\boldsymbol{D}}^{-\\frac{1}{2}} X$ 由此可见，GCN 的本质是在网络上做拉普拉斯平 滑.拉普拉斯平滑使用每个节点的邻居及自身表达 的加权和作为当前节点的新特征，由于在分类任务 中，同类节点倾向于稠密连接，平滑使得同类节点的 特征更为相似，进而提升了下游的分类任务.同时文 章指出，多层的 GCN 在更新目标节点表达时，会混合其他类节点的信息，进而导致效果降低，这是多层GCN存在的过平滑现象。\n过平滑现象的缓解方案 浅层图卷积网络的感受野，特征提取能力有限，多层图卷积网络具有强大特征提取能力，但多次拉普拉斯平滑会使输出特征过度平滑，同时会带来梯度消失，过拟合的问题。\n缓解过平滑问题的一些方法：\n深度拓展：高层特征融合底层特征 宽度拓展：全局特征融合局部特征 图卷积神经网络的任务分类 图卷积神经网络在计算机视觉中的应用 基于图匹配的直线匹配 基于谱聚类的模型拟合 参考文献 [1] The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and other Irregular Domains. IEEE Signal Processing Magazine [2] Discrete Regularization on Weighted Graphs for Image and Mesh Filtering [3] Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in neural information processing systems (NIPS), 2016. [4] Graph Attention Networks ICLR 2018 [5] Inductive representation learning on large graphs, in Proc. of NIPS, 2017 [6] Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action. AAAI. 2018. [7] Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, in AAAI 2019. ","title":"图卷积神经网络总结","uri":"/contents/gnn/introducegcnn/"},{"categories":["contents"],"content":"效果图 基于模板匹配的方法，用双目相机测量圆柱体的两端距离，示意图：\n重建模板点云：\n依赖环境： 1 2 3 4 5 6 ubuntu 18.04 opencv 3.4.3 opencv_contrib 3.4.3 (为了使用sift) eigen 3.2.10 ceres-solover 1.13.0 pcl-1.9.1 流程 读取双目相机标定参数，完成图像去畸变\n模板图片与图片中的模板用sift提取特征，匹配，得到图像中模板所在的区域\n对左右图进行特征提取和匹配，只保留在模板区域内的特征。\n对这些匹配的特征点三角化，在pcl中拟合出圆柱参数，完成半径拟合。\n对3中的匹配特征对，在模板中找到它的位置，记录它的(x,y)坐标，筛选一些在左右模板尽可能在y坐标上接近的点云对，计算它的距离，利用圆柱的几何信息，计算它沿着圆柱高的分量的长度。\n该分量加上模板中的(x,y)信息，即为左右模板两端的实际距离，完成测距。\n","title":"模板匹配求圆柱的半径和长度","uri":"/contents/c++/templatefitdistanceandradius/"},{"categories":["contents"],"content":"标定流程 读取所有图像，将所有图像灰度化并保存，对于黑色边沿的棋盘格，需要对灰度图做反色处理。\n选择两相机图像质量好的图像对，评估标准是图像上棋盘格的所有角点均被检测出来。\n首先对双目相机的每个镜头完成单目标定，获取相机的内参数和一组棋盘格与相机的相对 $R,t$\n利用非线性优化求解两相机的相对 $R,t$\n假设棋盘格的世界坐标是 $X_w$，左相机中棋盘格的相机坐标是 $X_c^l$， 右相机中棋盘格的相机坐标是 $X_c^r$，由标定出来的棋盘格到相机的 $R,t$，有：\n$$ X_c^l = [R_l|t_l] \\centerdot X_w = R_l \\centerdot \\tilde{X_w} + t_l $$\n同理：\n$$ X_c^r= [R_r|t_r] \\centerdot X_w = R_r \\centerdot \\tilde{X_w} + t_r $$\n上面两式子提取 $x_w$，等价于：\n$$ R_r \\centerdot R_l^{-1} (X_c^l - t_l) = X_c^r - t_r $$\n令： $$ R_{\\_l\\_r} = R_r \\centerdot R_l^{-1} $$\n而：\n$$ t_{\\_l\\_r} = t_r - R_{\\_l\\_r} \\centerdot t_l $$\n有：\n$$ R_{\\_l\\_r} X_c^l + t_{\\_l\\_r} = X_c^r $$\n这两项即为左右相机的相对$R,t$\n构建误差项：\n$$ E = R_{\\_l\\_r} X_c^l + t_{\\_l\\_r} - X_c^r $$\n把以上计算的值当做初始值，用 ceres BA算法即可算出两相机的相对 $R,t$\n5.计算本质矩阵 E和基础矩阵F\n$$ E = t^{rm} \\centerdot R $$\n$t^{rm}$代表平移向量的反对称矩阵\n$$ F = K^{-T}EK^{-1} $$\n至此完成相机的双目标定。\n","title":"双目相机标定","uri":"/contents/c++/binocularcalibration/"},{"categories":["contents"],"content":"总结文章主要来自令人拍案称奇的Mask RCNN 和 一文读懂Faster RCNN。\nFaster RCNN的网络结构 Mask RCNN沿用了Faster RCNN的思想，特征提取采用ResNet-FPN的架构，另外多加了一个Mask预测分支。首先分析Faster RCNN的网络结构：\n依作者看来，如图1，Faster RCNN其实可以分为4个主要内容：\n1.Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：\n所有的conv层都是：kernel_size=3，pad=1，stride=1 所有的pooling层都是：kernel_size=2，pad=0，stride=2。 在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ pad=1，即填充一圈0），导致原图变为 (M+2)x(N+2)大小，再做3x3卷积后输出MxN 。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。\n类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)x(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。\n那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的feature map中都可以和原图对应起来。\n2.Region Proposal Networks。经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。 Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：\n生成anchors，利用[公式]对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致） 按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors 限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界（见文章底部QA部分图21） 剔除尺寸非常小的positive anchors 对剩余的positive anchors进行NMS（nonmaximum suppression） Proposal Layer有3个输入：positive和negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的(e.g. 300)结果作为proposal输出。\n3.Roi Pooling。对于传统的CNN（如AlexNet和VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。所以Faster R-CNN中提出了RoI Pooling解决这个问题。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。\n流程：\n由于proposal是对应MXN尺度的，所以首先使用spatial_scale参数将其映射回(M/16)X(N/16)大小的feature map尺度； 再将每个proposal对应的feature map区域水平分为 [公式] 的网格； 对网格的每一份都进行max pooling处理。 4.Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。\n下图展示了python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构，可以清晰的看到该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络；而Conv layers中包含了13个conv层+13个relu层+4个pooling层；RPN网络首先经过3x3卷积，再分别生成positive anchors和对应bounding box regression偏移量，然后计算出proposals；而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object。\nmask r-cnn 新增的网络结构 ResNet-FPN 多尺度检测在目标检测中变得越来越重要，对小目标的检测尤其如此。现在主流的目标检测方法很多都用到了多尺度的方法，包括最新的yolo v3。Feature Pyramid Network (FPN)则是一种精心设计的多尺度检测方法，下面就开始简要介绍FPN。\nFPN结构中包括自下而上，自上而下和横向连接三个部分，如下图所示。这种结构可以将各个层级的特征进行融合，使其同时具有强语义信息和强空间信息，在特征学习中算是一把利器了。\nFPN实际上是一种通用架构，可以结合各种骨架网络使用，比如VGG，ResNet等。Mask RCNN文章中使用了ResNNet-FPN网络结构。如下图：\nResNet-FPN包括3个部分，自下而上连接，自上而下连接和横向连接。下面分别介绍。\n自下而上 从下到上路径。可以明显看出，其实就是简单的特征提取过程，和传统的没有区别。具体就是将ResNet作为骨架网络，根据feature map的大小分为5个stage。stage2，stage3，stage4和stage5各自最后一层输出conv2，conv3，conv4和conv5分别定义为 $C_2,C_3,C_4,C_5$ ，他们相对于原始图片的stride是{4,8,16,32}。需要注意的是，考虑到内存原因，stage1的conv1并没有使用。\n自上而下和横向连接 自上而下是从最高层开始进行上采样，这里的上采样直接使用的是最近邻上采样，而不是使用反卷积操作，一方面简单，另外一方面可以减少训练参数。横向连接则是将上采样的结果和自底向上生成的相同大小的feature map进行融合。具体就是对 $C_2,C_3,C_4,C_5$ 中的每一层经过一个conv 1x1操作（1x1卷积用于降低通道数），无激活函数操作，输出通道全部设置为相同的256通道，然后和上采样的feature map进行加和操作。在融合之后还会再采用3*3的卷积核对已经融合的特征进行处理，目的是消除上采样的混叠效应（aliasing effect）。\n实际上，上图少绘制了一个分支：M5经过步长为2的max pooling下采样得到 P6，作者指出使用P6是想得到更大的anchor尺度512×512。但P6是只用在 RPN中用来得到region proposal的，并不会作为后续Fast RCNN的输入。\n总结一下，ResNet-FPN作为RPN输入的feature map是 $P_2,P_3,P_4,P_5,P_6$ ，而作为后续Fast RCNN的输入则是 $P_2,P_3,P_4,P_5$ 。\nmask-rcnn中的特征提取网络 将ResNet-FPN和Fast RCNN进行结合，实际上就是Faster RCNN的了，但与最初的Faster RCNN不同的是，FPN产生了特征金字塔 $P_2,P_3,P_4,P_5,P_6$ ，而并非只是一个feature map。金字塔经过RPN之后会产生很多region proposal。这些region proposal是分别由 $P_2,P_3,P_4,P_5,P_6$ 经过RPN产生的，但用于输入到Fast RCNN中的是 $P_2,P_3,P_4,P_5$ ，也就是说要在 $P_2,P_3,P_4,P_5$ 中根据region proposal切出ROI进行后续的分类和回归预测。问题来了，我们要选择哪个feature map来切出这些ROI区域呢？实际上，我们会选择最合适的尺度的feature map来切ROI。具体来说，我们通过一个公式来决定宽w和高h的ROI到底要从哪个 $P_k$ 来切：\n$$k=\\left\\lfloor k_{0}+\\log _{2}(\\sqrt{w h} / 224) \\right\\rfloor.$$\n这里224表示用于预训练的ImageNet图片的大小。 $k_0$ 表示面积为 $wh$ 的ROI所应该在的层级。作者将 $k_0$ 设置为4，也就是说 $wh$ 的ROI应该从 $P_4$ 中切出来。假设ROI的scale小于224（比如说是112 * 112）， k计算出等于3，就意味着要从更高分辨率的 $P_3$ 中产生。另外，$k$ 值会做取整处理，防止结果不是整数。这种做法很合理，大尺度的ROI要从低分辨率的feature map上切，有利于检测大目标，小尺度的ROI要从高分辨率的feature map上切，有利于检测小目标。\nmask分支 Mask RCNN的构建很简单，只是在ROI pooling（实际上用到的是ROIAlign，后面会讲到）之后添加卷积层，进行mask预测的任务。\nROI Align 实际上，Mask RCNN中还有一个很重要的改进，就是ROIAlign。Faster R-CNN存在的问题是：特征图与原始图像是不对准的（mis-alignment），所以会影响检测精度。而Mask R-CNN提出了RoIAlign的方法来取代ROI pooling，RoIAlign可以保留大致的空间位置。\n在Faster RCNN中，有两次整数化的过程：\nregion proposal的 $xywh$ 通常是小数，但是为了方便操作会把它整数化。 将整数化后的边界区域平均分割成 $k * k$ 个单元，对每一个单元的边界进行整数化。 事实上，经过上述两次整数化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度。在论文里，作者把它总结为“不匹配问题”（misalignment）。\n为了解决这个问题，ROI Align方法取消整数化操作，保留了小数，使用以上介绍的双线性插值的方法获得坐标为浮点数的像素点上的图像数值。但在实际操作中，ROI Align并不是简单地补充出候选区域边界上的坐标点，然后进行池化，而是重新进行设计。\n下面通过一个例子来讲解ROI Align操作。如下图所示，虚线部分表示feature map，实线表示ROI，这里将ROI切分成2x2的单元格。如果采样点数是4，那我们首先将每个单元格子均分成四个小方格（如红色线所示），每个小方格中心就是采样点。这些采样点的坐标通常是浮点数，所以需要对采样点像素进行双线性插值（如四个箭头所示），就可以得到该像素点的值了。然后对每个单元格内的四个采样点进行maxpooling，就可以得到最终的ROIAlign的结果。\nMask Rcnn 网络结构： ","title":"Mask Rcnn 网络结构总结","uri":"/contents/dl/mask-rcnn/"},{"categories":["contents"],"content":"依赖环境 系统环境 gcc/g++-7 cuda-10.1 conda环境 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 conda create -n sparsencnet python=3.6.9=h265db76_0 conda activate sparsencnet conda install numpy openblas conda install libstdcxx-ng # set environment variables for the compilation of MinkowskiEngine export CUDA_HOME=/your_path_to/cuda-10.1.243 export LD_LIBRARY_PATH=\"${CUDA_HOME}/lib64\":\"${CONDA_PREFIX}/lib\":\"/usr/lib/x86_64-linux-gnu/\" export PATH=\"${CONDA_PREFIX}/bin\":\"${CUDA_HOME}/bin\":/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin # 这里我设置为系统的g++-7 export CPP=\"/usr/bin/g++ -E\" export CXX=\"/usr/bin/g++\" export LIBRARY_PATH=$LD_LIBRARY_PATH export PYTHONPATH=\"${CONDA_PREFIX}/lib/python3.6/site-packages/\" # install PyTorch and ME ,ME源码安装 pip install torch==1.3.1 torchvision==0.4.2 -i https://pypi.tuna.tsinghua.edu.cn/simple python import torch torch.cuda.is_available() git clone https://github.com/StanfordVL/MinkowskiEngine.git cd MinkowskiEngine export CXX=g++-7; python setup.py install # 测试 MinkowskiEngine python import MinkowskiEngine as ME # 改用源码编译faiss-gpu,用conda安装没跑成功 git clone https://github.com/facebookresearch/faiss.git cd faiss LDFLAGS=-L/home/jiajie/anaconda3/envs/sparsencnet-test/lib/ ./configure --with-cuda=/usr/local/cuda --with-python=python3.6m --with-cuda-arch=\"-gencode=arch=compute_75,code=sm_75\" make \u0026\u0026 sudo make install make -C python make -C python install # install some additional libraries conda install matplotlib scikit-image pandas # replace pillow with pillow-simd pip uninstall pillow CC=\"cc -mavx2\" pip install -U --force-reinstall \"pillow-simd==6.2.2.post1\" -i https://pypi.tuna.tsinghua.edu.cn/simple # install jupyter lab for evaluation on HPatches-Seq conda install -c conda-forge jupyterlab ","title":"Sparse Ncnet","uri":"/contents/dl/sparse-ncnet/"},{"categories":["contents"],"content":"之前用detectron2 的keypoint_rcnn训练自己的数据集完成物体特征点检测，这篇文章是将pytorch模型转为c++调用的形式。 实现效果： Detectron2 提供了将pytorch模型转为c++调用的例程:Deployment,gpu版本的模型转换需要pytorch版本\u003e=1.5.0,ONNX 版本 \u003e= 1.6。跟着官方的demo转换就行。\npytorch的安装命令如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # conda切换清华源，速度快很多 vim ~/.condarc channels: - defaults show_channel_urls: true channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch conda clean -i # 创建conda环境 conda create -n detectron2-pytorch1.5 python=3.7 source activate detectron2-pytorch1.5 conda install pytorch=1.5.0 torchvision cudatoolkit=10.1 -c pytorch # 若速度还是慢，把 “-c pytorch” 去掉 detectron2 的编译参考官方文档 pytorch 模型通过onnx转caffe2，下载libtorch并完成转换。 1 2 3 4 5 6 7 8 9 conda install protobuf numpy pip install onnx ./caffe2_converter.py --config-file ../configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml --output ./caffe2_model_gpu MODEL.WEIGHTS ../output/model_final.pth MODEL.DEVICE cuda cmake -DCMAKE_PREFIX_PATH=/home/jiajie/pytorch/libtorch .. make ./build/caffe2_mask_rcnn --predict_net=./model.pb --init_net=./model_init.pb --input=crop_imglab_45.png caffe2_mask_rcnn.cpp的代码如下，较官方的demo只是添加了我需要的特征点输出部分：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 // Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved #include \u003cc10/util/Flags.h\u003e #include \u003ccaffe2/core/blob.h\u003e #include \u003ccaffe2/core/common.h\u003e #include \u003ccaffe2/core/init.h\u003e #include \u003ccaffe2/core/net.h\u003e #include \u003ccaffe2/core/workspace.h\u003e #include \u003ccaffe2/core/context_gpu.h\u003e #include \u003ccaffe2/utils/proto_utils.h\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003ccassert\u003e #include \u003cchrono\u003e #include \u003ciostream\u003e #include \u003cstring\u003e #include \u003cmath.h\u003e C10_DEFINE_string(predict_net, \"\", \"path to model.pb\"); C10_DEFINE_string(init_net, \"\", \"path to model_init.pb\"); C10_DEFINE_string(input, \"\", \"path to input image\"); using namespace std; using namespace caffe2; int main(int argc, char** argv) { caffe2::GlobalInit(\u0026argc, \u0026argv); string predictNetPath = FLAGS_predict_net; string initNetPath = FLAGS_init_net; cv::Mat input = cv::imread(FLAGS_input, cv::IMREAD_COLOR); const int height = input.rows; const int width = input.cols; // FPN models require divisibility of 32 assert(height % 32 == 0 \u0026\u0026 width % 32 == 0); const int batch = 1; const int channels = 3; // initialize Net and Workspace caffe2::NetDef initNet_, predictNet_; CAFFE_ENFORCE(ReadProtoFromFile(initNetPath, \u0026initNet_)); CAFFE_ENFORCE(ReadProtoFromFile(predictNetPath, \u0026predictNet_)); Workspace workSpace; for (auto\u0026 str : predictNet_.external_input()) { workSpace.CreateBlob(str); } for (auto\u0026 str : predictNet_.external_output()) { cout\u003c\u003c\"name is: \"\u003c\u003cstr\u003c\u003cendl; } CAFFE_ENFORCE(workSpace.CreateNet(predictNet_)); CAFFE_ENFORCE(workSpace.RunNetOnce(initNet_)); // setup inputs auto data = BlobGetMutableTensor(workSpace.GetBlob(\"data\"), caffe2::CPU); data-\u003eResize(batch, channels, height, width); float* ptr = data-\u003emutable_data\u003cfloat\u003e(); // HWC to CHW for (int c = 0; c \u003c 3; ++c) { for (int i = 0; i \u003c height * width; ++i) { ptr[c * height * width + i] = static_cast\u003cfloat\u003e(input.data[3 * i + c]); } } auto im_info = BlobGetMutableTensor(workSpace.GetBlob(\"im_info\"), caffe2::CPU); im_info-\u003eResize(batch, 3); float* im_info_ptr = im_info-\u003emutable_data\u003cfloat\u003e(); im_info_ptr[0] = height; im_info_ptr[1] = width; im_info_ptr[2] = 1.0; // run the network CAFFE_ENFORCE(workSpace.RunNet(predictNet_.name())); // run 3 more times to benchmark int N_benchmark = 3; auto start_time = chrono::high_resolution_clock::now(); for (int i = 0; i \u003c N_benchmark; ++i) { CAFFE_ENFORCE(workSpace.RunNet(predictNet_.name())); } auto end_time = chrono::high_resolution_clock::now(); auto ms = std::chrono::duration_cast\u003cstd::chrono::microseconds\u003e( end_time - start_time) .count(); cout \u003c\u003c \"Latency: \" \u003c\u003c ms * 1.0 / 1e6 / N_benchmark \u003c\u003c \" seconds\" \u003c\u003c endl; // parse Mask R-CNN outputs /* auto\u0026 bbox = BlobGetTensor(*workSpace.GetBlob(\"bbox_nms\"), caffe2::CPU); auto\u0026 scores = BlobGetTensor(*workSpace.GetBlob(\"score_nms\"), caffe2::CPU); auto\u0026 labels = BlobGetTensor(*workSpace.GetBlob(\"class_nms\"), caffe2::CPU); auto\u0026 kps_score = BlobGetTensor(*workSpace.GetBlob(\"kps_score\"), caffe2::CPU); */ caffe2::Tensor bbox( workSpace.GetBlob(\"bbox_nms\")-\u003eGet\u003ccaffe2::Tensor\u003e(), caffe2::CPU); caffe2::Tensor scores( workSpace.GetBlob(\"score_nms\")-\u003eGet\u003ccaffe2::Tensor\u003e(), caffe2::CPU); caffe2::Tensor labels( workSpace.GetBlob(\"class_nms\")-\u003eGet\u003ccaffe2::Tensor\u003e(), caffe2::CPU); caffe2::Tensor kps_score( workSpace.GetBlob(\"kps_score\")-\u003eGet\u003ccaffe2::Tensor\u003e(), caffe2::CPU); cout \u003c\u003c \"bbox:\" \u003c\u003c bbox.DebugString() \u003c\u003c endl; cout \u003c\u003c \"scores:\" \u003c\u003c scores.DebugString() \u003c\u003c endl; cout \u003c\u003c \"labels:\" \u003c\u003c labels.DebugString() \u003c\u003c endl; //(#ROIs, #keypoints, POOL_H, POOL_W) cout \u003c\u003c \"kps_score: \" \u003c\u003c kps_score.DebugString() \u003c\u003c endl; //vector\u003cfloat\u003e kps_vec; //const float* kps = kps_score.data\u003cfloat\u003e() + 0 * kps_score.size_from_dim(1); //const float* kps_1 = kps + 0 * kps_score.size_from_dim(2); //cout \u003c\u003c\"the size is: \"\u003c\u003c kps_score.size_from_dim(0) \u003c\u003c endl; int num_instances = bbox.sizes()[0]; for (int i = 0; i \u003c num_instances; ++i) { float score = scores.data\u003cfloat\u003e()[i]; if (score \u003c 0.9) continue; // skip them const float* box = bbox.data\u003cfloat\u003e() + i * 4; int label = labels.data\u003cfloat\u003e()[i]; cout \u003c\u003c \"Prediction \" \u003c\u003c i \u003c\u003c \", xyxy=(\"; cout \u003c\u003c box[0] \u003c\u003c \", \" \u003c\u003c box[1] \u003c\u003c \", \" \u003c\u003c box[2] \u003c\u003c \", \" \u003c\u003c box[3] \u003c\u003c \"); score=\" \u003c\u003c score \u003c\u003c \"; label=\" \u003c\u003c label \u003c\u003c endl; // 从heatmaps中提取特征点. const float* kps = kps_score.data\u003cfloat\u003e() + i * kps_score.size_from_dim(1); float bbx_offset_x = box[0]; float bbx_offset_y = box[1]; float bbx_width = (box[2]-box[0])\u003e0?(box[2]-box[0]):1; float bbx_heights = (box[3]-box[1])\u003e0?(box[3]-box[1]):1; int widths_ceil = ceil(bbx_width); int heights_ceil = ceil(bbx_heights); //cout\u003c\u003c\"widths_ceil size: \"\u003c\u003cwidths_ceil\u003c\u003cendl; //cout\u003c\u003c\"heights_ceil size: \"\u003c\u003cheights_ceil\u003c\u003cendl; int num_keypoints = kps_score.size_from_dim(1)/kps_score.size_from_dim(2); //cout \u003c\u003c \"num_keypoints: \" \u003c\u003c num_keypoints \u003c\u003c endl; float xy_preds[num_keypoints*2] = {0}; float width_corrections = bbx_width / widths_ceil; float height_corrections = bbx_heights / heights_ceil; cv::Size dsize = cv::Size(widths_ceil,heights_ceil); //双线性插值 for(int n = 0;n\u003cnum_keypoints;++n) { cv::Mat src_map = cv::Mat::zeros(56,56,CV_32FC1); cv::Mat resize_roi_map = cv::Mat::zeros(dsize,CV_32FC1); //cout\u003c\u003c\"intput size: \"\u003c\u003csrc_map.cols\u003c\u003c\" , \"\u003c\u003csrc_map.rows\u003c\u003cendl; //cout\u003c\u003c\"output size: \"\u003c\u003cresize_roi_map.cols\u003c\u003c\" , \"\u003c\u003cresize_roi_map.rows\u003c\u003cendl; for(int j = 0;j\u003csrc_map.rows;++j) { for(int k = 0;k\u003csrc_map.cols;++k) { *((float*)(src_map.data+j*src_map.step[0]+k*src_map.step[1])) = kps[n*56*56+j*src_map.cols+k]; } } /* double maxVal1 = 0; cv::Point maxLoc1; cv::minMaxLoc(src_map, NULL, \u0026maxVal1, NULL, \u0026maxLoc1); cout \u003c\u003c \"最大值： \" \u003c\u003c maxVal1 \u003c\u003c endl; cout \u003c\u003c \"最大值位置： \" \u003c\u003c maxLoc1 \u003c\u003c endl; */ cv::resize(src_map, resize_roi_map, dsize, 0, 0); //默认双线性插值 cout\u003c\u003c\"output data size :\"\u003c\u003cresize_roi_map.size()\u003c\u003cendl; double maxVal = 0; cv::Point maxLoc; cv::minMaxLoc(resize_roi_map, NULL, \u0026maxVal, NULL, \u0026maxLoc); //cout \u003c\u003c \"插值最大值： \" \u003c\u003c maxVal \u003c\u003c endl; //cout \u003c\u003c \"插值最大值位置： \" \u003c\u003c maxLoc \u003c\u003c endl; float x_int = float(maxLoc.x); int y_int = maxLoc.y; //尽量减小舍入误差 float x = (float(maxLoc.x) + 0.5) * width_corrections; float y = (float(maxLoc.y) + 0.5) * height_corrections; cout\u003c\u003c\"x: \"\u003c\u003cx\u003c\u003c\" y: \"\u003c\u003cy\u003c\u003cendl; xy_preds[n*2] = bbx_offset_x + x ; xy_preds[n*2 + 1] = bbx_offset_y + y ; } for(int i = 0;i\u003cnum_keypoints;++i) { cout\u003c\u003c\"pred_x: \"\u003c\u003cxy_preds[i*2+0]\u003c\u003c\" pred_y: \"\u003c\u003cxy_preds[i*2+1]\u003c\u003cendl; } } return 0; } 由于转换后的网络输出只是原始输出格式，想要得到特征点在图像中的位置还需要一定的后处理，原始输出是：\n1 2 caffe2::Tensor kps_score( workSpace.GetBlob(\"kps_score\")-\u003eGet\u003ccaffe2::Tensor\u003e(), caffe2::CPU); 输出的kps_score是 numInstance * numKeypoint * heatMap_width * heatMap_height 的数组格式，对于每一个keypoint，网络会输出一个$56 * 56$的heatMap，首先将heapMap通过双线性插值resize到和对应的bbox一样的大小，把它记为heapMap_resize,其中heapMap_resize中最大值元素所在位置即为keypoint的local_position，再加上对应的bbox左上角的偏移量即为keypoint在图像中的坐标了。\n","title":"Detectron2 keypoint_rcnn 网络c++版本部署","uri":"/contents/dl/keypoint-rcnn-caffe2-c++/"},{"categories":["contents"],"content":"官方文档\ndemo 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 import logging import os from collections import OrderedDict import torch from torch.nn.parallel import DistributedDataParallel import detectron2.utils.comm as comm from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer from detectron2.config import get_cfg from detectron2.data import ( MetadataCatalog, build_detection_train_loader, ) from detectron2.engine import default_argument_parser, default_setup, launch from detectron2.modeling import build_model from detectron2.solver import build_lr_scheduler, build_optimizer from detectron2.utils.events import ( CommonMetricPrinter, EventStorage, JSONWriter, TensorboardXWriter, ) logger = logging.getLogger(\"detectron2\") def do_train(cfg, model, resume=False): model.train() optimizer = build_optimizer(cfg, model) scheduler = build_lr_scheduler(cfg, optimizer) checkpointer = DetectionCheckpointer( model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler ) start_iter = ( checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get(\"iteration\", -1) + 1 ) max_iter = cfg.SOLVER.MAX_ITER periodic_checkpointer = PeriodicCheckpointer( checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter ) writers = ( [ CommonMetricPrinter(max_iter), JSONWriter(os.path.join(cfg.OUTPUT_DIR, \"metrics.json\")), TensorboardXWriter(cfg.OUTPUT_DIR), ] if comm.is_main_process() else [] ) # compared to \"train_net.py\", we do not support accurate timing and # precise BN here, because they are not trivial to implement data_loader = build_detection_train_loader(cfg) logger.info(\"Starting training from iteration {}\".format(start_iter)) with EventStorage(start_iter) as storage: for data, iteration in zip(data_loader, range(start_iter, max_iter)): iteration = iteration + 1 storage.step() loss_dict = model(data) losses = sum(loss_dict.values()) #print(data) #print(loss_dict) assert torch.isfinite(losses).all(), loss_dict loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()} losses_reduced = sum(loss for loss in loss_dict_reduced.values()) if comm.is_main_process(): storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced) optimizer.zero_grad() losses.backward() optimizer.step() storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False) scheduler.step() if ( cfg.TEST.EVAL_PERIOD \u003e 0 and iteration % cfg.TEST.EVAL_PERIOD == 0 and iteration != max_iter ): # Compared to \"train_net.py\", the test results are not dumped to EventStorage comm.synchronize() if iteration - start_iter \u003e 5 and (iteration % 20 == 0 or iteration == max_iter): for writer in writers: writer.write() periodic_checkpointer.step(iteration) def setup(args): \"\"\" Create configs and perform basic setups. \"\"\" cfg = get_cfg() cfg.merge_from_file(args.config_file) cfg.merge_from_list(args.opts) cfg.freeze() default_setup( cfg, args ) # if you don't like any of the default setup, write your own setup code return cfg def main(args): cfg = setup(args) model = build_model(cfg) logger.info(\"Model:\\n{}\".format(model)) do_train(cfg, model) if __name__ == \"__main__\": args = default_argument_parser().parse_args() print(\"Command Line Args:\", args) launch( main, args.num_gpus, num_machines=args.num_machines, machine_rank=args.machine_rank, dist_url=args.dist_url, args=(args,), ) setup() 函数：\n加载配置文件，detectron2 里面模型参数，超参数在配置文件里面定义。配置文件里面的参数可以被重覆盖。如：\n1 2 3 4 5 6 7 8 9 10 11 12 _BASE_: \"Base-Keypoint-RCNN-FPN.yaml\" MODEL: #WEIGHTS: \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\" ROI_KEYPOINT_HEAD: NUM_KEYPOINTS: 4 # modify by jiajie # modify by jiajie \"/home/jiajie/pytorch/detectron2/output/model_0004999.pth\" \"/home/jiajie/pytorch/detectron2/pretrain_model/MSRA/R-50.pkl\" WEIGHTS: \"/home/jiajie/pytorch/detectron2/output/model_0072999.pth\" RESNETS: DEPTH: 50 SOLVER: MAX_ITER: 90000 # modify by jiajie CHECKPOINT_PERIOD: 1000 main() 函数：\n1 2 3 4 cfg = setup(args) model = build_model(cfg) logger.info(\"Model:\\n{}\".format(model)) do_train(cfg, model) do_train() 加载配置参数和模型开始训练。\ndo_train() 函数：\n设置优化器，学习率，迭代次数，输出文件: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 model.train() optimizer = build_optimizer(cfg, model) scheduler = build_lr_scheduler(cfg, optimizer) checkpointer = DetectionCheckpointer( model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler ) start_iter = ( checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get(\"iteration\", -1) + 1 ) max_iter = cfg.SOLVER.MAX_ITER periodic_checkpointer = PeriodicCheckpointer( checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter ) writers = ( [ CommonMetricPrinter(max_iter), JSONWriter(os.path.join(cfg.OUTPUT_DIR, \"metrics.json\")), TensorboardXWriter(cfg.OUTPUT_DIR), ] if comm.is_main_process() else [] ) 数据加载: 1 2 data_loader = build_detection_train_loader(cfg) logger.info(\"Starting training from iteration {}\".format(start_iter)) detectron2 里面数据加载分为两部分：\nRegister a Dataset Register your dataset (i.e., tell detectron2 how to obtain your dataset).\nOptionally, register metadata for your dataset.\n需要实现以下的函数：\n1 2 3 4 5 6 def get_dicts(): ... return list[dict] in the following format from detectron2.data import DatasetCatalog DatasetCatalog.register(\"my_dataset\", get_dicts) 此时只是告诉detectron2 数据在哪，图像具体数据还没加载进内存。\nMap a Dataset The role of the mapper is to transform the lightweight, canonical representation of a dataset item into a format that is ready for the model to consume (including, e.g., read images, perform random data augmentation and convert to torch Tensors).\nMapper 的作用是将数据集加载进内存，数据增强，并将数据转换成适合于网络输入的格式。\n训练 接下来是pytorch 的训练步骤：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 with EventStorage(start_iter) as storage: for data, iteration in zip(data_loader, range(start_iter, max_iter)): iteration = iteration + 1 storage.step() loss_dict = model(data) losses = sum(loss_dict.values()) #print(data) #print(loss_dict) assert torch.isfinite(losses).all(), loss_dict loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()} losses_reduced = sum(loss for loss in loss_dict_reduced.values()) if comm.is_main_process(): storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced) optimizer.zero_grad() losses.backward() optimizer.step() storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False) scheduler.step() if ( cfg.TEST.EVAL_PERIOD \u003e 0 and iteration % cfg.TEST.EVAL_PERIOD == 0 and iteration != max_iter ): # Compared to \"train_net.py\", the test results are not dumped to EventStorage comm.synchronize() if iteration - start_iter \u003e 5 and (iteration % 20 == 0 or iteration == max_iter): for writer in writers: writer.write() periodic_checkpointer.step(iteration) ","title":"Detectron2训练步骤","uri":"/contents/tools/detectron2_intro/"},{"categories":["contents"],"content":"原始csv格式数据 标签每行包含图像名称，图像id，物体类别，每个instance的特征点数目，每个instance的id，以及具体每个特征点在图像上的坐标。\n代码根据特征点坐标离原点坐标的距离进行重排序，计算锚框，然后转换为coco数据集的格式。\n转换代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import csv import json image_w = 640 image_h = 640 axis_start_num = 10000 info = {\"description\": \"COCO 2017 Dataset\",\"url\": \"http://cocodataset.org\",\"version\": \"1.0\",\"year\": 2017,\"contributor\": \"COCO Consortium\",\"date_created\": \"2017/09/01\"} licenses = [{\"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\",\"id\": 1,\"name\": \"Attribution-NonCommercial-ShareAlike License\"}, {\"url\": \"http://creativecommons.org/licenses/by-nc/2.0/\",\"id\": 2,\"name\": \"Attribution-NonCommercial License\"}, {\"url\": \"http://creativecommons.org/licenses/by-nc-nd/2.0/\",\"id\": 3,\"name\": \"Attribution-NonCommercial-NoDerivs License\"}, {\"url\": \"http://creativecommons.org/licenses/by/2.0/\",\"id\": 4,\"name\": \"Attribution License\"}, {\"url\": \"http://creativecommons.org/licenses/by-sa/2.0/\",\"id\": 5,\"name\": \"Attribution-ShareAlike License\"}, {\"url\": \"http://creativecommons.org/licenses/by-nd/2.0/\",\"id\": 6,\"name\": \"Attribution-NoDerivs License\"}, {\"url\": \"http://flickr.com/commons/usage/\",\"id\": 7,\"name\": \"No known copyright restrictions\"}, {\"url\": \"http://www.usa.gov/copyright.shtml\",\"id\": 8,\"name\": \"United States Government Work\"}] images = [] annotations = [] categories = [{\"supercategory\": \"person\",\"id\": 1,\"name\": \"person\",\"keypoints\": [\"nose\",\"left_eye\",\"right_eye\",\"left_ear\",\"right_ear\",\"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\"left_wrist\",\"right_wrist\",\"left_hip\",\"right_hip\",\"left_knee\",\"right_knee\",\"left_ankle\",\"right_ankle\"], \"skeleton\": []}] #[16,14],[14,12],[17,15],[15,13],[12,13],[6,12],[7,13],[6,7],[6,8],[7,9],[8,10],[9,11],[2,3],[1,2],[1,3],[2,4],[3,5],[4,6],[5,7]] csv_reader=csv.reader(open('/home/jiajie/pytorch/detectron2/datasets/coco/train_vertexs.csv',encoding='utf-8')) max_image_id = 0 for i,row in enumerate(csv_reader): if(i!=0): #images if(int(row[1])\u003emax_image_id): image_info = {\"license\": 1} image_info['file_name'] = row[0] image_info['coco_url'] = \"\" image_info['height'] = image_h image_info['width'] = image_w image_info['date_captured'] = \"\" image_info['flickr_url'] = \"\" image_info['id'] = int(row[1]) images.append(image_info) max_image_id = int(row[1]) # annotations annotations_info = {\"segmentation\":[[]]} annotations_info['num_keypoints'] = int(row[3]) annotations_info['area'] = 0.0 annotations_info['iscrowd'] = 0 keypoints_dict = {} rank_kp_list = [] for n in range(5,9): keypoints_dict[str(row[n])+\"_\"+str(n)] = pow(int(str(row[n]).split(\"_\")[0]),2)+pow(int(str(row[n]).split(\"_\")[1]),2) rank_kp_list.append(keypoints_dict[str(row[n])+\"_\"+str(n)]) #rank_kp_list.sort() sort_list = sorted(keypoints_dict.items(), key=lambda d: d[1], reverse=False) #print(keypoints_dict) #print(sort_list) keypoints_list = [] keypoints_list.append(int(str(sort_list[0][0]).split(\"_\")[0])) keypoints_list.append(int(str(sort_list[0][0]).split(\"_\")[1])) keypoints_list.append(int(str(sort_list[0][0]).split(\"_\")[2])) keypoints_list.append(int(str(sort_list[1][0]).split(\"_\")[0])) keypoints_list.append(int(str(sort_list[1][0]).split(\"_\")[1])) keypoints_list.append(int(str(sort_list[1][0]).split(\"_\")[2])) keypoints_list.append(int(str(sort_list[2][0]).split(\"_\")[0])) keypoints_list.append(int(str(sort_list[2][0]).split(\"_\")[1])) keypoints_list.append(int(str(sort_list[2][0]).split(\"_\")[2])) keypoints_list.append(int(str(sort_list[3][0]).split(\"_\")[0])) keypoints_list.append(int(str(sort_list[3][0]).split(\"_\")[1])) keypoints_list.append(int(str(sort_list[3][0]).split(\"_\")[2])) annotations_info['keypoints'] = keypoints_list annotations_info['image_id'] = int(row[1]) # bbox x_list = [] y_list = [] for n in range(5,9): if(int(str(row[n]).split(\"_\")[2])!=0 ): x_list.append(int(str(row[n]).split(\"_\")[0])) y_list.append(int(str(row[n]).split(\"_\")[1])) min_x = min(x_list) min_y = min(y_list) max_x = max(x_list) max_y = max(y_list) annotations_info['bbox'] = [min(max(0,min_x-(max_x-min_x)/4),int(image_w)),min(max(0,min_y-(max_y-min_y)/4),int(image_h)),3*(max_x-min_x)/2,3*(max_y-min_y)/2] if(int(row[1])==588): print(min_x,min_y,max_x,max_y) print(annotations_info['bbox']) annotations_info['category_id'] = 1 annotations_info['id'] = axis_start_num+int(i) annotations.append(annotations_info) image = { 'info': info, 'licenses': licenses, 'images': images, 'annotations': annotations, 'categories': categories } with open('/home/jiajie/pytorch/detectron2/datasets/coco/annotations/person_keypoints_train2017.json', 'w') as f: json.dump(image,f) 标签可视化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 import os import cv2 import json import numpy as np from pycocotools.coco import COCO from pycocotools.cocoeval import COCOeval from pycocotools import mask as maskUtils from skimage import io from matplotlib import pyplot as plt # 验证数据集 特征点 dataset_dir = \"/home/jiajie/pytorch/detectron2/datasets/coco/\" coco = COCO(os.path.join(dataset_dir,'annotations','person_keypoints_train2017.json')) catIds = coco.getCatIds(catNms=['person']) # catIds=1 表示人这一类 imgIds = 1771 while(True): img = coco.loadImgs(imgIds)[0] image_path = os.path.join(dataset_dir, 'train2017', '4axis_'+str(imgIds) + '.png') I = io.imread(image_path) plt.axis('off') plt.imshow(I) #绘制图像，显示交给plt.show()处理 bg = np.zeros((img['height'], img['width'], 3)) annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None) anns = coco.loadAnns(annIds) coco.showAnns(anns) plt.show() imgIds = imgIds+3 print(imgIds) # 验证数据集 锚框 json_file = '/home/jiajie/pytorch/detectron2/datasets/coco/annotations/person_keypoints_train2017.json' with open(json_file) as annos: anno = json.load(annos) axis_id = 1102 annotations = anno['annotations'] #print(annotations[imgIds-1]['bbox']) x, y, w, h = annotations[axis_id-1]['bbox'] print(annotations[axis_id-1]['bbox']) image_path = os.path.join(dataset_dir, 'train2017', '4axis_'+str(imgIds) + '.png') image = cv2.imread(image_path) anno_image = cv2.rectangle(image, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 255), 2) cv2.imshow('demo', anno_image) cv2.waitKey(100) plt.show() ","title":"csv转coco人体特征点数据格式","uri":"/contents/tools/cvs2coco_format/"},{"categories":["contents"],"content":" *Ain't got no home, ain't got no shoes* *Ain't got no money, ain't got no class* *Ain't got no skirts, ain't got no sweater* *Ain't got no perfume, ain't got bed* *Ain't got no man* *Ain't got no mother, ain't got no culture* *Ain't got no friends, ain't got no schoolin?* *Ain't got no love, ain't got no name* *Ain't got no ticket, ain't got no token* *Ain't got no god* *What about god, why I am alive anyway* *What about god, nobody can take away* *I got my hair, I got my head* *I got my brains, I got my ears* *I got my eyes, I got my nose* *I got my mouth, I got my smile* *I got my tongue, I got my chin* *I got my neck, I got my boobs* *I got my heart, I got my soul* *I got my back, I got my \\*\\*\\** *I got my arms, got my hands* *I got my fingers, got my legs* *I got my feet, I got my toes* *I got my liver, Got my blood* *I've got life* *I've got my freedom* *I've got a life* *I've got a life* *And I'm gonna give it* *I've got a life* *and nobody's gonna take it away* *I've got a life* ___","title":"Ain’t Got No - I Got Life","uri":"/contents/jazz/ain_t_got_no/"},{"categories":["contents"],"content":" 插件安装 1 2 3 pip install beautifulsoup4 pip install requests pip install lxml 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 import re import requests from urllib import error from bs4 import BeautifulSoup import os import json num = 0 numPicture = 0 file = '' List = [] global num_per_page # 每个页面的图片数 num_per_page=20 def Find(url): global List print('正在检测图片总数.....') t = 0 i = 1 s = 0 while t \u003c 2000: Url = url + str(t) try: Result = requests.get(Url, timeout=7) except BaseException: t = t + num_per_page continue else: result = Result.text pic_url = re.findall('\"objURL\":\"(.*?)\",', result, re.S) # 先利用正则表达式找到图片url s += len(pic_url) if len(pic_url) == 0: break else: List.append(pic_url) t = t + num_per_page return s def recommend(url): Re = [] try: html = requests.get(url) except error.HTTPError as e: return else: html.encoding = 'utf-8' bsObj = BeautifulSoup(html.text, 'html.parser') div = bsObj.find('div', id='topRS') if div is not None: listA = div.findAll('a') for i in listA: if i is not None: Re.append(i.get_text()) return Re def dowmloadPicture(html, keyword ,file): global num # t =0 pic_url = re.findall('\"objURL\":\"(.*?)\",', html, re.S) # 先利用正则表达式找到图片url print('找到关键词:' + keyword + '的图片，即将开始下载图片...') for each in pic_url: print('正在下载第' + str(num + 1) + '张图片，图片地址:' + str(each)) try: if each is not None: pic = requests.get(each, timeout=7) else: continue except BaseException: print('错误，当前图片无法下载') continue else: string = file + '/' + keyword + '_' + str(num) + '.jpg' fp = open(string, 'wb') fp.write(pic.content) fp.close() num += 1 if num \u003e= numPicture: return if __name__ == '__main__': # 主函数入口 t = 0 word = input(\"请输入搜索关键词 （最好中文关键字）: \") height = input(\"输入照片的高度（若需要指定，直接回车跳过）： \") width = input(\"输入照片宽度 （若需要指定，直接回车跳过）: \") #url = 'http://image.baidu.com/search/flip?tn=baiduimage\u0026ie=utf-8\u0026word='+word+'\u0026pn='+str(t)+'\u0026gsm=50\u0026ct=\u0026ic=0\u0026lm=-1\u0026width='+width+'\u0026height='+height #Recommend = recommend(url) # 记录相关推荐 #tot = Find(url) #print('经过检测%s类图片共有%d张' % (word, tot)) numPicture = int(input('请输入想要下载的图片数量 ：')) file = input('请建立一个存储图片的文件夹，输入文件夹名称即可 ： ') y = os.path.exists(file) if y == 1: print('该文件已存在，请重新输入') file = input('请建立一个存储图片的文件夹，)输入文件夹名称即可') os.mkdir(file) else: os.mkdir(file) #tmp = url while t \u003c numPicture: try: #url = tmp #+ str(t) url = 'http://image.baidu.com/search/flip?tn=baiduimage\u0026ie=utf-8\u0026word='+word+'\u0026pn='+str(t)+'\u0026gsm=50\u0026ct=\u0026ic=0\u0026lm=-1\u0026width='+width+'\u0026height='+height result = requests.get(url, timeout=20) print(url) except error.HTTPError as e: print('网络错误，请调整网络后重试') t = t + num_per_page else: dowmloadPicture(result.text, word ,file) t = t + num_per_page print('当前搜索结束。') ''' print('相似关键词： ') for re in Recommend: print(re, end=' ') ''' ","title":"python3 爬取百度图片–指定分辨率","uri":"/contents/tools/robot/"},{"categories":["contents"],"content":"记录三维重建的流程和代码。\n特征提取和匹配 一些常见的特征提取和匹配方法很多，利用opencv集成的‘SIFT,SURF,ORB’等算法可以完成特征提取和匹配的流程。由于本文中提取的是电线的特征（如下图，需要重建的是中间的电线），不能用上面的方法，所以首先用传统算法提取图像中地面部分的特征点，匹配点算出相机外参数（相对的），由对极约束找出电线的匹配点。\n这里提供一个demo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 #include \u003ciostream\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; int main ( int argc, char** argv ) { if ( argc != 3 ) { cout\u003c\u003c\"usage: feature_extraction img1 img2\"\u003c\u003cendl; return 1; } //-- 读取图像 Mat img_1 = imread ( argv[1], CV_LOAD_IMAGE_COLOR ); Mat img_2 = imread ( argv[2], CV_LOAD_IMAGE_COLOR ); //-- 初始化 std::vector\u003cKeyPoint\u003e keypoints_1, keypoints_2; Mat descriptors_1, descriptors_2; Ptr\u003cFeatureDetector\u003e detector = ORB::create(); Ptr\u003cDescriptorExtractor\u003e descriptor = ORB::create(); // Ptr\u003cFeatureDetector\u003e detector = FeatureDetector::create(detector_name); // Ptr\u003cDescriptorExtractor\u003e descriptor = DescriptorExtractor::create(descriptor_name); Ptr\u003cDescriptorMatcher\u003e matcher = DescriptorMatcher::create ( \"BruteForce-Hamming\" ); //-- 第一步:检测 Oriented FAST 角点位置 detector-\u003edetect ( img_1,keypoints_1 ); detector-\u003edetect ( img_2,keypoints_2 ); //-- 第二步:根据角点位置计算 BRIEF 描述子 descriptor-\u003ecompute ( img_1, keypoints_1, descriptors_1 ); descriptor-\u003ecompute ( img_2, keypoints_2, descriptors_2 ); Mat outimg1; drawKeypoints( img_1, keypoints_1, outimg1, Scalar::all(-1), DrawMatchesFlags::DEFAULT ); imshow(\"ORB特征点\",outimg1); //-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离 vector\u003cDMatch\u003e matches; //BFMatcher matcher ( NORM_HAMMING ); matcher-\u003ematch ( descriptors_1, descriptors_2, matches ); //-- 第四步:匹配点对筛选 double min_dist=10000, max_dist=0; //找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离 for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { double dist = matches[i].distance; if ( dist \u003c min_dist ) min_dist = dist; if ( dist \u003e max_dist ) max_dist = dist; } // 仅供娱乐的写法 min_dist = min_element( matches.begin(), matches.end(), [](const DMatch\u0026 m1, const DMatch\u0026 m2) {return m1.distance\u003cm2.distance;} )-\u003edistance; max_dist = max_element( matches.begin(), matches.end(), [](const DMatch\u0026 m1, const DMatch\u0026 m2) {return m1.distance\u003cm2.distance;} )-\u003edistance; printf ( \"-- Max dist : %f \\n\", max_dist ); printf ( \"-- Min dist : %f \\n\", min_dist ); //当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限. std::vector\u003c DMatch \u003e good_matches; for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { if ( matches[i].distance \u003c= max ( 2*min_dist, 30.0 ) ) { good_matches.push_back ( matches[i] ); } } //-- 第五步:绘制匹配结果 Mat img_match; Mat img_goodmatch; drawMatches ( img_1, keypoints_1, img_2, keypoints_2, matches, img_match ); drawMatches ( img_1, keypoints_1, img_2, keypoints_2, good_matches, img_goodmatch ); imshow ( \"所有匹配点对\", img_match ); imshow ( \"优化后匹配点对\", img_goodmatch ); waitKey(0); return 0; } 由算出来的 $R,t$ 算出 $F$ 这里假设得到的$R,t$是世界坐标系下的参数，计算F需要先折算会相机坐标系下的相对参数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 void CaclulateFByRGlobal() { Eigen::Matrix3d Rl0; /*Rl0 \u003c\u003c 0.775071, -0.631781, -0.0108743, 0.0352285, 0.0603884, -0.997553, 0.630892, 0.772791, 0.069062;*/ //BA huzhou Rl0 \u003c\u003c -0.75596, -0.654554, 0.00914511, -0.0120758, -2.38882e-05, -0.999927, 0.654507, -0.756015, -0.0078862; //dalian Eigen::Vector3d cl0; /*cl0 \u003c\u003c 943.42, 1019.13, 54.7952;*/ //huzhou cl0 \u003c\u003c 974.676, 1021.41, 16.9945; //dalian Eigen::Matrix3d Rm0; /*Rm0 \u003c\u003c 0.785548, -0.618716, -0.010268, 0.0190466, 0.0407612, -0.998987, 0.618508, 0.784557, 0.0438043;*/ //huzhou Rm0 \u003c\u003c -0.740306, -0.672267, -0.00202373, 0.00930091, -0.00723215, -0.999931, 0.672205, -0.740274, 0.0116067; //dalian Eigen::Vector3d cm0; /*cm0 \u003c\u003c 947.823, 1016.18, 54.5612;*/ //huzhou cm0 \u003c\u003c 970.431, 1017.26, 15.493; //dalian Eigen::Matrix3d Rr0; /*Rr0 \u003c\u003c 0.79421, -0.60764, 0.00190083, 0.0225506, 0.0263483, -0.999398, 0.607224, 0.793775, 0.0346288; */ //huzhou Rr0 \u003c\u003c -0.762675, -0.646221, -0.0269323, 0.0197389, 0.0183655, -0.999636, 0.64648, -0.76293, -0.00125122; //dalian Eigen::Vector3d cr0; /*cr0 \u003c\u003c 951.094, 1014.74, 54.7585;*/ //huzhou cr0 \u003c\u003c 966.922, 1014.19, 17.0275; //dalian Eigen::Matrix3d Kl; /*Kl \u003c\u003c 3.7612906067774788e+003, 0., 2.0457995013785016e+003, 0., 3.7159736130516289e+003, 1.4796241830921265e+003, 0., 0., 1.;*/ //huzhou Kl \u003c\u003c 3.7343664904677757e+003, 0., 2.0458971603779855e+003, 0., 3.6840622549040932e+003, 1.5309521054590580e+003, 0., 0., 1.; //dalian Eigen::Matrix3d Km; /*Km \u003c\u003c 3.7516261809333914e+003, 0., 2.0321668349416393e+003, 0., 3.7075460080696898e+003, 1.4757973862050546e+003, 0., 0., 1.;*///huzhou Km \u003c\u003c 3.6807406030598072e+003, 0., 2.0575809009145160e+003, 0., 3.6316348410018245e+003, 1.5323338632609398e+003, 0., 0., 1.; //dalian Eigen::Matrix3d Kr; /*Kr \u003c\u003c 3.7604405383652875e+003, 0., 2.0415438075955501e+003, 0., 3.7160779331549415e+003, 1.4845370511494227e+003, 0., 0., 1.;*/ //huzhou Kr \u003c\u003c 3.6902009246169255e+003, 0., 2.0795207000036444e+003, 0., 3.6414589667424284e+003, 1.5276047177099535e+003, 0., 0., 1.; //dalian std::vector\u003cEigen::Matrix3d\u003e vec_rotation{ Rl0,Rm0,Rr0 }; std::vector\u003cEigen::Vector3d\u003e vec_center{ cl0,cm0,cr0 }; std::vector\u003cEigen::Matrix3d\u003e vec_k_inv{ Kl.inverse(),Km.inverse(),Kr.inverse() }; std::cout \u003c\u003c \"0: l0, 1: m0, 2 r0 \" \u003c\u003c std::endl; for (uint i = 0; i \u003c vec_rotation.size(); i++) { for (uint j = i + 1; j \u003c vec_rotation.size(); j++) { Eigen::Matrix3d R_relative = vec_rotation[j] * (vec_rotation[i].transpose()); Eigen::Vector3d t_relative = vec_rotation[j] * (vec_center[i] - vec_center[j]); Eigen::Matrix3d t_relative_AntisymmetricMatrix; t_relative_AntisymmetricMatrix \u003c\u003c 0, -t_relative[2], t_relative[1], t_relative[2], 0, -t_relative[0], -t_relative[1], t_relative[0], 0; //cout \u003c\u003c \"t_relative_AntisymmetricMatrix\" \u003c\u003c endl \u003c\u003c t_relative_AntisymmetricMatrix \u003c\u003c endl; Eigen::Matrix3d F = vec_k_inv[j].transpose() * t_relative_AntisymmetricMatrix * R_relative * vec_k_inv[i]; F /= F(2, 2); std::cout \u003c\u003c \"i: \" \u003c\u003c i \u003c\u003c \" , j: \" \u003c\u003c j \u003c\u003c \" F:\" \u003c\u003c std::endl \u003c\u003c F \u003c\u003c std::endl; std::cout \u003c\u003c \"c_relative: \" \u003c\u003c (-R_relative.transpose() * t_relative).transpose() \u003c\u003c std::endl; } } } 电线提取，匹配，重建 代码我放github了。\n","title":"“Demo for 3D Reconstruction”","uri":"/contents/slam-sfm/voltagelinereconstruction/"},{"categories":["contents"],"content":"c++ 面向对象编程的基本结构。\nmain.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #include \"line_extract.h\" string undistort_image_path = \"C:/Users/1/Desktop/test2SPmatch/001_l_Undistort.png\"; string final_line_path = \"C:/Users/1/Desktop/test2SPmatch/final_line_l_1.png\"; string add_path = \"C:/Users/1/Desktop/test2SPmatch/add_image_l_1.png\"; int main() { Mat undistort_image = imread(undistort_image_path); Mat gray_image; cvtColor(undistort_image, gray_image, CV_BGR2GRAY);//×ª»»³É»Ò¶ÈÍŒ //寻找线头 vector\u003cPoint2i\u003e init_point; Line_extract line; line.InitLineHead(gray_image, init_point); //提取电力线 Mat final_line = Mat::zeros(gray_image.rows, gray_image.cols, CV_8UC1); line.ExtractLine(gray_image, init_point, final_line); imwrite(final_line_path, final_line); //结果保存 Mat add_image; add_image = gray_image + final_line; imwrite(add_path, add_image); return 1; } line_extract.h 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #pragma once #include \u003ccstdio\u003e #include \u003cvector\u003e #include \u003ciostream\u003e #include \u003cfstream\u003e #include \u003ccstring\u003e #include \u003ccstdlib\u003e #include \u003ccmath\u003e #include \u003calgorithm\u003e #include \"opencv\\cv.h\" #include \"opencv2\\core\\core.hpp\" #include \"opencv2\\highgui\\highgui.hpp\" #include \"opencv2\\imgproc\\imgproc.hpp\" #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003copencv2/calib3d/calib3d.hpp\u003e using namespace std; using namespace cv; class Line_extract { public: Line_extract(); void InitLineHead(Mat gray_image, vector\u003cPoint2i\u003e \u0026 initpoint); void FindCenterPoint(Mat gray_image, Mat \u0026 center_point); void FindInitLineHead(Mat center_point, vector\u003cPoint2i\u003e \u0026 initpoint); void ExtractLine(Mat gray_image, vector\u003cPoint2i\u003e initpoint,Mat \u0026 final_line); void ExtractLineSingle(Mat gray_image, Point2i init_point_index,Mat \u0026 final_line); ~Line_extract(); }; Line_extract.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 ﻿#include \"line_extract.h\" #define outlier_threshold 3 //二值化的参数 #define init_point_row 10 //初始化的行数 #define vector_template_size 30 // 存储正确索引的模板长度 #define move_threshold 1 //卷积核左右移动的大小 ////可修改调整 #define template_size 7 //值模板大小 7*7 #define end_size 3 //中值模板大小 3*3 #define end_threshold 1 //中断模板的方差小于该阈值就退出 #define update_threshold 10 //每隔10格判断是否更新一次 值模板 #define value_threshold 5 //用 status_vector 的倒数 5 格来判断是否更新 值模板 Line_extract::Line_extract() { cout \u003c\u003c \"class create!\" \u003c\u003c endl; } Line_extract::~Line_extract() { cout \u003c\u003c \"class destory!\" \u003c\u003c endl; } void Line_extract::InitLineHead(Mat gray_image, vector\u003cPoint2i\u003e \u0026 init_point) {\tMat center_point = Mat::ones(gray_image.rows, gray_image.cols, CV_8UC1); // 提取电力线的质心 FindCenterPoint(gray_image, center_point); FindInitLineHead(center_point, init_point); } void Line_extract::FindCenterPoint(Mat gray_image, Mat \u0026 center_point) { Mat mask = Mat::zeros(gray_image.rows, gray_image.cols, CV_8UC1); Mat mask_process = Mat::zeros(gray_image.rows, gray_image.cols, CV_8UC1); for (int i = 0; i \u003c gray_image.rows; ++i) { for (int j = 0; j \u003c gray_image.cols - 1; ++j) { uchar value_1 = *(gray_image.data + i * gray_image.step[0] + j * gray_image.step[1]); uchar value_2 = *(gray_image.data + i * gray_image.step[0] + (j + 1)*gray_image.step[1]); //线头左侧为(i,j+1) if ((value_1 - value_2) \u003e value_threshold) { *(mask.data + i * mask.step[0] + (j + 1)*mask.step[1]) = 255; } //线头右侧为(i,j) else if ((value_2 - value_1) \u003e value_threshold) { *(mask.data + i * mask.step[0] + j * mask.step[1]) = 255; } } } int first_index = init_point_row, last_index = mask.cols - init_point_row; int first_count = 0, last_count = 0; //要求拍摄时，需要保留的电力线在图像最上边 for (int i = 1; i \u003c mask.rows; ++i) { for (int j = first_index; j \u003c last_index; ++j) { if (*(mask.data + i * mask.step[0] + j * mask.step[1]) == 255) { *(mask_process.data + i * mask_process.step[0] + j * mask_process.step[1]) = 255; last_count = j; if (first_count == 0) { first_count = j; } } } first_index = (first_count - init_point_row) \u003e 0 ? (first_count - init_point_row) : 0; first_count = 0; last_index = (last_count + init_point_row) \u003c mask.cols ? (last_count + init_point_row) : mask.cols; last_count = 0; } Mat dst = mask_process.clone(); //先腐蚀去除部分离群点，再膨胀，使六轴线连在一起 // 获得结构元 Mat se = getStructuringElement(MORPH_RECT, Size(1, 1)); Mat se1 = getStructuringElement(MORPH_RECT, Size(1, 1)); //dilate(dst, dst, se1); erode(dst, dst, se); //只保留电力线的“质心”位置的像素 vector\u003cint\u003e vec_first, vec_last; for (int i = 0; i \u003c dst.rows; ++i) { for (int j = 0; j \u003c dst.cols - 1; ++j) { if (*(dst.data + i * dst.step[0] + j * dst.step[1]) == 0 \u0026\u0026 *(dst.data + i * dst.step[0] + (j + 1)*dst.step[1]) == 255) { vec_first.push_back(j + 1); } if (*(dst.data + i * dst.step[0] + j * dst.step[1]) == 255 \u0026\u0026 *(dst.data + i * dst.step[0] + (j + 1)*dst.step[1]) == 0) { vec_last.push_back(j); } } if (vec_first.size() == vec_last.size()) { for (int k = 0; k \u003c vec_first.size(); ++k) { int index = ceil((vec_first[k] + vec_last[k]) / 2.f); *(center_point.data + i * center_point.step[0] + index * center_point.step[1]) = 255; } } vec_first.clear(); vec_last.clear(); } } void Line_extract::FindInitLineHead(Mat center_point, vector\u003cPoint2i\u003e \u0026 init_point) { // 初始化好点,不选择开始的几行 for (int i = 0; i \u003c center_point.cols; ++i) { if (*(center_point.data + init_point_row * center_point.step[0] + i * center_point.step[1]) == 255) { // 要排除离群点的情况 int winsize = init_point_row \u003c= outlier_threshold ? init_point_row : outlier_threshold; int point_num = 0; for (int j = init_point_row - winsize; j \u003c= init_point_row + winsize; ++j) { for (int k = i - winsize; k \u003c= i + winsize; ++k) { if (*(center_point.data + j * center_point.step[0] + k * center_point.step[1]) == 255) point_num++; } } if (point_num \u003e= outlier_threshold) init_point.push_back(Point2i(init_point_row, i)); } } } void Line_extract::ExtractLine(Mat gray_image, vector\u003cPoint2i\u003e initpoint, Mat \u0026 final_line) { for (auto init_point_index : initpoint) { ExtractLineSingle(gray_image, init_point_index, final_line); } } void Line_extract::ExtractLineSingle(Mat undistort_image, Point2i init_point_index, Mat \u0026 final_line) { vector\u003cint\u003e status_vector; // 首先根据线头，构建一个 7*7 的像素值模板，它代表了电力线的值特征 int template_radius = (template_size - 1) / 2; Mat image_template = Mat::zeros(template_size, template_size, CV_8UC1); image_template = undistort_image(Rect(init_point_index.y - template_radius, init_point_index.x - template_radius, template_size, template_size)); cout \u003c\u003c image_template \u003c\u003c endl; int sum_value = 0, med_value = 0, max_value = -1, min_value = 999; // 找出模板内元素的中值 for (int i = 0; i \u003c image_template.rows; ++i) { for (int j = 0; j \u003c image_template.cols; ++j) { if (*(image_template.data + i * image_template.step[0] + j * image_template.step[1]) \u003e max_value) max_value = *(image_template.data + i * image_template.step[0] + j * image_template.step[1]); if (*(image_template.data + i * image_template.step[0] + j * image_template.step[1]) \u003c min_value) min_value = *(image_template.data + i * image_template.step[0] + j * image_template.step[1]); } } med_value = (int)((max_value - min_value) / 2 + min_value); // 根据med_value构建卷积核 Mat template_mask = Mat::zeros(template_size, template_size, CV_8UC1); for (int i = 0; i \u003c image_template.rows; ++i) { for (int j = 0; j \u003c image_template.cols; ++j) { if (*(image_template.data + i * image_template.step[0] + j * image_template.step[1]) \u003c= med_value) { *(template_mask.data + i * template_mask.step[0] + j * template_mask.step[1]) = 1; } } } cout \u003c\u003c \"template_mask\" \u003c\u003c template_mask \u003c\u003c endl; vector\u003cPoint2i\u003e vector_template; int x_index = init_point_index.x; int y_index = init_point_index.y; //初始化第一个索引模板 for (int i = 0; i \u003c vector_template_size; ++i) { int min_x_index = 0, min_y_index = 0, min_sum_value = 9999; for (int bias = -1 * move_threshold; bias \u003c= move_threshold; ++bias) { int local_x_index = x_index + 1; int local_y_index = y_index + bias; int local_sum_value = 0; int tem_x = 0, tem_y = 0; // 当前值模板与值模板差值，再卷积核卷积，求出最小值对应的模板中心索引 for (int j = local_x_index - template_radius; j \u003c= local_x_index + template_radius; ++j) { for (int k = local_y_index - template_radius; k \u003c= local_y_index + template_radius; ++k) { int mask_value = *(template_mask.data + tem_x * template_mask.step[0] + tem_y * template_mask.step[1]); int global_tem_value = *(image_template.data + tem_x * image_template.step[0] + tem_y * image_template.step[1]); int local_tem_value = *(undistort_image.data + j * undistort_image.step[0] + k * undistort_image.step[1]); local_sum_value += abs(global_tem_value - local_tem_value)*mask_value; tem_y++; } tem_y = 0; tem_x++; } if (local_sum_value \u003c min_sum_value) { min_sum_value = local_sum_value; min_x_index = local_x_index; min_y_index = local_y_index; } } //存储到vector_template中\tvector_template.push_back(Point2i(min_x_index, min_y_index)); *(final_line.data + min_x_index * final_line.step[0] + min_y_index * final_line.step[1]) = 255; x_index = min_x_index; y_index = min_y_index; } //cout \u003c\u003c \" vector_template \" \u003c\u003c vector_template \u003c\u003c endl; Mat matrix_template = Mat(vector_template_size, 3, CV_64FC1); for (int j = 0; j \u003c vector_template_size; ++j) {\t*((double*)(matrix_template.data + j * matrix_template.step[0] + 0 * matrix_template.step[1])) = vector_template[j].x;// *((double*)(matrix_template.data + j * matrix_template.step[0] + 1 * matrix_template.step[1])) = vector_template[j].y;// *((double*)(matrix_template.data + j * matrix_template.step[0] + 2 * matrix_template.step[1])) = 1.0; } //cout \u003c\u003c \" matrix_template \" \u003c\u003c matrix_template \u003c\u003c endl; //初始化 V和D Mat mat_square = matrix_template.t()*matrix_template; Mat D, V; eigen(mat_square, D, V); cout \u003c\u003c \"原矩阵 A =\" \u003c\u003c endl \u003c\u003c mat_square \u003c\u003c endl; cout \u003c\u003c \"D=\" \u003c\u003c endl \u003c\u003c D \u003c\u003c endl; cout \u003c\u003c \"V=\" \u003c\u003c endl \u003c\u003c V \u003c\u003c endl; // 根据后续数据，更新当前的索引模板，判断遇到环的条件，并特殊处理 for (int i = init_point_row + vector_template_size + 1; i \u003c 2900; ++i) { int min_x_index = 0, min_y_index = 0, min_sum_value = 9999; for (int bias = -1 * move_threshold; bias \u003c= move_threshold; ++bias) { int local_x_index = x_index + 1; int local_y_index = y_index + bias; int local_sum_value = 0; int tem_x = 0, tem_y = 0; // 当前值模板与值模板差值，再卷积核卷积，求出最小值对应的模板中心索引 for (int j = local_x_index - template_radius; j \u003c= local_x_index + template_radius; ++j) { for (int k = local_y_index - template_radius; k \u003c= local_y_index + template_radius; ++k) { int mask_value = *(template_mask.data + tem_x * template_mask.step[0] + tem_y * template_mask.step[1]); int global_tem_value = *(image_template.data + tem_x * image_template.step[0] + tem_y * image_template.step[1]); int local_tem_value = *(undistort_image.data + j * undistort_image.step[0] + k * undistort_image.step[1]); local_sum_value += abs(global_tem_value - local_tem_value)*mask_value; tem_y++; } tem_y = 0; tem_x++; } if (local_sum_value \u003c min_sum_value) { min_sum_value = local_sum_value; min_x_index = local_x_index; min_y_index = local_y_index; } } /*cout \u003c\u003c \"当前的x,y为：\" \u003c\u003c min_y_index \u003c\u003c \" ,\" \u003c\u003c min_x_index \u003c\u003c endl;*/ // 由索引模板，判断是否遇到环\tint pre_pre_y_index = vector_template[vector_template_size - 2].y; int pre_y_index = vector_template[vector_template_size - 1].y; double min_eigenvalues = 1e20; int min_index = -1; for (int j = 0; j \u003c D.rows; ++j) { if (*((double*)(D.data + j * D.step[0] + 0 * D.step[1])) \u003c= min_eigenvalues) { min_index = j; min_eigenvalues = *((double*)(D.data + j * D.step[0] + 0 * D.step[1])); } } double min_eigen_vector_value1 = *((double*)(V.data + min_index * V.step[0] + 0 * V.step[1])); double min_eigen_vector_value2 = *((double*)(V.data + min_index * V.step[0] + 1 * V.step[1])); double min_eigen_vector_value3 = *((double*)(V.data + min_index * V.step[0] + 2 * V.step[1])); double value1 = abs((min_eigen_vector_value1*min_x_index + min_eigen_vector_value2 * min_y_index + min_eigen_vector_value3 * 1) / sqrt(pow(min_eigen_vector_value1, 2) + pow(min_eigen_vector_value2, 2))); //正常 if (value1 \u003c 1) { status_vector.push_back(-1); //cout \u003c\u003c value1 \u003c\u003c endl; // 更新索引模板，将正确索引写入final_line中 for (int j = 0; j \u003c vector_template_size - 1; ++j) { vector_template[j] = vector_template[j + 1]; } vector_template[vector_template_size - 1] = Point2i(min_x_index, min_y_index); *(final_line.data + min_x_index * final_line.step[0] + min_y_index * final_line.step[1]) = 255; x_index = min_x_index; y_index = min_y_index; } //修正 else { status_vector.push_back(1); int value2 = static_cast\u003cint\u003e(-(min_eigen_vector_value1*min_x_index + min_eigen_vector_value3) / min_eigen_vector_value2); if (value2 \u003e= template_radius \u0026\u0026 value2 \u003c undistort_image.cols - template_radius) { // 更新索引模板，将正确索引写入final_line中 for (int j = 0; j \u003c vector_template_size - 1; ++j) { vector_template[j] = vector_template[j + 1]; } vector_template[vector_template_size - 1] = Point2i(min_x_index, value2); *(final_line.data + min_x_index * final_line.step[0] + value2 * final_line.step[1]) = 255; x_index = min_x_index; y_index = value2; } } if ((i - (init_point_row + vector_template_size + 1)) % update_threshold == (update_threshold - 1)) { int status_sum = 0; for (int j = 0; j \u003c value_threshold; ++j) { status_sum += abs(status_vector[i - (init_point_row + vector_template_size + 1) - j] - status_vector[i - (init_point_row + vector_template_size + 1) - j - 1]); } if (status_sum \u003e 0) { image_template.empty(); image_template = undistort_image(Rect(y_index - template_radius, x_index - template_radius, template_size, template_size)); } } // 更新 V和D Mat matrix_template = Mat(vector_template_size, 3, CV_64FC1); for (int j = 0; j \u003c vector_template_size; ++j) { *((double*)(matrix_template.data + j * matrix_template.step[0] + 0 * matrix_template.step[1])) = vector_template[j].x; *((double*)(matrix_template.data + j * matrix_template.step[0] + 1 * matrix_template.step[1])) = vector_template[j].y; *((double*)(matrix_template.data + j * matrix_template.step[0] + 2 * matrix_template.step[1])) = 1.0; } mat_square.empty(); mat_square = matrix_template.t()*matrix_template; D.empty(); V.empty(); eigen(mat_square, D, V); // 是否满足终止条件 int end_radius = (end_size - 1) / 2; int outliner_count = 0; Mat end_mean, end_std; meanStdDev(undistort_image(Rect(y_index - end_radius, x_index - end_radius, end_size, end_size)), end_mean, end_std); if (end_std.at\u003cdouble\u003e(0, 0) \u003c= end_threshold) break; } } ","title":"C++面向对象例子","uri":"/contents/c++/c++%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E4%BE%8B%E5%AD%90/"},{"categories":["contents"],"content":"部分摘自泡泡机器人公开课，原文链接\n算法流程 2D 深度图像转换为3D点云并求得每一点的法向量 $$ \\mathrm{D}_{k}(\\mathbf{u})=\\frac{1}{W_{\\mathbf{p}}} \\sum_{\\mathbf{q} \\in \\mathbb{U}} \\mathscr{N}_{\\sigma_{s}}\\left(||\\mathbf{u}-\\mathbf{q}||_{2}\\right) \\mathscr{N}_{\\sigma_{r}}\\left(\\left||\\mathbf{R}_{k}(\\mathbf{u})-\\mathrm{R}_{k}(\\mathbf{q})\\right||_{2}\\right) \\mathrm{R}_{k}(\\mathbf{q}) $$\n$$ \\mathbf{V}_{k}(\\mathbf{u})=\\mathrm{D}_{k}(\\mathbf{u}) \\mathbf{K}^{-1} \\mathbf{\\dot { u }} $$\n$$ \\mathbf{N}_{k}(\\mathbf{u})=v\\left[\\left(\\mathbf{V}_{k}(u+1, v)-\\mathbf{V}_{k}(u, v)\\right) \\times\\left(\\mathbf{V}_{k}(u, v+1)-\\mathbf{V}_{k}(u, v)\\right)\\right] $$\n上面第一条公式表示对深度图进行双边滤波操作。第二条公式表示将滤波后的深度图转化为点云。第三条公式为深度图中每个点计算法向量。\nICP算法迭代求当前帧相机位姿 对点云分层抽样并且按照coarse-to-fine的方式匹配。\n当然在算法实现的时候，ICP这个操作的计算误差放在GPU上做，一个线程格处理一个像素，每个线程块先单独做误差累积放在显存中，然后这些误差再累加送回CPU内存，最后调整相机位姿。\n根据相机位姿将点云数据融合到全局模型中 这里利用到TSDF（截断符号距离函数）模型。TSDF模型中网格的数值代表重建场景表面的距离，所以TSDF模型中数据由正到负过渡，数值为零的点表示重建的表面。\n对于每一帧点云，算法需要在GPU中维持TSDF数据，如上图右侧，每一个线程处理一个“蓝条的网格”。首先将，TSDF单元由全局坐标系，由ICP得到的外参数转换到相机坐标系，再由相机内参得到像素坐标系下的值，如果这个像素值在相机视场内，就更新TSDF数据。\n更新规则：\n如上图左边的更新公式：\n相机光心的全局坐标减去TSDF单元的全局坐标表示单元到光心的距离，再减去深度值表征第i个像素的sfd值，如果这个值接近0，说明该值应该更新到模型的表面。下面的截断公式防止离表面过远的单元有太大的更新值。最后将更新的TSDF数据与上一帧的TSDF数据加权融合得到当前TSDF模型。\n利用光线投影算法求得当前视角下的场景表面点云 光线投影假设一束从相机光心中投射出来的射线穿过当前的TSDF模型，所有交点的值是由正到负变化的，值为0的位置即为该射线与相机平面的交点对应像素值所对应的点云坐标。这样就可以利用当前TSDF模型做模型投影得到深度图，该深度图用于和下一帧输入的深度图做配准。这样利用帧和模型投影计算的位姿比利用帧和帧之间的方法要准确。\nTSDF网格模型的优劣点 优点：\n网格模型隐含重建好的表面，加权融合消除误差。\n便于做并行计算\n缺点：\n非常消耗显存，显存使用率和重建场景体积成正比。\n回环，位姿优化做得差。\n","title":"KinectFusion框架","uri":"/contents/slam-sfm/kinectfusion/"},{"categories":["contents"],"content":"SurfelWarp是类似于DynamicFusion的动态重建系统。 与其他动态重建方法相比，surfelwarp使用平坦的surfel数组（而不是体积场）作为几何图形表示，这使管道更鲁棒有效。\n摘要 我们贡献了一个密集的SLAM系统，该系统需要实时深度图像流作为输入，并实时重建非刚性变形场景，而无需模板或者先验模型。与现有方法相反，我们不维护-保留任何体积数据结构，例如截断符号距离函数（TSDF）字段或变形字段，它们是性能和内存密集型的。我们的系统有效使用可以直接从商品深度传感器获取的基于平面（surfel）的几何图形表示。标准图形管线和通用GPU（GPGPU）计算可用于所有中央操作：即最近邻维护，非刚性变形场估计 以及深度测量的融合。我们的管道本质上避免了如立方体匹配，体积融合和致密变形场更新等昂贵的体积操作，以至于明显提高性能。此外，显式灵活的基于surfel的几何图形表示可实现高效处理拓扑更改和跟踪故障，这使得我们的重建与最新的深度观测结果一致。我们的系统允许机器人使用非刚性变形的物体，从而可能与动态的工作环境交互。\n基于体积和基于表面重建的优劣势 基于体积的方法能生成更平滑的三角网格，但这类方法是性能和内存密集型的。基于表面的重建方法更具效率，但如果需要生成网格模型，则需要后期处理。\n系统框架 如下图： 系统逐帧处理输入深度图像，在接收到新的深度图像时，首先通过与迭代最近点类似的方法将当前的深度观察值与参考几何(reference geometry)模型对齐，得到更新后的形变场，再与全局模型做数据融合，最后更新参考几何模型。\n","title":"SurfelWarp: Efficient Non-Volumetric Single View Dynamic Reconstruction","uri":"/contents/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/surfelwarp/"},{"categories":["contents"],"content":"从五月份开始搭建博客，原博客有hexo驱动，主题是NexT,部署到github page和coding page上，现在仍可以访问(但没必要…)。\n现在本博客自豪地由hugo驱动，主题是MemE,部署在gitee page上。相较于原博客，访问速度更快，界面更简洁。去除了花里胡哨的相册功能，当然花里胡哨的歌单仍然保留下来。\n本博客食用方法：\n最上面妖娆的顶栏中间的jiajie是一个返回键，切换到博客主界面。 然后没了～\n希望新的一年，少说骚话，多写骚话。\n","title":"Hello","uri":"/contents/broken-thoughts/hello/"},{"categories":["contents"],"content":"开头BB 24岁，2020，本命年，除了要在王者荣耀买一波本命年皮肤(偏题了…)，更希望在这一年切勿荒废时光(游戏除外)，所以觉得有必要列一个规划表，以我对自己博客臭美的性子，估计会每天都翻到这篇文章，这样就达到了提醒的作用了～\n正经地说，这篇文章希望能起到这样的功能：\n充分利用碎片化时间 关注的公众号很多，但平时看文章的频率极低，在这里会贴出我平时(相对)经常浏览的链接，这样就可以从一篇文章找到所有链接的入口。\n主线\u0026\u0026支线任务进度概览 以往总存在想学的东西太多，结果都想学一学，但都学不精通的情况。2020年，把需要掌握的技能分为主线任务和支线任务，主线任务着重学习，支线任务先学习列出的我认为优先级最高的几个。\n阅读书单 哪些书值得一读，大佬的书单往往有借鉴作用，今年跟着大佬阮一峰读书，把重点推荐的书看一遍。把看过的书更新到本文最底下。\n正文 平时阅读关注 计算机视觉文章列表\n专知\n五分钟学算法\n泡泡机器人\n摄影文章清单\nWelens\n主线：人脸人体重建 Face \u0026\u0026 Human Body Reconstruction using DL 三大会议的最新进展，paperswithcode里面的state of the art。\nFusion 深入研究DynamicFusion的算法原理，源码。\n多视图几何，计算机视觉中的算法与应用 把需要掌握的部分总结成文。\n支线学习 Python工具集 machine_learning_beginner ML Coursera-ML-AndrewNg-Notes DL deeplearning_ai_books SLAM十四讲 把第二版的再看一遍，写好总结。不做SLAM，把数学工具部分学号，再把SLAM系统了解即可。\nc++ primer 当工具书，在阅读DynamicFusion遇到困难时，查阅本书并且写总结。\ncmake 只是一本小书，花一天时间把它看完并写总结。\n2020 我的阅读书单","title":"2020 To-do-list","uri":"/contents/day-day-up/2020-todolist/"},{"categories":["contents"],"content":"使用 share.js为MemE主题添加社交分享功能。\n首先 拷贝 share.js仓库到本地 git clone https://github.com/overtrue/share.js.git 分别把share.js/dist/目录下的css,fonts,js拷贝到static目录下的css,fonts,js中。\n在博客根目录创建partials/share.html，添加内容：\n\u003cdiv class=\"social-share\" data-initialized=\"true\" data-wechat-qrcode-title=\"扫一扫分享到微信\"\u003e \u003ccenter\u003e \u003cfont style=\"font-size:18px;color:darkcyan;\"\u003e分享到：\u003c/font\u003e \u003ca href=\" \" class=\"social-share-icon icon-weibo\"\u003e\u003c/a \u003e \u003ca href=\"#\" class=\"social-share-icon icon-wechat\"\u003e\u003c/a \u003e \u003ca href=\"#\" class=\"social-share-icon icon-twitter\"\u003e\u003c/a \u003e \u003ca href=\"#\" class=\"social-share-icon icon-linkedin\"\u003e\u003c/a \u003e \u003ca href=\"#\" class=\"social-share-icon icon-facebook\"\u003e\u003c/a \u003e \u003ca href=\"#\" class=\"social-share-icon icon-qq\"\u003e\u003c/a \u003e \u003ca href=\"#\" class=\"social-share-icon icon-qzone\"\u003e\u003c/a \u003e \u003c/center\u003e \u003c/div\u003e \u003c!-- css \u0026 js --\u003e \u003clink rel=\"stylesheet\" href=\"{{ \"css/share.min.css\" | absURL }}\" /\u003e \u003cscript src=\"https://hugo-picture.oss-cn-beijing.aliyuncs.com/social-share.min.js\"\u003e\u003c/script\u003e 在博客根目录创建partials/custom/footer.html，添加内容： \u003cdiv class=\"container\" role=\"main\" itemscope itemtype=\"http://schema.org/Article\"\u003e \u003cdiv class=\"row\"\u003e \u003cdiv class=\"col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1\"\u003e \u003carticle role=\"main\" class=\"blog-post\" itemprop=\"articleBody\" id=\"content\"\u003e {{ if (.Params.share) }} {{ partial \"share.html\" }} {{ end }} \u003c/article\u003e 最后，在你想开启分享功能的.md文章里面，添加：\nshare: true 即可。\n","title":"hugo 博客添加分享功能","uri":"/contents/tools/add-share.js/"},{"categories":["contents"],"content":" 诺拉·琼斯（Norah Jones）（出生于Geetali诺拉·琼斯·香卡（Geetali Norah Jones Shankar）； 1979年3月30日）是美国歌手，词曲作者，音乐家和演员。她是西塔琴演奏家Ravi Shankar和Sue Jones的女儿。她是Anoushka Shankar的同父异母姐姐。\n2002年，她发行了商业上成功且广受好评的专辑Come Comes with Me，开始了她的个人音乐事业，该专辑融合了乡村音乐和流行音乐和爵士元素，并获得了钻石唱片的认可，销量超过2600万张。该唱片获得了琼斯五项格莱美奖，包括年度专辑，年度唱片和最佳新歌手。她随后的录音室专辑《 Feels Like Home》于2004年发行； 《不是太晚》于2007年发行，同年，她在《我的蓝莓之夜》中首次亮相。 2009年的《秋天》（The Fall）均获得白金级地位，每本销量超过一百万册，并受到评论家的普遍好评。琼斯的第五张录音室专辑《小伤心》于2012年4月27日发行。\n琼斯获得了9项格莱美奖，并且在Billboard杂志2000-2009十年排行榜的艺术家中排名第60。在整个职业生涯中，琼斯赢得了无数奖项，并在全球范围内售出了超过5000万张专辑。 Billboard将她评为2000-2009十年间顶级爵士音乐人。\n1986年父母分居后，琼斯在德克萨斯州格雷普韦恩（Grapevine）和母亲度过了童年。她曾就读于Colleyville中学和Grapevine高中，然后转到达拉斯的布克·T·华盛顿表演与视觉艺术高中。中学时，琼斯在学校合唱团中唱歌，参加乐队并演奏中音萨克斯风。在她父母的同意下，她16岁时正式将她的名字改名为诺拉·琼斯（Norah Jones）。\n琼斯开始在教堂唱歌，还从小上钢琴和语音课。她仍然参加教堂。她认为自己是精神上欣赏她教堂的仪式，但并不认为自己具有深厚的宗教信仰。\n夏季，她参加了Interlochen艺术中心。在高中时，她获得了DownBeat学生音乐奖的最佳爵士歌手奖（分别是1996年和1997年的两次）和最佳原创作品奖（1996年）。\n琼斯就读于北德克萨斯大学（UNT），主修爵士钢琴，并与UNT爵士歌手一起唱歌。在这段时间里，她有机会与未来的合作者Jesse Harris会面。她组织着一支乐队在大学里玩，乐队的成员碰巧是哈里斯的朋友。他正在与朋友和未来的小威利成员理查德·朱利安（Richard Julian）进行越野旅行，并停下来观看乐队的演奏。与琼斯见面后，哈里斯开始发送她的歌曲主唱。 1999年，她离开纽约。不到一年后，她与哈里斯（Harris）成立了一支乐队，这将证明是她事业的起步枪。\n凭借成功的独奏生涯，琼斯于2003年与理查德·朱利安（Richard Julian）进行人声，吉姆·坎皮隆哥（Jim Campilongo）于吉他，李·亚历山大（Lee Alexander）和鼓手丹·里瑟（Dan Rieser）一起组建了《小威利》。另一个乡村乐队在2006年发行了同名首张专辑，在2012年发行了《美好时光》。\n在2008年，琼斯与其他朋友和布鲁克林尼特，萨莎·多布森和凯瑟琳·波普组成了另一个乡村乐队。 Blue Note Records于2014年7月15日发行了他们的首张全长专辑《 No Fools，No Fun》。\n琼斯的独奏专辑包括《与我同行》（2002），《与我同行》（2003），《像家》（2004），《不嫌晚》（2007），《秋天》（2009），《小伤心》（2012）和《与比利永远在一起》乔·阿姆斯特朗（2013）。\n截至2015年，尽管诺拉·琼斯（Norah Jones）取得了非凡的成就，但她仍然拥有良好的基础，她谦虚，务实的天性以及出色的音乐才能继续为世界各地数百万歌迷所喜爱。\n","title":"Norah Jones","uri":"/contents/jazz/norah-jones/"},{"categories":["contents"],"content":"科技英语写作和学术交际英语要点。\nParts of Speech 词类 冠词 1.速度是涉及大小和方向的一个矢量。\nVelocity (Speed) is a vector quantity involving both magnitude and direction.\n2.速度等于距离与时间之比。\nVelocity (Speed) is equal to the ratio of distance to time.\n3.如果在电阻器上加上电压，电阻器中就会有电流流动。\nIf a voltage is applied across a resistor, an electric current will flow in the resistor.\n4.电功率的单位是瓦特。\nThe unit of electrical power is the watt.\n5.电功率是用瓦特测量的。\nElectrical power is measured in watts.\n6.光是由波还是微粒构成的？\nconsist of//be made up of//be composed of\nDoes light consist of waves or particles?\nIs light made up of/composed of waves or particles?\n7.应当对计算机科学有一个充分的了解。\nA good knowledge/understanding of computer science is necessary.\nShe has a good command of English grammar.\n8.本文对A与B进行了详细的比较。\ncompare A and B\nMake a comparison of A and B\nThis paper makes a detailed comparison of A and B.\nA detailed comparison of A and B (between A and B) is made (in the paper).\n9.对该电路做定量分析很难。\nquality—quantity\nqualitative—quantitative\nmake an analysis of…\nIt is very difficult to make a quantitative analysis of the circuit.\n10.为阻止地球转动，得给赤道施加一个多大的力？\nHow great a force would have to be applied to the equator in order to stop the earth from rotating?\n11.输入信号太大会引起非线形失真。\nnonlinear distortion\na large signal\nToo large an input signal causes nonlinear distortion.\n12.末速度是平均速度的一半/三倍 。\nThe final velocity is half the average velocity.\nThe final velocity is three times the average velocity.\n这两台设备的质量都很好。\nBoth the devices are good in quality.\nThe device is good in quality.\nThe device is low in cost.\nThe device is light in weight.\nThe device is small in size.\nThe device is simple in operation.\nThe computer is high in speed and accurate in operation.\n13.这里应该使用一根S形管。\nS-shaped pipe\nAn S-shaped pipe should be used here.\n磁铁具有一个S极和一个N极。\nmagnet/magnetic/electric field\nelectromagnetic wave\nA magnet has an S pole and an N pole.\nConjunctions 连词 1.现在假设电场具有矢量E的方向，但这个场并不均匀。（宾语从句并列）\nNow let’s assume/suppose that the electric field has the direction of the vector E but that it is not uniform.\n2.在这种情况下，每个部件基部的压力是相同的，所以该系统处于平衡状态。\nIn this case, the pressure at the base of each component is the same, therefore the system is in equilibrium.\nNumerals 量词 1.该电容器上的电压为零点零几伏。\nthe voltage across the capacitor a few hundredths of a volt (P 18)\n十年前该厂的产量仅为现在的五分之一。\noutput\none fifth what it is now\none fifth the present output\n2.施加在该物体上的力为原来的五分之二。\nexerted on/applied to two fifths what it was\n这两个数值之差为万分之十三。\nvalue difference 13 parts in $10^4$\n3.这根导线比那根粗三倍。\nwire thick\n3 or 4 times as thick as…?\n3 or 4 times thicker than…? (P 19)\n该信号被放大了十倍。\namplify\n11 times / 11 fold / by a factor of 11 (P 19,23)\nPrepositions 介词 1.这本书对电子工程师来说是极有帮助的。\n…is very helpful to…\n…is of great help to…\nThis definition is of great importance /significance.\n2.电容是以法拉为单位来度量的。\nCapacitance is measured in farads.\nResistance is measured in ohms.\nP 25-26\n3.在把这些数值代入该方程后，人们发现v等于光速。 (P 28)\nsubstitute …into…\nOn/Upon substitution of these values into the equation, it is found that v equals (is equal to) the velocity of light.\n4.这是解方程的一个充分必要条件。\nThis is a sufficient and necessary condition for solving the equation.\n若x \u003e 1,则该方程无解。 (P 28)\nFor x \u003e 1, there is no solution to the equation.\n5.在研究电学时，物理学家们定义了电场强度。\nWhen they studied electricity, physicists defined electric field intensity.\nIn their study of electricity, physicists defined electric field intensity.\nVerbs 动词 1.这个系数有待确定。 (P 43)\nThis coefficient remains to be determined.\nThis problem remains to be solved.\n2.输入成为负的，而输出保持不变。 (P 43)\npositive negative\nThe input turns/goes negative while/whereas the output remains/stays constant/unchanged/unaltered/fixed/the same.\n3.做试验时，越细心越好。 这一点再怎么强调都不过分。\nIn doing/conducting an experiment, one cannot be too careful.\nThis point cannot be overemphasized.\nAdverbs 副词 1.下面两个等式 (P 43)\nthe two equations below (cf. above)\n相距10厘米的两个金属球 (P 43)\ntwo metal balls 10 cm apart\n2.相距一段距离d的两根平行导线 (P 43)\ntwo parallel wires a distance of d apart\n2千米外的计算中心 (P 43)\n3.这个系数的典型值为0.35。 (P 43)\nThe typical value of this coefficient is 0.35.\nthe computing center 2 km away\n4.该设备主要由三部分组成。\nThe device consists of three parts. The device consists mainly of three parts.\n这一现象基本上是由…引起的。\nThis phenomenon is due to … This phenomenon is due largely to …\nAdjectives 形容词 1.现有的教科书 (P 43)\nall the textbooks available\n电子是存在于普通物质中的最小微粒。\nElectrons are the smallest particles present in ordinary substances.\n2.该力垂直作用于桌面。 (P 43)\nThe force acts perpendicular to the surface of the table.\n该电荷正平行于电场运动。\nThe charge is moving parallel to the electric field.\n3.X必须大于或等于1.\nX must be greater than or equal to 1.\nX必须小于或等于1.\nX must be less than or equal to 1. P 42\n4.计算机由于运算准确、速度快而得到了广泛的应用。 (P 43)\nComputers have been widely used/applied because they are accurate in operation and high in speed.\nPronouns 代词 1.从把它定义为距离与时间之比来看，速度是以米每秒来表示的。\nFrom its definition as the ratio of distance to time, speed is expressed in meters per second.\nNouns 名词 1.这种新材料耐腐蚀。\nbe resistant to corrosion\nThis new material is corrosion resistant.\n该器件是防水的。\nbe resistant to water\nThis device is water resistant.\n这些材料是压敏的。\nbe sensitive to pressure ……pressure sensitive.\n这个设备是无线电控制的。\nbe controlled by radio\nThis device is radio controlled.\n程控设备\nequipment controlled by a program\nprogram controlled equipment\n2.这些工厂生产指甲盖大小的集成电路。\nThese factories produce/manufacture integrated circuits the size of a finger-nail.\n我们将会制造出手表大小的计算机。\n…computers the size of a watch.\n3.一根丝线 (的百分之一那样) 粗细的导线 (P 57)\na wire the diameter of a silk thread\na wire one hundredth the diameter of a silk thread\n4.该曲线显示了A随B变化的情况。\nThis curve shows the variation of A with B.\n5.交流电可以转变为直流电，这一过程称为整流。 (P 57)\nAC can be changed/transformed/converted into DC, (which is) a process called rectification.\n磁铁能吸引铁质材料，这是大家都熟悉的现象。(P 57)\nA magnet attracts iron materials, (which is) a phenomenon familiar to all.\n激光是20世纪60年代发展起来的一项技术，它能穿透像金刚石这样最坚硬的物质。\nA new technology introduced in the 1960s, laser can pierce the hardest substance such as diamond.\nParenthesis, Negation, Tense, Voice and Comparative degree of adj. \u0026 adv. Parenthesis 插入语 1.前面已经学过，速度是个矢量。 (P 73)\nVelocity, it has been learned before, is a vector quantity.\n2.作者希望，本研究将有助于增强外语学习者的文化意识。\nThis study, it is hoped, will help enhance the cultural awareness of foreign language learners.\nNegation (full \u0026 partial) 全/部分否定 1.部分否定\n并非全部数据都要核对。\nNot all the data are to be checked.\nAll the data are not to be checked.\n我们实验室里的计算机的质量并非都很好。\nNot all the computers in our laboratory are very good in quality.\nAll … are not very good in quality.\n2.全否定\n这两个力均不能离开对方而存在。\nNeither force (Neither of the forces) can exist without the other.\n这些窗户均承受不了这么大的力。 (P 74)\nNone of these windows can withstand so large a force/such a large force.\n现有的教科书都没有提到这一点。(P 73)\nNo textbooks available …\nNone of the textbooks available …\nTense 被动 1.前一章讨论了变压器的工作原理。\nThe previous/preceding chapter discussed/ has discussed the working principle of the transformer.\n… was discussed/has been discussed in …\n2.据估计下个月到该设备运抵时，技术人员对其工作原理已研究过了。\nexpect, arrive, study\nIt is expected that the technical staff/personnel there will have studied its principle of operation by the time the device arrives at that research institute next month.\nVoice 强调 1.该物体正受到两个(大小相等、方向相反的)力的作用。\nTwo forces are acting on the body.\nThe body is being acted on by two forces (which are) equal in magnitude and opposite in direction.\n2.这种电池的极性必须加以注意。\nWe must pay attention to the polarity of the battery.\nAttention must be paid to the polarity of the battery.\nThe polarity of the battery must be paid attention to.\nComparative degree 1.这台仪器实际上比图2所示的要稍微复杂一些。\nA is slightly/a little more complex than B.\n2.A比B复杂得多。\nA is much/far/a great deal more complex than B.\n3.笔记本电脑的尺寸越来越小，但功能越来越复杂。\never smaller\nincreasingly sophisticated\n4.早期的计算机有几个房间那么大。\nAn early computer was as large as several rooms.\n小至0.1 A的电流不能产生足够的热。 (P 74)\nThe current as small as 0.1 A cannot produce/generate enough heat.\n5.似乎有多少设计者就有多少种设计方法。\nIt seems that there are as many designer as there are design method.\n世界上有多少人，就有多少种指纹。\nThere are as many individual as there are fingerprint.\n6.物体被举得越高，它所具有得势能就越大。\nIf a body is lifted higher, it possesses greater potential energy.\nThe higher a body is lifted, the greater potential energy it possesses.\n7.Java与其说是一种编程语言，不如说是一种技术。\nJava is more a technology than a programming language.\nNon-finite Verbs 非限定动词 Infinitive 不定式 1.雷达的发明使人们能在夜间探测到飞机。\nThe invention of radar enables people to detect airplanes at night.\nThe invention of radar makes it possible to detect airplanes at night.\n2.借助于计算机设计一种新型飞机只需几个月的时间。 (P 78)\nIt takes some time to do sth.\nIt takes only a few months to design a new type of airplane…\n3.目标越远，回波返回所需的时间就越长。\nThe farther away the target (is), the longer time it takes for the echo to return.\n4.该电路的输出正是待发送的信号。\nThe output of the circuit is just the signal to be transmitted.\n要执行的程序储存在这一单元中。 (p. 78)\nThe program to be executed is stored in this unit.\n5.存储单元是存储信息的地方。\nA memory unit is a place to store information in. (= in which we can store information)\n这是画画所用的笔。 (p. 79)\nThis is a pen to draw pictures with.\nThis is a pen with which to draw pictures.\n6.为了使这种转换可逆，就必须保持平衡。\nFor the conversion to be reversible, equilibrium must be maintained.\n为使该级数收敛，x必须小于1。\nFor this series to converge, x must be less than 1.\n7.数字计算机高速进行精确计算的能力已使人们能够将其用作控制系统的部件。\nThe ability of digital computers to make precise calculations at high speeds has made it possible for them to be used as parts of control systems.\n8.重要的是如何确定发射天线的模式。\nHow to determine the transmitting antenna pattern is important.\nIt is important how the transmitting antenna pattern is determined.\nParticiple (“-ing” \u0026 “-ed”) 1.若已知A和B，我们就能求出C。\nIf we know A and B, we can find out/determine C.\nKnowing A and B, we can find out/determine C.\n2.由于具有这两种性质，金属可被制成所需的形状。\nSince they have/possess these two properties, metals can be made into the shapes required.\n3.电路中流动的电流等于电压除以电阻。\nThe current flowing in the circuit is equal to the voltage divided by the resistance.\nGerund (-ing) 1.值得注意的是…\nIt deserves attention that…\nIt deserves to be noted that…\nIt is worth noting that…\nbe worth doing = deserve to be done\nThis book is worth reading. = … deserves to be read.\nThis place is worth visiting. = …deserves to be visited.\n2.求出这些符号的可能值就称为解方程。\nFinding the possible values of these symbols is called solving the equation.\n把A代入B，就得到以下表达式。\nSubstituting A into B yields the following expression. Substituting A into B, we will get the following expression.\n3.这涉及到要计算分子的总能量。\nThis involves calculating the total energy of the molecules.\ninvolve doing\n(avoid, consider, involve, facilitate, finish, suggest, practice, resist, ect. + doing)\n4.一经挤压，气体的体积就会缩小。\nOn being compressed, a gas will reduce its volume.\nWith structure 1.当放大器截止时，增益为…\nWhen the amplifier is off, the gain is…\n(With) the amplifier off, the gain is… (adverbial of time)\n2.由于存在摩擦，一部分功率以热的形式损失掉了。\nSince friction is present, a part of power has been lost as heat.\n(With) friction present, a part of power has been lost as heat. (adverbial of reason)\n3.本文既讨论了理论问题，又讨论了实际的设计方法，重点在后者。\nThis paper discusses both A and B. Emphasis is put on the latter.\nThis paper discusses/deals with both theoretical issues and practical design methods with emphasis on the latter. (adverbial of further statement)\n4.让我们以原点为圆心画一个半径为R的圆。 (p. 89)\nLet’s draw a circle of radius R with the origin as its center/with its center at the origin. (adverbial of manner)\nClauses 从句 Adverbial clauses 状语从句 1.只要有摩擦力存在，就会产生热。\nHeat will be created as long as//whenever a friction force exists/is present/is available.\n2.直到引入了一种新概念,该问题才得到解决。(p. 94)\nThe problem was not solved until a new concept had been introduced.\n3.电流在电路中流动就像水在水管中流动那样。\nCurrent flows in the circuit (just) as water flows/does in the water-pipe.\n4.就电效应而言，基本粒子分为三种：中性的、正的和负的。\nAs far as electrical effects are concerned, fundamental particles fall into 3 groups: neutral, positive and negative.\n5.一切物质，不论是气体、液体还是固体，均由原子构成。\nAll the substances are made up of atoms whether they are gas, liquid or solid.\n(be they gas, liquid or solid)\n6.一切物体不论其温度有多低，均发射热射线。\nAll bodies emit heat rays no matter how low/however low their temperatures are.\n尽管原子很小，它们是由更小的微粒构成的。(P 94)\nSmall as they are, atoms are made up of even smaller particles.\n7.角的测量是独特的，在于单位是无量纲的。\nThe measurement of angles is unique in that the unit is dimensionless.\n该物体处于这样的状态以至于它能做功。(p.94)\nThe body is in such a state that it can do work.\n8.the way = (just) as\nCurrent flows in…the way water does…\nthe moment/instant = as soon as\nThe moment the gas is heated, its volume expands.\n9.the first/second time = when…for the first/second time\nThe first time they designed…, scientists met with a lot of difficulties.\nthe next/last time = when…next/last time\nThe next time you deal with such a problem, you’ll become more experienced.\neach/every/any time = whenever\nEvery time I’m in trouble, he is ready to help me.\nthe day = on the day when…\nThe day I graduated from college, I was determined to run my own business one day.\nAppositive clause 1.在这种情况下，不能保证该级数是收敛的。 (p. 95)\n…there is no guarantee/assurance that this series is convergent.\n毫无疑问……\nThere is no doubt that…….\n2.人们越来越认识到这些方法是很有用的。 (p. 95)\nPeople have become increasingly aware that…\nThere has been a growing/an increasing awareness/recognition that…\nNominal clauses (subject, object, predicative) 1.人们已经发现，电流的方向正好与电子流动的方向相反。\nIt has been found that the direction of the current is just opposite to that of the electron flow.\nIt has been found that the current flows in the opposite direction to the electrons.\n2.（由此）我们得知…\nFrom this we know that…\nFrom this we can see that…\nFrom this it follows that…\nWhat clause 现在我们把到目前为止所证明了的内容小结一下。\n…summarize what has been proved so far.\n发电机所做的是把机械能转变成电能。(p.100)\nWhat a generator does is (to) change mechanical energy into electrical energy.\nRelative/Attribute/Adjective clause 1.“that” instead of “which”\nanything that is stored in the memory (存储 器）\nAll (that) you need to do is (to) press the button. 你只需按一下这个按钮。\n2.the same/such … as …\nSuch meters as we use to measure resistance are called ohmmeters.\nWe have used the same machine in this experiment as we used in the previous experiment.\n3.prep. + which\n这些式子是X和Y均为零的特殊情况。\nThese equations are the special cases in which/where X and Y are both zero.\n这是光线通过的焦点。\nThis is the focus through which the rays pass.\n功率是做功的速率。\nPower is the rate at which work is done.\n水结冰的温度一般为$0^oC$。 (p. 117)\nThe temperature at which water freezes is generally 00 C .\n我们有待于求出该物体运动到原点右侧的最大距离。\nIt remains (for us) to find the maximum distance through which the body moves to the right of the origin.\n4.as…\n顾名思义，流体就是一种容易流动的物质。(p. 117)\nAs the name suggests, a fluid is a substance that/which flows readily. (p. 114)\nSubjunctive Mood 虚拟语气 Used in Conditionals (‘Were’ Pattern) 1.If there were no gravitation/gravity, everything on the earth would lose its weight.\nWere there no gravitation, everything …would lose…\n2.如果那个实验中用的水更多些的话，温度变化会更小些。\nIf more water had been used in that experiment, the change in temperature would have been smaller.\n3.If electronic computers had not been used, it would have taken them a long time to solve this problem.\n4.如果他当时在大学里学过科技英语，他现在被这家合资企业聘用的可能性就更大了。\nIf he had taken the EST course at college, he would be more likely to be hired by this joint venture now.\nUsed in Nominal Clauses (‘Be’ Pattern) 1.他们建议将该点接地。\n(Express this idea by using 4 different sentence patterns.)\nThey suggest (that) this point be grounded.\nTheir suggestion is that this point be grounded.\nIt is suggested that this point be grounded.\nThey have the suggestion that this point be grounded.\n2.把电路中所有的电阻都并联起来是不必要的。\nIt is unnecessary to combine all the resistors in parallel.\nIt is unnecessary that all the resistors be combined in parallel.\nUsed in Adverbial Clauses (‘Were’ or ‘Be’ Pattern) 1.在实际运算中我们处理电流的方式就好象导体中所有的自由电子都以相同的恒定速度运动。\nIn practical calculations we deal with current as if/though all the free electrons in a conductor moved at the same constant speed.\n2.这里的每样东西，不论是元件还是设备，均是国产的。（P 127， 2.8）\nwhether it be a component or a device be it a component or a device\n应把电池存放在干燥的地方以免漏电。\nBatteries should be kept in dry places lest/for fear that electricity (should) leak away.\nEmphasis of Sentence Elements 句子成分强调 Cleft Sentence 1.我们正是把电子的流动称为电流。\nWe call the flow of electrons electric current.\nIt is the flow of electrons that we call electric current.\n2.Which book on earth are you looking for?\nWhich book is it that you are looking for?\nThey don’t know how on earth electricity is generated.\nThey don’t know how it is that electricity is generated.\nAuxiliary “do” used to emphasize a verb 1.这个方法确实管用，不过尚不清楚到底在什么条件下才能使用它。(P 127, 3.4)\nThis method does work, but it is not clear yet under what conditions (it is that) the method can be used.\n“Very” used as an adj. to emphasize a noun 1.这正是我们预测的效果。\nThis is the effect that we have predicted. (just, very?)\nThis is just the effect that we have predicted.\nThis is the very effect that we have predicted.\n2.钨就是用于电灯里的那种金属。（P 127, 3.2）\nTungsten is the very metal used in electric lamps.\nInversion used to achieve emphasis 1.这一性质我们称之为惯性。（P 128, 3.5）\nWe call this property inertia.\nThis property we call inertia.\nThey have paid no attention to this point.\nNo attention have they paid to this point.\nWhat-clause used to show emphasis 1.This phenomenon has long confused scientists.\nIt is this phenomenon that …\nWhat has long confused scientists is this phenomenon.\nI want to have a good sleep after a long walk.\nWhat I want to do after a long walk is to have a good sleep.\nInversion 倒装 Full Inversion \u0026 Partial Inversion 1.Here came my friend.\nOnly yesterday did my friend come.\nThere exists a friction force on the surface of…\nHardly does a friction force exist on the surface of…\n2.常见的元素有氢、氧、碳等。\nAmong the common elements are hydrogen, oxygen, carbon, etc. (full inversion)\n应用更为广泛的是物质热胀（expand）冷缩（contract）的这一现象。\nOf wider application is the fact that matter usually expands when (it is) heated and contracts when (it is) cooled. (full inversion)\n3.By A is meant B.\n所谓 ( )，指的是 ( )。\n所谓伏特计，指的是用于测量电压的仪表。\nBy a voltmeter is meant the instrument used to measure voltage. (full inversion)\n所谓线性工作，指的是放大器在几乎没有或没有失真的情况下放大信号的能力。\nBy linear operation is meant the ability of an amplifier to amplify a signal with little or no distortion .\n4.只有当克服守恒力做功时，势能才会增加。\nOnly when work is done against a conservative force will there be an increase in potential energy. (partial inversion)\nOnly in this way can we build a harmonious society.\n绝不能把该点接地。\nNever/By no means/On no account/Under no circumstances can this point be grounded. (partial inversion)\nUnder no circumstances will China interfere in other countries’ internal affairs.\nEllipsis 省略 1.必须求出电阻器上的电流和电压。\nIt is necessary to find out the current through and the voltage across the resistor.\n加速度与外力成正比，并与其方向相同。\nThe acceleration is in direct proportion to and in the same direction as the applied force.\n2.必要时会采取适当措施的。\nProper measures will be taken if/when (they are) necessary/needed.\n不论状态如何，细胞的变化会不断进行。\nWhatever the state (is), the changes in the cell will go on constantly.\nSeparation 1.一定存在着阻止该物体下滑的一些力。\nSome forces must exist that prevent the body from sliding.\n2.The question arises how the damage can be minimized.\nToward the end of the 19th century, evidence began to appear that the atom was not the smallest particle.\n","title":"英语语法笔记","uri":"/contents/day-day-up/%E8%8B%B1%E8%AF%AD%E8%AF%AD%E6%B3%95/"},{"categories":["contents"],"content":"一些3d Face的相关论文，不定期更新。\n3D Face Surveys \u0026 Doctoral Thesis Face Image Analysis using a Multiple Features Fitting Strategy(2005, Basel)\n3D Face Modelling for 2D+3D Face Recognition(2007, Surrey)\nImage Based 3D Face Reconstruction: A Survey(IJIG2009, Georgios Stylianou, Andreas Lanitis, EUC, CUT)\nearly 3D facial acquisition approaches\nanimation reconstruction of deformable surfaces(2010, Hao Li, ETHz)\nInverse Rendering of Faces with a 3D Morphable Model(2012, Oswald Aldrian, York)\nDigital Geometry Processing Theory and Applications(2012, Kun Zhou, Zhengjiang, 中文)\nState of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications\nState of the Art on 3D Reconstruction with RGB-D Cameras(EG2018, MZ, CT, MPI, Stanford, TUM, Disney, Technicolor, UEN) [talks]\nPapers \u0026 Codes Reconstruction\u00263D Alignment\u0026Correspondences 1998 - 2015 A Morphable Model For The Synthesis Of 3D Faces\n(SIGGRAPH1998, V Blanz, T Vetter , MPI)\n3dmm,analysis-by-synthesis(cascaded, coarse to fine, using texture information), mid-detail Efficient, Robust and Accurate Fitting of a 3D Morphable Model\n(ICCV2003, S Romdhani, T Vetter , Basel)\n3dmm, fitting Algorithm needs: Efficient, Robust, Accurate, Automatic. Mid-detail\nEstimating 3D Shape and Texture Using Pixel Intensity, Edges, Specular Highlights, Texture Constraints and a Prior\n(CVPR2005, S Romdhani, T Vetter , Basel)\n3dmm, multiple features\nA 3D Face Model for Pose and Illumination Invariant Face Recognition\n(AVSS2009, Paysan, P., Knothe, R., Amberg, B., Romdhani, S., \u0026 Vetter, T. , Basel) [data](https://faces.cs.unibas.ch/bfm/)\n3D Face Reconstruction from a Single Image Using a Single Reference Face Shape\n(TPAMI2011, I Kemelmacher-Shlizerman, Basri R, UW)\ntemplate, sfs, texture information, mid-detail\nFace Reconstruction in the Wild\n(ICCV2011, Kemelmacher-Shlizerman I, Seitz S M , UW)\ncollection, sparse correspondence, warp template, low-rank approximation(photometric stereo, for expression normalization), mid-detail A FACS Valid 3D Dynamic Action Unit Database with Applications to 3D Dynamic Morphable Facial Modeling\n(ICCV2011, Cosker D, Krumhuber E, Hilton A. , UofSurrey)\naam, expression\nViewing Real-World Faces in 3D\n(ICCV2013, T Hassner, Open U Israel)\ntemplate, sparse correspondence, pose adjustment, depth optimization(SIFT) Improving 3D Face Details based on Normal Map of Hetero-source Images\n(CVPRW2014, Yang, C., Chen, J., Su, N., \u0026 Su, G. , Tsinghua University)\nTotal Moving Face Reconstruction\n(LNCS2014, Suwajanakorn S, Kemelmacher-Shlizerman I, Seitz S M. , Washington)\nvideo(collections), template, average shape, pose estimation, 3d flow(correspondence), refinement, high-detail FaceWarehouse: a 3D Facial Expression Database for Visual Computing\n(VCG2014, Cao, C., Weng, Y., Zhou, S., Tong, Y., \u0026 Zhou, K., Zhejiang) [data]\nIntrinsic Face Image Decomposition with Human Face Priors\n(ECCV2014, Li C, Zhou K, Lin S , Zhejiang)\nFitting 3D Morphable Models using Local Features\n(ICIP2015, Huber, P., Feng, Z. H., Christmas, W., Kittler, J., \u0026 Ratsch, M, Surrey)\nsparse correspondence, 3dmm, regression\nWhat Makes Tom Hanks Look Like Tom Hanks\n(ICCV2015, Suwajanakorn S, Seitz S M, Kemelmacher-Shlizerman I. , Washington)\ncollection, template, average model, 3D flow, correspondence, deformation vector, TPS, expression sililarity weighted, high-frequency details, Laplacian pyramid Unconstrained Realtime Facial Performance Capture\n(CVPR2015, Hsieh, P. L., Ma, C., Yu, J., \u0026 Li, H. , USC)\nvideo,image collections, occlusion, segmentation, landmarks Unconstrained 3D Face Reconstruction\n(CVPR2015, Roth, J., Tong, Y., \u0026 Liu, X, MSU)\ncollection, sparse correspondence(landmarks), template, photometric stereo(SVD), matrix completion, Pose-Invariant 3D Face Alignment\n(ICCV2015, Jourabloo, A., \u0026 Liu, X., MSU)\nalignment, dense correspondence, visibility, cascaded regressor, 3DPDM. Discriminative 3D Morphable Model Fitting\n(CVPR2015)\n2016 #CVPR\nLarge-pose Face Alignment via CNN-based Dense 3D Model Fitting\n(CVPR2016, Jourabloo, A., \u0026 Liu, X., MSU)\nalignment, 3dmm Automated 3D Face Reconstruction from Multiple Images using Quality Measures\n(CVPR2016, Piotraschke, M., \u0026 Blanz, V , Siegen)\nA Robust Multilinear Model Learning Framework for 3D Faces\n(CVPR2016, Bolkart, T., \u0026 Wuhrer, S., Saarland)\nFace Alignment Across Large Poses: A 3D Solution\n(CVPR2016, Zhu, X., Lei, Z., Liu, X., Shi, H., \u0026 Li, S. Z. , MSU, CASIA)\nAdaptive 3D Face Reconstruction from Unconstrained Photo Collections\n(CVPR2016, Roth, J., Tong, Y., \u0026 Liu, X, MSU)\nlandmarks, 3dmm, coarse-to-fine,photometric stereo, time: 7 minutes Augmented Blendshapes for Real-time Simultaneous 3D Head Modeling and Facial Motion Capture\n(CVPR2016, Thomas, D., \u0026 Taniguchi, R. I. , Kyushu University)\nA 3D Morphable Model learnt from 10,000 faces\n(CVPR2016, Booth, J., Roussos, A., Zafeiriou, S., Ponniah, A., \u0026 Dunaway, D., ICL)\n#ECCV\nJoint Face Alignment and 3D Face Reconstruction\n(ECCV2016, Liu, F., Zeng, D., Zhao, Q., \u0026 Liu, X, MSU, Sichuan U)\nalignment, landmarks\nReal-Time Facial Segmentation and Performance Capture from RGB Input\n(ECCV2016, Saito, S., Li, T., \u0026 Li, H. , USC)\nocclusions, tracking\n#Others\n3D Face Reconstruction by Learning from Synthetic Data\n(3DV2016, Richardson, E., Sela, M., \u0026 Kimmel, R, IIT)\n3dmm, cnn, regress 3dmm parameters, sfs\nFace Reconstruction on Mobile Devices Using a Height Map Shape Model and Fast Regularization\n(3DV2016, Maninchedda, F., Häne, C., Oswald, M. R., \u0026 Pollefeys, M., ETH)\nA Multiresolution 3D Morphable Face Model and Fitting Framework [code]\n(IJCV2016, Huber, P., Hu, G., Tena, R., Mortazavian, P., Koppen, P., Christmas, W. J., … \u0026 Kittler, J. ，Surrey)\nRapid Photorealistic Blendshape Modeling from RGB-D Sensors\n(2016, USC)\n3D Face Reconstruction with Region Based Best Fit Blending Using Mobile Phone for Virtual Reality Based Social Media\n(2016, Anbarjafari, G., Haamer, R. E., Lusi, I., Tikk, T., \u0026 Valgma, L. , Turkey)\nlandmarks, uv texture, region\n2017 #CVPR\n3D Face Morphable Models “In-the-Wild”\n(CVPR2017, Booth, J., Antonakos, E., Ploumpis, S., Trigeorgis, G., Panagakis, Y., \u0026 Zafeiriou, S., ICL)\n3dmm, register, UV\nFace Normals “in-the-wild” using Fully Convolutional Networks\n(CVPR2017, Trigeorgis, G., Snape, P., Kokkinos, I., \u0026 Zafeiriou, S. , ICL)\nRegressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network\n(CVPR2017, Tran, A. T., Hassner, T., Masi, I., \u0026 Medioni, G. , USC)\nFast 3D Reconstruction of Faces with Glasses\n(CVPR2017, Maninchedda, F., Oswald, M. R., \u0026 Pollefeys, M., ETH)\nDenseReg: Fully Convolutional Dense Shape Regression In-the-Wild\n(CVPR2017, Güler, R. A., Trigeorgis, G., Antonakos, E., Snape, P., Zafeiriou, S., \u0026 Kokkinos, I. , ICL)\ndense correspondence, uv\nLearning Detailed Face Reconstruction from a Single Image\n(CVPR2017, Richardson, E., Sela, M., Or-El, R., \u0026 Kimmel, R. , Washington)\nEnd-to-end 3D face reconstruction with deep neural networks\n(CVPR2017, Dou, P., Shah, S. K., \u0026 Kakadiaris, I. A., UofHouston)\n3dmm, dl, directly learn 3dmm parameters\nA Generative Model for Depth-based Robust 3D Facial Pose Tracking\n(CVPR2017, Cai, L. S. J., Pavlovic, T. J. C. V., \u0026 Ngan, K. N. , CUHK)\nocclusions\n#ICCV\n3D Morphable Models as Spatial Transformer Networks\n(ICCV2017, Bas, A., Huber, P., Smith, W. A., Awais, M., \u0026 Kittler, J. , York, Surrey )\ndl, cnn, uv texture, landmarks, stn, 3dmm\nFaster Than Real-time Facial Alignment: A 3D Spatial Transformer Network Approach in Unconstrained Poses\n(ICCV2017, Bhagavatula, C., Zhu, C., Luu, K., \u0026 Savvides, M., CMU)\nPose-Invariant Face Alignment with a Single CNN\n(ICCV2017, Jourabloo, A., Ye, M., Liu, X., \u0026 Ren, L. , MSU)\nalignment, 3dmm, dl, cnn\nLarge Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression code\n(ICCV2017, Jackson, A. S., Bulat, A., Argyriou, V., \u0026 Tzimiropoulos, G. , Nottingham)\nend-to-end, 3dmm, dl, cnn, landmarks, voxel Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation\n(ICCV2017, Sela, M., Richardson, E., \u0026 Kimmel, R. , IIT)\nMoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction\n(ICCV2017, Tewari, A., Zollhöfer, M., Kim, H., Garrido, P., Bernard, F., Pérez, P., \u0026 Theobalt, C. , MPI)\nDense Face Alignment code\n(ICCVW2017, Liu, Y., Jourabloo, A., Ren, W., \u0026 Liu, X. , MSU)\nRealtime Dynamic 3D Facial Reconstruction for Monocular Video In-the-Wild\n(ICCVW2017)\nLearning Dense Facial Correspondences in Unconstrained Images\n(ICCV2017, Yu, R., Saito, S., Li, H., Ceylan, D., \u0026 Li, H. , USC)\ndense correspondence\n#others\nLarge Scale 3D Morphable Models [data](https://faces.cs.unibas.ch/bfm/)\n(IJCV2017, Booth, J., Roussos, A., Ponniah, A., Dunaway, D., \u0026 Zafeiriou, S. , ICL)\nfor alignment, template, cnn, dl, sparse correspondence, landmarks, tps warping What does 2D geometric information really tell us about 3D face shape? (2017, Bas, A., \u0026 Smith, W. A )\nshape from landmarks, shape from contours\nPix2Face: Direct 3D Face Model Estimation\n(2017)\ndense correspondence, 3dmm\n3D Face Reconstruction with Geometry Details from a Single Image\n(TIP2017, Jiang, L., Zhang, J., Deng, B., Li, H., \u0026 Liu, L. , USC, USTC)\ncoarse-to-fine, landmarks, corrective deformatio, sfs 2018 #CVPR\nUnsupervised Training for 3D Morphable Model Regression code\n(CVPR2018, Genova, K., Cole, F., Maschinot, A., Sarna, A., Vlasic, D., \u0026 Freeman, W. T , Google)\n4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications data\n(CVPR2018, Cheng, S., Kotsia, I., Pantic, M., \u0026 Zafeiriou, S., ICL)\nSparse Photometric 3D Face Reconstruction Guided by Morphable Models\n(CVPR2018, Cao, X., Chen, Z., Chen, A., Chen, X., Li, S., \u0026 Yu, J. , shanghaitech)\n5 input images, 3dmm, shadow processing, light calibration, photometric stereo, denoising Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition\n(CVPR2018, Liu, F., Zhu, R., Zeng, D., Zhao, Q., \u0026 Liu, X. , MSU, Sichuan)\nMesoscopic Facial Geometry Inference Using Deep Neural Networks\n(CVPR2018, Hao Li, USC)\nhigh-detail, dl, scan, uv texture, displacement Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz\n(CVPR2018, Tewari, A., Zollhöfer, M., Garrido, P., Bernard, F., Kim, H., Pérez, P., \u0026 Theobalt, C., MPI)\nSfSNet : Learning Shape, Reflectance and Illuminance of Faces in the Wild\n(CVPR2018, Sengupta, S., Kanazawa, A., Castillo, C. D., \u0026 Jacobs, D. , Maryland, UCB )\nProbabilistic Joint Face-Skull Modelling for Facial Reconstruction\n(CVPR2018, Madsen, D., Lüthi, M., Schneider, A., \u0026 Vetter, T. , Basel)\nAlive Caricature from 2D to 3D\n(CVPR2018, Wu, Q., Zhang, J., Lai, Y. K., Zheng, J., \u0026 Cai, J, USTC)\nNonlinear 3D Face Morphable Model\n(CVPR2018, Tran, L., \u0026 Liu, X. , MSU)\nInverseFaceNet: Deep Monocular Inverse Face Rendering\n(CVPR2018, Kim, H., Zollhöfer, M., Tewari, A., Thies, J., Richardt, C., \u0026 Theobalt, C. , MPI)\nSelf-Supervised Bootstrapping\nExtreme 3D Face Reconstruction: Looking Past Occlusions\n(CVPR2018, Tran, A. T., Hassner, T., Masi, I., Paz, E., Nirkin, Y., \u0026 Medioni, G. , USC)\nTotal Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies\n(CVPR2018, best student paper, Joo, H., Simon, T., \u0026 Sheikh, Y. , CMU)\nModeling Facial Geometry using Compositional VAEs\n(CVPR2018, Bagautdinov, T., Wu, C., Saragih, J., Fua, P., \u0026 Sheikh, Y., EPEL, FRL )\n#ECCV\n3D Face Reconstruction from Light Field Images: A Model-free Approach\n(ECCV2018, Feng, M., Gilani, S. Z., Wang, Y., \u0026 Mian, A., Western Australia, Hunan)\nepopolar plane images\nGenerating 3D Faces using Convolutional Mesh Autoencoders [code]\n(ECCV2018, Ranjan, A., Bolkart, T., Sanyal, S., \u0026 Black, M. J., MPI)\nJoint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network [code]\n(ECCV2018, Feng, Y., Wu, F., Shao, X., Wang, Y., \u0026 Zhou, X., SJTU)\n#others\nMorphable Face Models - An Open Framework\n(FG2018, Gerig, T., Morel-Forster, A., Blumer, C., Egger, B., Luthi, M., Schönborn, S., \u0026 Vetter, T. , Basel)\nCNN-based Real-time Dense Face Reconstruction with Inverse-rendered Photo-realistic Face Images [data\u0026code]\n(TPAMI2018, Yudong Guo, Juyong Zhang, Jianfei Cai, Boyi Jiang, Jianmin Zheng, USTC)\nMultilinear Autoencoder for 3D Face Model Learning(WACV 2018, Universite Grenoble Alpes (LJK), France)\n3d scan to registered mesh. dl. height map On Face Segmentation, Face Swapping, and Face Perception(AFGR,2018, HT)\nEvaluation of Dense 3D Reconstruction from 2D Face Images in the Wild(FG2018) data\n#arxiv\nJoint Face Alignment and 3D Face Reconstruction with Application to Face Recognition(2017, Feng Liu, Xiaoming Liu)\nConvolutional Point-set Representation: A Convolutional Bridge Between a Densely Annotated Image and 3D Face Alignment(20180317)\nUnsupervised Depth Estimation, 3D Face Rotation and Replacement(20180325)\nProduction-level Reconstruction more in computer graphics\nHigh-Quality Single-Shot Capture of Facial Geometry(TOG2010, ETHZ, Disney)\ncg, high-detail,stereo system, calibration, surface refinement, normal direction, mesoscopic Multiview Face Capture using Polarized Spherical Gradient Illumination(TOG2011)\nimage collecitons\nHigh-Quality Passive Facial Performance Capture using Anchor Frames(SIGGRAPH2011, ETHZ, Disney)\ncg, stereo,anchor frame, tracking, mesh progration, physical movement, motion estimation, refinement Lightweight binocular facial perfor- mance capture under uncontrolled lighting(TOG2012, MPI)\ncg, high-detail, stereo, template,flow,data term, geometry term, smoothness term, mesh tracking, motion refinement, shape refinement, sfs Reconstructing Detailed Dynamic Face Geometry from Monocular Video(TOG2013, MPI)\ncg, dynamic, high-detail, blend model, sparse correspondence, dense correspondence(appearance matching, LBP), pose estimation , shape refinement, sfs 3D Shape Regression for Real-time Facial Animation(TOG2013, ZJU)\nReal-Time High-Fidelity Facial Performance Capture (TOG2015, ZJU)\ncg, landmarks, optical flow, train a regressor to learn detail Dynamic 3D Avatar Creation from Hand-held Video Input(TOG2015, EPEL)\ncg, dynamic, mobile, high-detail, avatar, 3dmm,sparse correspondence, eye mesh, tracking, refinement, sfs, detail map Reconstruction of Personalized 3D Face Rigs from Monocular Video(TOG2016, MPI)\nparametric shape prior, coarse-scale reconstruction, fine-scale(sfs), coase-\u003emedium-\u003efine, 3dmm, corrective Production-Level Facial Performance Capture Using Deep Convolutional Neural Networks（ASCA2017, USC)\nMulti-View Stereo on Consistent Face Topology(EG2017, USC)\ncg, high-detail, landmarks, template, pose estimation, refinement\nAvatar Digitization From a Single Image For Real-Time Rendering(SIGGRAPH Asia 2017, USC)\ncg, avatar, segmentation, head, hair, 3DMM, landmarks, texture completion Learning a model of facial shape and expression from 4D scans(TOG2017, USC, MPI)\nDeepSketch2Face: A Deep Learning Based Sketching System for 3D Face and Caricature Modeling(SIGGRAPH2017)\nHigh-Fidelity Facial Reflectance and Geometry Inference From an Unconstrained Image(SIGGRAPH2018, USC)\nTexture 3D-aid texture generation/ UV texture completion\nKeys: GAN\nFace Synthesis from Facial Identity Features(CVPR2017, google)\n3dmm, dl, landmarks\nPhotorealistic Facial Texture Inference Using Deep Neural Networks(CVPR2017, Hao Li, USC)\ntexture completion\nUV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition(CVPR2018, SZ, ICL)\ngan, 3dmm, uv texture completion\nMulti-Attribute Robust Component Analysis for Facial UV Maps(2017, SZ, ICL)\nRealistic Dynamic Facial Textures from a Single Image using GANs(CVPR2017, Hao Li, USC, DeepMind)\nSemi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model(2018)\nSide Information for Face Completion: a Robust PCA Approach(20180120, SZ, ICL)\n​\nTransfer\u0026Reenactment(Applications) Face Transfer with Multilinear Models (SIGGRAPH2005)\nCartesian product(ID x EX x VI)\nOnline Modeling For Realtime Facial Animation(TOG2013)\nrgbd, blendshape, corrective field Displaced Dynamic Expression Regression for Real-time Facial Tracking and Animation(SIGGRAPH2014)\nReal-time Expression Transfer for Facial Reenactment(SIGGRAPH AISA 2015)\nFace2Face: Real-time Face Capture and Reenactment of RGB Videos(CVPR2016)\ncapture, transfer, 3dmm, landmarks, texture, expression, mouth retrieval Synthesizing Obama: Learning Lip Sync from Audio(SIGGRAPH2017)\nDeep Video Portrait(SIGGRAPH2018)\nHeadOn: Real-time Reenactment of Human Portrait Videos(SIGGRAPH2018)\n3D-aid 2D face recognition Tom-vs-Pete Classifiers and Identity-Preserving Alignment for Face Verification(ECCV2012, Columbia University)\nFace Recognition from a Single Training Image under Arbitrary Unknown Lighting Using Spherical Harmonics(PAMI2006)\n3D-aided face recognition robust to expression and pose variations (CVPR2014)\nEffective 3D based Frontalization for Unconstrained Face Recognition(ICPR2016, MICC, Florence)\nEffective Face Frontalization in Unconstrained Images(CVPR2015, TH, Israel)\nDo We Really Need to Collect Millions of Faces for Effective Face Recognition(ECCV2016, TH, USC, Israel)\nHigh-Fidelity Pose and Expression Normalization for Face Recognition in the Wild(CVPR2015)\nWhen 3D-Aided 2D Face Recognition Meets Deep Learning: An extended UR2D for Pose-Invariant Face Recognition(2017)\nTowards Large-Pose Face Frontalization in the Wild\nFully Automatic Pose-Invariant Face Recognition via 3D Pose Normalization (ICCV2011, Cambridge, MA, USA)\n3D face recognition Face Identification across Different Poses and Illuminations with a 3D Morphable Model(Automatic Face and Gesture Recognition2002, VB\u0026TV)\nPreliminary Face Recognition Grand Challenge Results(2006)\nexpression Invariant 3D Face Recognition with a Morphable Model(FG2008, TV, Basel)\nBosphorus Database for 3D Face Analysis(2008)data\nRobust Learning from Normals for 3D face recognition(ECCV2012, SZ, ICL)\nStatic and dynamic 3D facial expression recognition: A comprehensive survey(IVC2012, SZ, LijunYin)\nDeep 3D Face Identification(2017, USC)\nRobust Face Recognition with Deeply Normalized Depth Images (2018)\ndepth image(front\u0026neural)\nLearning from Millions of 3D Scans for Large-scale 3D Face Recognition(CVPR2018, Western Australia)\n","title":"3D Face 论文索引","uri":"/contents/face-reconstruction/3d-face-papers/"},{"categories":["contents"],"content":"本文是cmu开源项目MonocularTotalCapture在ubuntu上的编译过程，把一些比较大的坑记录好说不定哪天得再编译一遍。环境是Ubuntu 18.04，gtx1060,显卡驱动4.10。cuda和cudnn版本因为在配置openpose和MonocularTotalCapture，我装了两个版本，留在下面再说。\nDependencies ffmpeg Python 3.5 (with TensorFlow 1.5.0, OpenCV, Matplotlib, packages installed with pip3) cmake \u003e= 2.8 OpenCV 2.4.13 (compiled from source with CUDA 9.0, CUDNN 7.0) Ceres-Solver 1.13.0 (with SuiteSparse) OpenGL, GLUT, GLEW libigl wget openPose 配置过程中遇到问题较多的是配置Opencv 2.4.13+CUDA 9.0+CUDNN对应版本，以及配置openpose。CUDNN不一定要7.0版本，与CUDA9.0适配即可。在ubuntu18.04配置openpose需要安装cuda10以及对应版本，我装的是cuda10.0。\n首先说明其他问题不是很大的依赖项：\nffmpeg下载最新版压缩包解压编译即可。 Python 3.5 (with TensorFlow 1.5.0, OpenCV, Matplotlib, packages installed with pip3)：如果你常用的python版本不是3.5,建议创建一个conda 环境，然后安装TensorFlow 1.5.0和对应依赖项，我装的是不带gpu版本。tensorflow各版本与cuda,cudnn的对应关系看：tensorflow各个版本的CUDA以及Cudnn版本对应关系 cmake，Ceres-Solver 1.13.0 (with SuiteSparse)，OpenGL, GLUT,wget都可以通过apt直接装，GLEW到官网下载压缩包编译即可，libigl可以git clone github上的源码安装。 编译openpose 主要参考文章： openpose环境搭建\nubuntu18.04+caffe+cuda10.1 + openpose工作环境搭建-2019-07-03\n由于我是在Ubuntu18.04安装openpose，需要安装cuda10.0和对应cudnn版本，安装cuda10.0和对应cudnn版本的有关文章网上很多，我这里就不写了。完整的安装过程看openPose的安装文档。\n关于依赖项Prerequisites 重要！如果已经安装了Anaconda，要卸载anaconda里面的Protobuf ，不然后面会报错。\n关于Custom Caffe openpose配置文档建议用cmu自家的caffe，但我配置的时候出错了。貌似是Protobuf的版本问题我一直不能解决。上面的文章建议安装官方的caffe，但我最后发现最新版的caffe修改了一些网络层，导致caffe的预训练不能用了。所以我安装的是：caffe f019。\n其他没有什么很大的问题了。\n编译Opencv 2.4.13+CUDA 9.0+CUDNN对应版本 参考文章：\n[ubuntu16.04 在cuda9.0环境下编译安装opencv2.4.13.7](https://blog.csdn.net/zhuangwu116/article/details/81136117）\nubuntu16.04+CUDA8.0+GTX1050Ti+caffe（GPU）的opencv2.4.10+CUDA环境配置\n按照上面文章修改opencv的一部分配置文件来适配cuda9.0，没错，这里需要装一个cuda9.0和对应cudnn版本。\n编译错误记录 首先看这篇文章，里面有一些常见的坑： 编译cuda版opencv遇到的坑\n我的gpu是gtx1060，GPU arch/PTX archs都要设置为6.1，不然后面运行代码会报错。\n其他错误：\nmodules/highgui/CMakeFiles/opencv_highgui.dir/build.make:230: recipe for target 'modules/highgui/CMakeFiles/opencv_highgui.dir/src/cap_ffmpeg.cpp.o' failed 参考： Error in building opencv with ffmpeg\nrelocation R_ARM_MOVW_ABS_NC against a local symbol' can not be used when making a shared object; recompile with -fPIC 参考： opencv交叉编译错误处理\n[ 50%] Linking CXX executable extract_cpu /usr/bin/ld: cannot find -lopencv_dep_nppial /usr/bin/ld: cannot find -lopencv_dep_nppicc /usr/bin/ld: cannot find -lopencv_dep_nppicom /usr/bin/ld: cannot find -lopencv_dep_nppidei /usr/bin/ld: cannot find -lopencv_dep_nppif /usr/bin/ld: cannot find -lopencv_dep_nppig /usr/bin/ld: cannot find -lopencv_dep_nppim /usr/bin/ld: cannot find -lopencv_dep_nppist /usr/bin/ld: cannot find -lopencv_dep_nppisu /usr/bin/ld: cannot find -lopencv_dep_nppitc collect2: error: ld returned 1 exit status CMakeFiles/extract_cpu.dir/build.make:121: recipe for target 'extract_cpu' failed make[2]: *** [extract_cpu] Error 1 CMakeFiles/Makefile2:104: recipe for target 'CMakeFiles/extract_cpu.dir/all' failed make[1]: *** [CMakeFiles/extract_cpu.dir/all] Error 2 Makefile:83: recipe for target 'all' failed make: *** [all] Error 2 参考： cannot find nppi series when building dense_flow with opencv2.4.13\nHow can you link GLEW to a project with CMake? 参考：\nLinking GLEW with CMake\n","title":"Ubuntu18.04编译MonocularTotalCapture","uri":"/contents/human-body-reconstruction/monocular-total-capture-compile/"},{"categories":["contents"],"content":"用光流跟踪特征点和用直接法计算相机位姿。\n为什么用直接法 尽管特征点法在视觉里程计中占据主流地位,研究者们认识到它至少有以下几个缺点:\n关键点的提取与描述子的计算非常耗时。实践当中,SIFT 目前在 CPU 上是无法实 时计算的,而 ORB 也需要近 20 毫秒的计算。如果整个 SLAM 以 30 毫秒/帧的速 度运行,那么一大半时间都花在计算特征点上。\n使用特征点时,忽略了除特征点以外的所有信息。一张图像有几十万个像素,而特征 点只有几百个。只使用特征点丢弃了大部分可能有用的图像信息。\n相机有时会运动到特征缺失的地方,往往这些地方没有明显的纹理信息。例如,有时 我们会面对一堵白墙,或者一个空荡荡的走廓。这些场景下特征点数量会明显减少, 我们可能找不到足够的匹配点来计算相机运动。\n我们看到使用特征点确实存在一些问题。有没有什么办法能够克服这些缺点呢?我们 有以下几种思路:\n保留特征点,但只计算关键点,不计算描述子。同时,使用光流法(Optical Flow)来 跟踪特征点的运动。这样可以回避计算和匹配描述子带来的时间,但光流本身的计算 需要一定时间; 只计算关键点,不计算描述子。同时,使用直接法(Direct Method)来计算特征点 在下一时刻图像的位置。这同样可以跳过描述子的计算过程,而且直接法的计算更加 简单。 既不计算关键点、也不计算描述子,而是根据像素灰度的差异,直接计算相机运动。 第一种方法仍然使用特征点,只是把匹配描述子替换成了光流跟踪,估计相机运动时 仍使用对极几何、PnP 或 ICP 算法。而在后两个方法中,我们会根据图像的像素灰度信 息来计算相机运动,它们都称为直接法。(在直接法中,我们并不需要知道点与点之间之间的对应关系,而是通过最小化光度误差(Photometric error)来求得它们。)\n光流(Optical Flow) 直接法是从光流演变而来的。它们非常相似,具有相同的假设条件。光流描述了像素 在图像中的运动,而直接法则附带着一个相机运动模型。 光流是一种描述像素随着时间,在图像之间运动的方法,如图 8-1 所示。随着时间的 经过,同一个像素会在图像中运动,而我们希望追踪它的运动过程。计算部分像素运动的 称为稀疏光流,计算所有像素的称为稠密光流。稀疏光流以 Lucas-Kanade 光流为代表,并 可以在 SLAM 中用于跟踪特征点位置。\nLucas-Kanade 光流 在 LK 光流中,我们认为来自相机的图像是随时间变化的。图像可以看作时间的函数: $\\boldsymbol{I}(t)$。那么,一个在 t 时刻,位于 (x, y) 处的像素,它的灰度可以写成 $$ \\boldsymbol{I}(x,y,t) $$ 这种方式把图像看成了关于位置与时间的函数,它的值域就是图像中像素的灰度。现在考 虑某个固定的空间点,它在 t 时刻的像素坐标为 x, y。由于相机的运动,它的图像坐标将 发生变化。我们希望估计这个空间点在其他时刻里图像的位置。怎么估计呢?这里要引入 光流法的基本假设:\n灰度不变假设:同一个空间点的像素灰度值,在各个图像中是固定不变的。 对于 t 时刻位于 (x, y) 处的像素,我们设 t + dt 时刻,它运动到 (x + dx, y + dy) 处。 由于灰度不变,我们有: $$ \\boldsymbol{I}(x + dx, y + dy, t + dt) = \\mathbf{I}(x,y,t) $$ 灰度不变假设是一个很强的假设,实际当中很可能不成立。事实上,由于物体的材质 不同,像素会出现高光和阴影部分;有时,相机会自动调整曝光参数,使得图像整体变亮 或变暗。这些时候灰度不变假设都是不成立的,因此光流的结果也不一定可靠。 对左边进行泰勒展开,保留一阶项,得: $$ \\boldsymbol{I}(x+\\mathrm{d} x, y+\\mathrm{d} y, t+\\mathrm{d} t) \\approx \\boldsymbol{I}(x, y, t)+\\frac{\\partial \\boldsymbol{I}}{\\partial x} \\mathrm{d} x+\\frac{\\partial \\boldsymbol{I}}{\\partial y} \\mathrm{d} y+\\frac{\\partial \\boldsymbol{I}}{\\partial t} \\mathrm{d} t $$ 因为我们假设了灰度不变,于是下一个时刻的灰度等于之前的灰度,从而 $$ \\frac{\\partial \\boldsymbol{I}}{\\partial x} \\mathrm{d} x+\\frac{\\partial \\boldsymbol{I}}{\\partial y} \\mathrm{d} y+\\frac{\\partial \\boldsymbol{I}}{\\partial t} \\mathrm{d} t=0 $$ 两边除以 dt,得: $$ \\frac{\\partial \\boldsymbol{I}}{\\partial x} \\frac{\\mathrm{d} x}{\\mathrm{d} t}+\\frac{\\partial \\boldsymbol{I}}{\\partial y} \\frac{\\mathrm{d} y}{\\mathrm{d} t}=-\\frac{\\partial \\boldsymbol{I}}{\\partial t} $$ 其中 dx/dt 为像素在 x 轴上运动速度,而 dy/dt 为 y 轴速度,把它们记为 u, v。同 时 $\\frac{\\partial \\boldsymbol{I}}{\\partial x}$为图像在该点处 x 方向的梯度,另一项则是在 y 方向的梯度,记为 $\\boldsymbol{I} _x, \\boldsymbol{I}_y$ 。把图像灰度对时间的变化量记为 $\\boldsymbol{I}_t$ ,写成矩阵形式,有:\n$$ \\left[\\begin{array}{ll}{\\boldsymbol{I}_{x}} \u0026 {\\boldsymbol{I}_{y}}\\end{array}\\right]\\left[\\begin{array}{l}{u} \\\\ {v}\\end{array}\\right]=-\\boldsymbol{I}_{t} $$ 我们想计算的是像素的运动 u, v,但是该式是带有两个变量的一次方程,仅凭它无法 计算出 u, v。因此,必须引入额外的约束来计算 u, v。在 LK 光流中,我们假设某一个窗 口内的像素具有相同的运动。 考虑一个大小为 w × w 大小的窗口,它含有 $w^2$数量的像素。由于该窗口内像素具有 同样的运动,因此我们共有 $w^2$ 个方程:\n$$ \\left[\\begin{array}{ll}{\\boldsymbol{I}_{x}} \u0026 {\\boldsymbol{I}_{y}}\\end{array}\\right]_{k}\\left[\\begin{array}{l}{u} \\\\ {v}\\end{array}\\right]=-\\boldsymbol{I}_{t k}, \\quad k=1, \\ldots, w^{2} $$\n记： $$ \\boldsymbol{A}=\\left[\\begin{array}{c}{\\left[\\boldsymbol{I}_{x}, \\boldsymbol{I}_{y}\\right]_{1}} \\\\ {\\vdots} \\\\ {\\left[\\boldsymbol{I}_{x}, \\boldsymbol{I}_{y}\\right]_{k}}\\end{array}\\right], \\boldsymbol{b}=\\left[\\begin{array}{c}{\\boldsymbol{I}_{t 1}} \\\\ {\\vdots} \\\\ {\\boldsymbol{I}_{t k}}\\end{array}\\right] $$ 于是整个方程： $$ A\\left[\\begin{array}{l}{u} \\ {v}\\end{array}\\right]=-b $$ 这是一个关于 u, v 的超定线性方程,传统解法是求最小二乘解。 $$ \\left[\\begin{array}{l}{u} \\ {v}\\end{array}\\right]^{*}=-\\left(\\boldsymbol{A}^{T} \\boldsymbol{A}\\right)^{-1} \\boldsymbol{A}^{T} \\boldsymbol{b} $$ 这样就得到了像素在图像间的运动速度 u, v。当 t 取离散的时刻而不是连续时间时,我 们可以估计某块像素在若干个图像中出现的位置。由于像素梯度仅在局部有效,所以如果 一次迭代不够好的话,我们会多迭代几次这个方程。\n代码实践：LK 光流 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 #include \u003ciostream\u003e #include \u003cfstream\u003e #include \u003clist\u003e #include \u003cvector\u003e #include \u003cchrono\u003e using namespace std; #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003copencv2/video/tracking.hpp\u003e int main( int argc, char** argv ) { if ( argc != 2 ) { cout\u003c\u003c\"usage: useLK path_to_dataset\"\u003c\u003cendl; return 1; } string path_to_dataset = argv[1]; string associate_file = path_to_dataset + \"/associate.txt\"; ifstream fin( associate_file ); if ( !fin ) { cerr\u003c\u003c\"I cann't find associate.txt!\"\u003c\u003cendl; return 1; } string rgb_file, depth_file, time_rgb, time_depth; list\u003c cv::Point2f \u003e keypoints; // 因为要删除跟踪失败的点，使用list cv::Mat color, depth, last_color; for ( int index=0; index\u003c100; index++ ) { fin\u003e\u003etime_rgb\u003e\u003ergb_file\u003e\u003etime_depth\u003e\u003edepth_file; color = cv::imread( path_to_dataset+\"/\"+rgb_file ); depth = cv::imread( path_to_dataset+\"/\"+depth_file, -1 ); if (index ==0 ) { // 对第一帧提取FAST特征点 vector\u003ccv::KeyPoint\u003e kps; cv::Ptr\u003ccv::FastFeatureDetector\u003e detector = cv::FastFeatureDetector::create(); detector-\u003edetect( color, kps ); for ( auto kp:kps ) keypoints.push_back( kp.pt ); last_color = color; continue; } if ( color.data==nullptr || depth.data==nullptr ) continue; // 对其他帧用LK跟踪特征点 vector\u003ccv::Point2f\u003e next_keypoints; vector\u003ccv::Point2f\u003e prev_keypoints; for ( auto kp:keypoints ) prev_keypoints.push_back(kp); vector\u003cunsigned char\u003e status; vector\u003cfloat\u003e error; chrono::steady_clock::time_point t1 = chrono::steady_clock::now(); cv::calcOpticalFlowPyrLK( last_color, color, prev_keypoints, next_keypoints, status, error ); chrono::steady_clock::time_point t2 = chrono::steady_clock::now(); chrono::duration\u003cdouble\u003e time_used = chrono::duration_cast\u003cchrono::duration\u003cdouble\u003e\u003e( t2-t1 ); cout\u003c\u003c\"LK Flow use time：\"\u003c\u003ctime_used.count()\u003c\u003c\" seconds.\"\u003c\u003cendl; // 把跟丢的点删掉,因为要删除跟踪失败的点，使用list,而不是vector // keypoint 的数目会逐渐减少 int i=0; for ( auto iter=keypoints.begin(); iter!=keypoints.end(); i++) { if ( status[i] == 0 ) { iter = keypoints.erase(iter); continue; } *iter = next_keypoints[i]; iter++; } cout\u003c\u003c\"tracked keypoints: \"\u003c\u003ckeypoints.size()\u003c\u003cendl; if (keypoints.size() == 0) { cout\u003c\u003c\"all keypoints are lost.\"\u003c\u003cendl; break; } // 画出 keypoints cv::Mat img_show = color.clone(); for ( auto kp:keypoints ) cv::circle(img_show, kp, 10, cv::Scalar(0, 240, 0), 1); cv::imshow(\"corners\", img_show); cv::waitKey(0); last_color = color; } return 0; } LK 光流跟踪能够直接得到特征点的对应关系。这个对应关系就像是描述子的匹配,但实际上我们大多数时候只会碰到特征点跟丢的情况,而不太会遇到误匹配,这应该是光流相对于描述子的一点优势。但是,匹配描述子的方法在相机运动较大时仍能成功,而光流必须要求相机运动是微小的。从这方面来说,光流的鲁棒性比描述子差一些。\n直接法 理论 考虑某个空间点$ P$ 和两个时刻的相机。$P$ 的世界坐标为 [X, Y, Z],它 在两个相机上成像,记非齐次像素坐标为$ p_ 1 , p_ 2 $。我们的目标是求第一个相机到第二个相 机的相对位姿变换。我们以第一个相机为参照系,设第二个相机旋转和平移为 $R, t$，对应李代数为$\\xi$\n$$ \\begin{array}{l}{p_{1}=\\left[\\begin{array}{c}{u} \\\\ {v} \\\\ {1}\\end{array}\\right]_{1}=\\frac{1}{Z_{1}} K P} \\\\ {p_{2}=\\left[\\begin{array}{c}{u} \\\\ {v} \\\\ {1}\\end{array}\\right]=\\frac{1}{Z_{2}} K(R P+t)=\\frac{1}{Z_{2}} K\\left(\\exp \\left(\\xi^{\\wedge}\\right) P\\right)_{1 : 3}}\\end{array} $$\n在直接法中,由于没有特征匹配,我们无从知道哪一个 $p _2$ 与 $p_1$ 对应 着同一个点。直接法的思路是根据当前相机的位姿估计值,来寻找$ p_ 2$ 的位置。但若相机 位姿不够好,$p_ 2 $的外观和$ p_ 1$ 会有明显差别。于是,为了减小这个差别,我们优化相机的 位姿,来寻找与 $p_ 1$ 更相似的$ p_ 2$ 。这同样可以通过解一个优化问题,但此时最小化的不是 重投影误差,而是光度误差(Photometric Error),也就是 $P$ 的两个像的亮度误差:\n$$ e=\\boldsymbol{I}_{1}\\left(\\boldsymbol{p}_{1}\\right)-\\boldsymbol{I}_{2}\\left(\\boldsymbol{p}_{2}\\right) $$\n在直接法中,我们假设一个空间点 在各个视角下,成像的灰度是不变的。能够做这种优化的理由,仍是基于灰度不变假设。我们有许多个(比如 N 个)空间点 P i ,那么,整 个相机位姿估计问题变为:\n$$ \\min _{\\xi} J(\\boldsymbol{\\xi})=\\sum_{i=1}^{N} e_{i}^{T} e_{i}, \\quad e_{i}=\\boldsymbol{I}_{1}\\left(\\boldsymbol{p}_{1, i}\\right)-\\boldsymbol{I}_{2}\\left(\\boldsymbol{p}_{2, i}\\right) $$\n注意这里的优化变量是相机位姿 $\\boldsymbol{\\xi}$。为了求解这个优化问题,我们关心误差 e 是如何 随着相机位姿 $\\boldsymbol{\\xi}$变化的,需要分析它们的导数关系。因此,使用李代数上的扰动模型。我 们给 $exp(\\xi)$ 左乘一个小扰动$ exp(\\delta \\xi)$,得:\n$$ \\begin{aligned} e(\\boldsymbol{\\xi} \\oplus \\delta \\boldsymbol{\\xi}) \u0026=\\boldsymbol{I}_{1}\\left(\\frac{1}{Z_{1}} \\boldsymbol{K} \\boldsymbol{P}\\right)-\\boldsymbol{I}_{2}\\left(\\frac{1}{Z_{2}} \\boldsymbol{K} \\exp \\left(\\delta \\boldsymbol{\\xi}^{\\wedge}\\right) \\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right) \\boldsymbol{P}\\right) \\\\ \u0026 \\approx \\boldsymbol{I}_{1}\\left(\\frac{1}{Z_{1}} \\boldsymbol{K} \\boldsymbol{P}\\right)-\\boldsymbol{I}_{2}\\left(\\frac{1}{Z_{2}} \\boldsymbol{K}\\left(1+\\delta \\boldsymbol{\\xi}^{\\wedge}\\right) \\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right) \\boldsymbol{P}\\right) \\\\ \u0026=\\boldsymbol{I}_{1}\\left(\\frac{1}{Z_{1}} \\boldsymbol{K} \\boldsymbol{P}\\right)-\\boldsymbol{I}_{2}\\left(\\frac{1}{Z_{2}} \\boldsymbol{K} \\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right) \\boldsymbol{P}+\\frac{1}{Z_{2}} \\boldsymbol{K} \\delta \\boldsymbol{\\xi}^{\\wedge} \\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right) \\boldsymbol{P}\\right) \\end{aligned} $$\n记： $$ \\begin{aligned} \\boldsymbol{q} \u0026=\\delta \\boldsymbol{\\xi}^{\\wedge} \\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right) \\boldsymbol{P} \\ \\boldsymbol{u} \u0026=\\frac{1}{Z_{2}} \\boldsymbol{K} \\boldsymbol{q} \\end{aligned} $$ 这里的 $\\boldsymbol{q}$ 为$\\boldsymbol{ P}$ 在扰动之后,位于第二个相机坐标系下的坐标,而 $u$ 为它的像素坐标。\n利用一阶泰勒展开,有:\n$$ \\begin{aligned} e(\\boldsymbol{\\xi} \\oplus \\delta \\boldsymbol{\\xi}) \u0026=\\boldsymbol{I}_{1}\\left(\\frac{1}{Z_{1}} \\boldsymbol{K} \\boldsymbol{P}\\right)-\\boldsymbol{I}_{2}\\left(\\frac{1}{Z_{2}} \\boldsymbol{K} \\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right) \\boldsymbol{P}+\\boldsymbol{u}\\right) \\\\ \u0026 \\approx \\boldsymbol{I}_{1}\\left(\\frac{1}{Z_{1}} \\boldsymbol{K} \\boldsymbol{P}\\right)-\\boldsymbol{I}_{2}\\left(\\frac{1}{Z_{2}} \\boldsymbol{K} \\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right) \\boldsymbol{P}\\right)-\\frac{\\partial \\boldsymbol{I}_{2}}{\\partial \\boldsymbol{u}} \\frac{\\partial \\boldsymbol{u}}{\\partial \\boldsymbol{q}} \\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{g}} \\frac{\\partial \\boldsymbol{q}}{\\partial \\delta \\xi} \\delta \\boldsymbol{\\xi} \\\\ \u0026=e(\\boldsymbol{\\xi})-\\frac{\\partial \\boldsymbol{I}_{2}}{\\partial \\boldsymbol{u}} \\frac{\\partial \\boldsymbol{u}}{\\partial \\boldsymbol{q}} \\frac{\\partial \\boldsymbol{q}}{\\partial \\delta \\xi} \\delta \\boldsymbol{\\xi} \\end{aligned} $$\n我们看到,一阶导数由于链式法则分成了三项,而这三项都是容易计算的:\n1 . $\\frac{\\partial \\boldsymbol{I}_{2}}{\\partial \\boldsymbol{u}}$ 为 $u$ 处的像素梯度;\n2 . $\\frac{\\partial \\boldsymbol{u}}{\\partial \\boldsymbol{q}}$ 为投影方程关于相机坐标系下的三维点的导数。记 $q = [X, Y, Z] ^T$ ,根据上一 节的推导,导数为:\n$$ \\frac{\\partial \\boldsymbol{u}}{\\partial \\boldsymbol{q}}=\\left[\\begin{array}{ccc}{\\frac{\\partial u}{\\partial X}} \u0026 {\\frac{\\partial u}{\\partial Y}} \u0026 {\\frac{\\partial u}{\\partial Z}} \\\\ {\\frac{\\partial v}{\\partial X}} \u0026 {\\frac{\\partial v}{\\partial Y}} \u0026 {\\frac{\\partial v}{\\partial Z}}\\end{array}\\right]=\\left[\\begin{array}{ccc}{\\frac{f_{x}}{Z}} \u0026 {0} \u0026 {-\\frac{f_{x} X}{Z^{2}}} \\\\ {0} \u0026 {\\frac{f_{y}}{Z}} \u0026 {-\\frac{f_{y} Y}{Z^{2}}}\\end{array}\\right] $$\n3 . $\\frac{\\partial \\boldsymbol{q}}{\\partial \\delta \\xi}$ 为变换后的三维点对变换的导数:\n$$ \\frac{\\partial \\boldsymbol{q}}{\\partial \\delta \\boldsymbol{\\xi}}=\\left[\\boldsymbol{I},-\\boldsymbol{q}^{\\wedge}\\right] $$\n由于后两项只与三维点 q 有关,而与图像无关,我们经常把它合并在一起:\n$$ \\frac{\\partial u}{\\partial \\delta \\xi}=\\left[\\begin{array}{cccccc}{\\frac{f_{x}}{Z}} \u0026 {0} \u0026 {-\\frac{f_{x} X}{Z^{2}}} \u0026 {-\\frac{f_{x} X Y}{Z^{2}}} \u0026 {f_{x}+\\frac{f_{x} X^{2}}{Z^{2}}} \u0026 {-\\frac{f_{x} Y}{Z}} \\\\ {0} \u0026 {\\frac{f_{y}}{Z}} \u0026 {-\\frac{f_{y} Y}{Z^{2}}} \u0026 {-f_{y}-\\frac{f_{y} Y^{2}}{Z^{2}}} \u0026 {\\frac{f_{y} X Y}{Z^{2}}} \u0026 {\\frac{f_{y} X}{Z}}\\end{array}\\right] $$ 于是,我们推导了误差相对于李代数的雅可 比矩阵: $$ \\boldsymbol{J}=-\\frac{\\partial \\boldsymbol{I}_{2}}{\\partial \\boldsymbol{u}} \\frac{\\partial \\boldsymbol{u}}{\\partial \\boldsymbol{\\xi}} $$ 对于 N 个点的问题,我们可以用这种方法计算优化问题的雅可比,然后使用 G-N 或 L-M 计算增量,迭代求解。\n直接法的分类 P 是一个已知位置的空间点，根据 P 的来源,我们可以把直接法进行分类:\nP 来自于稀疏关键点,我们称之为稀疏直接法。通常我们使用数百个至上千个关键 点,并且像 L-K 光流那样,假设它周围像素也是不变的。这种稀疏直接法不必计算 描述子,并且只使用数百个像素,因此速度最快,但只能计算稀疏的重构。\nP 来自部分像素。我们看到式(8.16)中,如果像素梯度为零,整一项雅可比就为零, 不会对计算运动增量有任何贡献。因此,可以考虑只使用带有梯度的像素点,舍弃像 素梯度不明显的地方。这称之为半稠密(Semi-Dense)的直接法,可以重构一个半稠 密结构。\nP 为所有像素,称为稠密直接法。稠密重构需要计算所有像素(一般几十万至几百万 个),因此多数不能在现有的 CPU 上实时计算,需要 GPU 的加速。但是,如前面 所讨论的,梯度不明显的点,在运动估计中不会有太大贡献,在重构时也会难以估计 位置。\n代码实践 稀疏直接法 在使用 g2o之前,需要把直接法抽象成一个图优化问题。显然,直接法是由以下顶点和边组成的:\n优化变量为一个相机位姿,因此需要一个位姿顶点。由于我们在推导中使用了李代 数,故程序中使用李代数表达的 SE(3) 位姿顶点。与上一章一样,我们将使用“Ver- texSE3Expmap”作为相机位姿。\n误差项为单个像素的光度误差。由于整个优化过程中 I 1 (p 1 ) 保持不变,我们可以把 它当成一个固定的预设值,然后调整相机位姿,使 I 2 (p 2 ) 接近这个值。于是,这种 边只连接一个顶点,为一元边。由于 g2o 中本身没有计算光度误差的边,我们需要 自己定义一种新的边。\n在上述的建模中,直接法图优化问题是由一个相机位姿顶点与许多条一元边组成的。 如果使用稀疏的直接法,那我们大约会有几百至几千条这样的边;稠密直接法则会有几十 万条边。优化问题对应的线性方程是计算李代数增量,本身规模不大(6×6) ,所以主要的 计算时间会花费在每条边的误差与雅可比的计算上。下面的实验中,我们先来定义一种用 于直接法位姿估计的边,然后,使用该边构建图优化问题并求解之。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 #include \u003ciostream\u003e #include \u003cfstream\u003e #include \u003clist\u003e #include \u003cvector\u003e #include \u003cchrono\u003e #include \u003cctime\u003e #include \u003cclimits\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003cg2o/core/base_unary_edge.h\u003e #include \u003cg2o/core/block_solver.h\u003e #include \u003cg2o/core/optimization_algorithm_levenberg.h\u003e #include \u003cg2o/solvers/dense/linear_solver_dense.h\u003e #include \u003cg2o/core/robust_kernel.h\u003e #include \u003cg2o/types/sba/types_six_dof_expmap.h\u003e using namespace std; using namespace g2o; /******************************************** * 本节演示了RGBD上的稀疏直接法 ********************************************/ // 一次测量的值，包括一个世界坐标系下三维点与一个灰度值 struct Measurement { Measurement ( Eigen::Vector3d p, float g ) : pos_world ( p ), grayscale ( g ) {} Eigen::Vector3d pos_world; float grayscale; }; inline Eigen::Vector3d project2Dto3D ( int x, int y, int d, float fx, float fy, float cx, float cy, float scale ) { float zz = float ( d ) /scale; float xx = zz* ( x-cx ) /fx; float yy = zz* ( y-cy ) /fy; return Eigen::Vector3d ( xx, yy, zz ); } inline Eigen::Vector2d project3Dto2D ( float x, float y, float z, float fx, float fy, float cx, float cy ) { float u = fx*x/z+cx; float v = fy*y/z+cy; return Eigen::Vector2d ( u,v ); } // 直接法估计位姿 // 输入：测量值（空间点的灰度），新的灰度图，相机内参； 输出：相机位姿 // 返回：true为成功，false失败 bool poseEstimationDirect ( const vector\u003cMeasurement\u003e\u0026 measurements, cv::Mat* gray, Eigen::Matrix3f\u0026 intrinsics, Eigen::Isometry3d\u0026 Tcw ); // project a 3d point into an image plane, the error is photometric error // an unary edge with one vertex SE3Expmap (the pose of camera) class EdgeSE3ProjectDirect: public BaseUnaryEdge\u003c 1, double, VertexSE3Expmap\u003e { public: EIGEN_MAKE_ALIGNED_OPERATOR_NEW EdgeSE3ProjectDirect() {} EdgeSE3ProjectDirect ( Eigen::Vector3d point, float fx, float fy, float cx, float cy, cv::Mat* image ) : x_world_ ( point ), fx_ ( fx ), fy_ ( fy ), cx_ ( cx ), cy_ ( cy ), image_ ( image ) {} //虚函数重写 误差项 virtual void computeError() { const VertexSE3Expmap* v =static_cast\u003cconst VertexSE3Expmap*\u003e ( _vertices[0] ); Eigen::Vector3d x_local = v-\u003eestimate().map ( x_world_ ); float x = x_local[0]*fx_/x_local[2] + cx_; float y = x_local[1]*fy_/x_local[2] + cy_; // check x,y is in the image if ( x-4\u003c0 || ( x+4 ) \u003eimage_-\u003ecols || ( y-4 ) \u003c0 || ( y+4 ) \u003eimage_-\u003erows ) { _error ( 0,0 ) = 0.0; this-\u003esetLevel ( 1 ); } else { _error ( 0,0 ) = getPixelValue ( x,y ) - _measurement; } } // plus in manifold virtual void linearizeOplus( ) { if ( level() == 1 ) { _jacobianOplusXi = Eigen::Matrix\u003cdouble, 1, 6\u003e::Zero(); return; } VertexSE3Expmap* vtx = static_cast\u003cVertexSE3Expmap*\u003e ( _vertices[0] ); Eigen::Vector3d xyz_trans = vtx-\u003eestimate().map ( x_world_ ); // q in book double x = xyz_trans[0]; double y = xyz_trans[1]; double invz = 1.0/xyz_trans[2]; double invz_2 = invz*invz; float u = x*fx_*invz + cx_; float v = y*fy_*invz + cy_; // jacobian from se3 to u,v // NOTE that in g2o the Lie algebra is (\\omega, \\epsilon), where \\omega is so(3) and \\epsilon the translation Eigen::Matrix\u003cdouble, 2, 6\u003e jacobian_uv_ksai; jacobian_uv_ksai ( 0,0 ) = - x*y*invz_2 *fx_; jacobian_uv_ksai ( 0,1 ) = ( 1+ ( x*x*invz_2 ) ) *fx_; jacobian_uv_ksai ( 0,2 ) = - y*invz *fx_; jacobian_uv_ksai ( 0,3 ) = invz *fx_; jacobian_uv_ksai ( 0,4 ) = 0; jacobian_uv_ksai ( 0,5 ) = -x*invz_2 *fx_; jacobian_uv_ksai ( 1,0 ) = - ( 1+y*y*invz_2 ) *fy_; jacobian_uv_ksai ( 1,1 ) = x*y*invz_2 *fy_; jacobian_uv_ksai ( 1,2 ) = x*invz *fy_; jacobian_uv_ksai ( 1,3 ) = 0; jacobian_uv_ksai ( 1,4 ) = invz *fy_; jacobian_uv_ksai ( 1,5 ) = -y*invz_2 *fy_; Eigen::Matrix\u003cdouble, 1, 2\u003e jacobian_pixel_uv; jacobian_pixel_uv ( 0,0 ) = ( getPixelValue ( u+1,v )-getPixelValue ( u-1,v ) ) /2; jacobian_pixel_uv ( 0,1 ) = ( getPixelValue ( u,v+1 )-getPixelValue ( u,v-1 ) ) /2; //雅克比矩阵 _jacobianOplusXi = jacobian_pixel_uv*jacobian_uv_ksai; } // dummy read and write functions because we don't care... virtual bool read ( std::istream\u0026 in ) {} virtual bool write ( std::ostream\u0026 out ) const {} protected: // get a gray scale value from reference image (bilinear interpolated) inline float getPixelValue ( float x, float y ) { uchar* data = \u0026 image_-\u003edata[ int ( y ) * image_-\u003estep + int ( x ) ]; float xx = x - floor ( x ); float yy = y - floor ( y ); return float ( ( 1-xx ) * ( 1-yy ) * data[0] + xx* ( 1-yy ) * data[1] + ( 1-xx ) *yy*data[ image_-\u003estep ] + xx*yy*data[image_-\u003estep+1] ); } public: Eigen::Vector3d x_world_; // 3D point in world frame float cx_=0, cy_=0, fx_=0, fy_=0; // Camera intrinsics cv::Mat* image_=nullptr; // reference image }; int main ( int argc, char** argv ) { if ( argc != 2 ) { cout\u003c\u003c\"usage: useLK path_to_dataset\"\u003c\u003cendl; return 1; } srand ( ( unsigned int ) time ( 0 ) ); string path_to_dataset = argv[1]; string associate_file = path_to_dataset + \"/associate.txt\"; ifstream fin ( associate_file ); string rgb_file, depth_file, time_rgb, time_depth; cv::Mat color, depth, gray; vector\u003cMeasurement\u003e measurements; // 相机内参 float cx = 325.5; float cy = 253.5; float fx = 518.0; float fy = 519.0; float depth_scale = 1000.0; Eigen::Matrix3f K; K\u003c\u003cfx,0.f,cx,0.f,fy,cy,0.f,0.f,1.0f; Eigen::Isometry3d Tcw = Eigen::Isometry3d::Identity(); cv::Mat prev_color; // 我们以第一个图像为参考，对后续图像和参考图像做直接法 for ( int index=0; index\u003c10; index++ ) { cout\u003c\u003c\"*********** loop \"\u003c\u003cindex\u003c\u003c\" ************\"\u003c\u003cendl; fin\u003e\u003etime_rgb\u003e\u003ergb_file\u003e\u003etime_depth\u003e\u003edepth_file; color = cv::imread ( path_to_dataset+\"/\"+rgb_file ); depth = cv::imread ( path_to_dataset+\"/\"+depth_file, -1 ); if ( color.data==nullptr || depth.data==nullptr ) continue; cv::cvtColor ( color, gray, cv::COLOR_BGR2GRAY ); if ( index ==0 ) { // 对第一帧提取FAST特征点 vector\u003ccv::KeyPoint\u003e keypoints; cv::Ptr\u003ccv::FastFeatureDetector\u003e detector = cv::FastFeatureDetector::create(); detector-\u003edetect ( color, keypoints ); for ( auto kp:keypoints ) { // 去掉邻近边缘处的点 if ( kp.pt.x \u003c 20 || kp.pt.y \u003c 20 || ( kp.pt.x+20 ) \u003ecolor.cols || ( kp.pt.y+20 ) \u003ecolor.rows ) continue; ushort d = depth.ptr\u003cushort\u003e ( cvRound ( kp.pt.y ) ) [ cvRound ( kp.pt.x ) ]; if ( d==0 ) continue; //获取第一帧特征点的世界坐标做参考坐标，后面帧的R,t都是相对于第一帧来讲 Eigen::Vector3d p3d = project2Dto3D ( kp.pt.x, kp.pt.y, d, fx, fy, cx, cy, depth_scale ); float grayscale = float ( gray.ptr\u003cuchar\u003e ( cvRound ( kp.pt.y ) ) [ cvRound ( kp.pt.x ) ] ); measurements.push_back ( Measurement ( p3d, grayscale ) ); } prev_color = color.clone(); continue; } // 使用直接法计算相机运动 chrono::steady_clock::time_point t1 = chrono::steady_clock::now(); poseEstimationDirect ( measurements, \u0026gray, K, Tcw ); chrono::steady_clock::time_point t2 = chrono::steady_clock::now(); chrono::duration\u003cdouble\u003e time_used = chrono::duration_cast\u003cchrono::duration\u003cdouble\u003e\u003e ( t2-t1 ); cout\u003c\u003c\"direct method costs time: \"\u003c\u003ctime_used.count() \u003c\u003c\" seconds.\"\u003c\u003cendl; cout\u003c\u003c\"Tcw=\"\u003c\u003cTcw.matrix() \u003c\u003cendl; // plot the feature points cv::Mat img_show ( color.rows*2, color.cols, CV_8UC3 ); prev_color.copyTo ( img_show ( cv::Rect ( 0,0,color.cols, color.rows ) ) ); color.copyTo ( img_show ( cv::Rect ( 0,color.rows,color.cols, color.rows ) ) ); for ( Measurement m:measurements ) { if ( rand() \u003e RAND_MAX/5 ) continue; Eigen::Vector3d p = m.pos_world; Eigen::Vector2d pixel_prev = project3Dto2D ( p ( 0,0 ), p ( 1,0 ), p ( 2,0 ), fx, fy, cx, cy ); Eigen::Vector3d p2 = Tcw*m.pos_world; Eigen::Vector2d pixel_now = project3Dto2D ( p2 ( 0,0 ), p2 ( 1,0 ), p2 ( 2,0 ), fx, fy, cx, cy ); if ( pixel_now(0,0)\u003c0 || pixel_now(0,0)\u003e=color.cols || pixel_now(1,0)\u003c0 || pixel_now(1,0)\u003e=color.rows ) continue; float b = 255*float ( rand() ) /RAND_MAX; float g = 255*float ( rand() ) /RAND_MAX; float r = 255*float ( rand() ) /RAND_MAX; cv::circle ( img_show, cv::Point2d ( pixel_prev ( 0,0 ), pixel_prev ( 1,0 ) ), 8, cv::Scalar ( b,g,r ), 2 ); cv::circle ( img_show, cv::Point2d ( pixel_now ( 0,0 ), pixel_now ( 1,0 ) +color.rows ), 8, cv::Scalar ( b,g,r ), 2 ); cv::line ( img_show, cv::Point2d ( pixel_prev ( 0,0 ), pixel_prev ( 1,0 ) ), cv::Point2d ( pixel_now ( 0,0 ), pixel_now ( 1,0 ) +color.rows ), cv::Scalar ( b,g,r ), 1 ); } cv::imshow ( \"result\", img_show ); cv::waitKey ( 0 ); } return 0; } bool poseEstimationDirect ( const vector\u003c Measurement \u003e\u0026 measurements, cv::Mat* gray, Eigen::Matrix3f\u0026 K, Eigen::Isometry3d\u0026 Tcw ) { // 初始化g2o typedef g2o::BlockSolver\u003cg2o::BlockSolverTraits\u003c6,1\u003e\u003e DirectBlock; // 求解的向量是6＊1的 DirectBlock::LinearSolverType* linearSolver = new g2o::LinearSolverDense\u003c DirectBlock::PoseMatrixType \u003e (); DirectBlock* solver_ptr = new DirectBlock ( linearSolver ); // g2o::OptimizationAlgorithmGaussNewton* solver = new g2o::OptimizationAlgorithmGaussNewton( solver_ptr ); // G-N g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg ( solver_ptr ); // L-M g2o::SparseOptimizer optimizer; optimizer.setAlgorithm ( solver ); optimizer.setVerbose( true ); g2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap(); pose-\u003esetEstimate ( g2o::SE3Quat ( Tcw.rotation(), Tcw.translation() ) ); pose-\u003esetId ( 0 ); optimizer.addVertex ( pose ); // 添加边 int id=1; for ( Measurement m: measurements ) { EdgeSE3ProjectDirect* edge = new EdgeSE3ProjectDirect ( m.pos_world, K ( 0,0 ), K ( 1,1 ), K ( 0,2 ), K ( 1,2 ), gray ); edge-\u003esetVertex ( 0, pose ); edge-\u003esetMeasurement ( m.grayscale ); edge-\u003esetInformation ( Eigen::Matrix\u003cdouble,1,1\u003e::Identity() ); edge-\u003esetId ( id++ ); optimizer.addEdge ( edge ); } cout\u003c\u003c\"edges in graph: \"\u003c\u003coptimizer.edges().size() \u003c\u003cendl; optimizer.initializeOptimization(); optimizer.optimize ( 30 ); Tcw = pose-\u003eestimate(); } 我们的边继承自 g2o::BaseUnaryEdge。在继承时,需要在模板参数里填入测量值的维 度、类型,以及连接此边的顶点,同时,我们把空间点 P 、相机内参和图像存储在该边的成 员变量中。为了让 g2o 优化该边对应的误差,我们需要覆写两个虚函数:用 computeError() 计算误差值,用 linearizeOplus() 计算雅可比。可以看到,这里的雅可比计算与式(8.16) 是一致的。注意我们在程序中的误差计算里,使用了 I 2 (p 2 ) − I 1 (p 1 ) 的形式,因此前面的 负号可以省去,只需把像素梯度乘以像素到李代数的梯度即可。\n半稠密 修改的部分：对参考帧中,先提取梯度较明显 的像素,然后用直接法,以这些像素为图优化边,来估计相机运动。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // select the pixels with high gradiants for ( int x=10; x\u003cgray.cols-10; x++ ) for ( int y=10; y\u003cgray.rows-10; y++ ) { Eigen::Vector2d delta ( gray.ptr\u003cuchar\u003e(y)[x+1] - gray.ptr\u003cuchar\u003e(y)[x-1], gray.ptr\u003cuchar\u003e(y+1)[x] - gray.ptr\u003cuchar\u003e(y-1)[x] ); if ( delta.norm() \u003c 50 ) continue; ushort d = depth.ptr\u003cushort\u003e (y)[x]; if ( d==0 ) continue; Eigen::Vector3d p3d = project2Dto3D ( x, y, d, fx, fy, cx, cy, depth_scale ); float grayscale = float ( gray.ptr\u003cuchar\u003e (y) [x] ); measurements.push_back ( Measurement ( p3d, grayscale ) ); } ","title":"视觉里程计2","uri":"/contents/slam-sfm/vo-2/"},{"categories":["contents"],"content":"利用pytorch搭建一个图像分类网络。\n如何处理数据 当处理图像，文本，语音或者视频数据时，你可以使用标准 python 包将数据加载成 numpy 数组格式，然后将这个数组转换成 torch.*Tensor\n对于图像，可以用 Pillow，OpenCV 对于语音，可以用 scipy，librosa 对于文本，可以直接用 Python 或 Cython 基础数据加载模块，或者用 NLTK 和 SpaCy 特别是对于视觉，我们已经创建了一个叫做 torchvision 的包，该包含有支持加载类似Imagenet，CIFAR10，MNIST 等公共数据集的数据加载模块 torchvision.datasets 和支持加载图像数据数据转换模块 torch.utils.data.DataLoader。\n训练一个图像分类器 我们将按次序的做如下几步：\n使用torchvision加载并且归一化CIFAR10的训练和测试数据集 定义一个卷积神经网络 定义一个损失函数 在训练样本数据上训练网络 在测试样本数据上测试网络 torchvision 数据集的输出是范围在[0,1]之间的 PILImage，我们将他们转换成归一化范围为[-1,1]之间的张量 Tensors。\n使用torchvision加载并且归一化CIFAR10的训练和测试数据集 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import torch import torchvision import torchvision.transforms as transforms import matplotlib.pyplot as plt import numpy as np transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') # functions to show an image def imshow(img): img = img / 2 + 0.5 # unnormalize 将[-1,1]转换为[0,1]，用plt显示 npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() # get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() # show images imshow(torchvision.utils.make_grid(images)) # print labels print(' '.join('%5s' % classes[labels[j]] for j in range(4))) 定义一个卷积神经网络 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() 定义一个损失函数和优化器 让我们使用分类交叉熵Cross-Entropy 作损失函数，动量SGD做优化器。\n1 2 3 4 import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 在训练样本数据上训练网络 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training') 在测试样本数据上测试网络 我们将用神经网络的输出作为预测的类标来检查网络的预测性能，用样本的真实类标来校对。如果预测是正确的，我们将样本添加到正确预测的列表里。\n1 2 3 4 5 6 7 8 9 10 11 12 correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 每个类别的准确率：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i].item() class_total[label] += 1 for i in range(10): print('Accuracy of %5s : %2d %%' % ( classes[i], 100 * class_correct[i] / class_total[i])) 怎么在GPU上跑这些神经网络？ 在GPU上训练 就像你怎么把一个张量转移到GPU上一样，你要将神经网络转到GPU上。 如果CUDA可以用，让我们首先定义下我们的设备为第一个可见的cuda设备。\n1 2 3 4 5 device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Assume that we are on a CUDA machine, then this should print a CUDA device: print(device) 接着这些方法会递归地遍历所有模块，并将它们的参数和缓冲器转换为CUDA张量。\n1 net.to(device) 记住你也必须在每一个步骤向GPU发送输入和目标：\n1 inputs, labels = inputs.to(device), labels.to(device) 完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 import torch import torchvision import torchvision.transforms as transforms import matplotlib.pyplot as plt import numpy as np import torch.nn as nn import torch.nn.functional as F import torch.optim as optim transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') # functions to show an image def imshow(img): img = img / 2 + 0.5 # unnormalize 将[-1,1]转换为[0,1]，用plt显示 npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() # get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() # show images imshow(torchvision.utils.make_grid(images)) # print labels print(' '.join('%5s' % classes[labels[j]] for j in range(4))) class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training') # Test correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) ","title":"pytorch图像分类","uri":"/contents/dl/pytorch%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"},{"categories":["contents"],"content":"记录pytorch进行迁移学习的两种思路。\n迁移学习的两个主要场景： 微调Convnet：使用预训练的网络(如在imagenet 1000上训练而来的网络)来初始化自己的网络，而不是随机初始化。其他的训练步骤不变。 将Convnet看成固定的特征提取器:首先固定ConvNet除了最后的全连接层外的其他所有层。最后的全连接层被替换成一个新的随机 初始化的层，只有这个新的层会被训练[只有这层参数会在反向传播时更新] 下面是利用PyTorch进行迁移学习步骤，要解决的问题是训练一个模型来对蚂蚁和蜜蜂进行分类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 # License: BSD # Author: Sasank Chilamkurthy from __future__ import print_function, division import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os import copy plt.ion() # interactive mode # 训练集数据扩充和归一化 # 在验证集上仅需要归一化 data_transforms = { 'train': transforms.Compose([ transforms.RandomResizedCrop(224), #随机裁剪一个area然后再resize transforms.RandomHorizontalFlip(), #随机水平翻转 transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = '/home/jiajie/Downloads/data/hymenoptera_data' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']} dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']} class_names = image_datasets['train'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") def imshow(inp, title=None): \"\"\"Imshow for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # 获取一批训练数据 inputs, classes = next(iter(dataloaders['train'])) # 批量制作网格 out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) ''' 编写一个通用函数来训练模型。下面将说明： 调整学习速率 保存最好的模型 下面的参数scheduler是一个来自 torch.optim.lr_scheduler的学习速率调整类的对象(LR scheduler object)。 ''' def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) print('-' * 10) # 每个epoch都有一个训练和验证阶段 for phase in ['train', 'val']: if phase == 'train': scheduler.step() model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # 迭代数据. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # 零参数梯度 optimizer.zero_grad() # 前向 # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # 后向+仅在训练阶段进行优化 if phase == 'train': loss.backward() optimizer.step() # 统计 running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc)) # 深度复制mo if phase == 'val' and epoch_acc \u003e best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print('Training complete in {:.0f}m {:.0f}s'.format( time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: {:4f}'.format(best_acc)) # 加载最佳模型权重 model.load_state_dict(best_model_wts) return model #一个通用的展示少量预测图片的函数 def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders['val']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis('off') ax.set_title('predicted: {}'.format(class_names[preds[j]])) imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) ''' # 场景1：微调ConvNet model_ft = models.resnet18(pretrained=True) num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # 观察所有参数都正在优化 optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # 每7个epochs衰减LR通过设置gamma=0.1 exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) # 模型评估效果可视化 visualize_model(model_ft) plt.ioff() plt.show() ''' # 场景2：ConvNet作为固定特征提取器 model_conv = torchvision.models.resnet18(pretrained=True) for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) visualize_model(model_conv) plt.ioff() plt.show() ","title":"pytorch迁移学习","uri":"/contents/dl/pytorch%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"},{"categories":["contents"],"content":"介绍pytorch的一些好用的特性。\nPyTorch：定义新的自动求导函数 在底层，每一个原始的自动求导运算实际上是两个在Tensor上运行的函数。其中，forward函数计算从输入Tensors获得的输出Tensors。而backward函数接收输出Tensors对于某个标量值的梯度，并且计算输入Tensors相对于该相同标量值的梯度。\n在PyTorch中，我们可以很容易地通过定义torch.autograd.Function的子类并实现forward和backward函数，来定义自己的自动求导运算。之后我们就可以使用这个新的自动梯度运算符了。然后，我们可以通过构造一个实例并像调用函数一样，传入包含输入数据的tensor调用它，这样来使用新的自动求导运算。\n这个例子中，我们自定义一个自动求导函数来展示ReLU的非线性。并用它实现我们的两层网络：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 import torch class MyReLU(torch.autograd.Function): \"\"\" 我们可以通过建立torch.autograd的子类来实现我们自定义的autograd函数， 并完成张量的正向和反向传播。 \"\"\" @staticmethod def forward(ctx, x): \"\"\" 在正向传播中，我们接收到一个上下文对象和一个包含输入的张量； 我们必须返回一个包含输出的张量， 并且我们可以使用上下文对象来缓存对象，以便在反向传播中使用。 \"\"\" ctx.save_for_backward(x) return x.clamp(min=0) @staticmethod def backward(ctx, grad_output): \"\"\" 在反向传播中，我们接收到上下文对象和一个张量， 其包含了相对于正向传播过程中产生的输出的损失的梯度。 我们可以从上下文对象中检索缓存的数据， 并且必须计算并返回与正向传播的输入相关的损失的梯度。 \"\"\" x, = ctx.saved_tensors grad_x = grad_output.clone() grad_x[x \u003c 0] = 0 return grad_x device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # N是批大小； D_in 是输入维度； # H 是隐藏层维度； D_out 是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生输入和输出的随机张量 x = torch.randn(N, D_in, device=device) y = torch.randn(N, D_out, device=device) # 产生随机权重的张量 w1 = torch.randn(D_in, H, device=device, requires_grad=True) w2 = torch.randn(H, D_out, device=device, requires_grad=True) learning_rate = 1e-6 for t in range(500): # 正向传播：使用张量上的操作来计算输出值y； # 我们通过调用 MyReLU.apply 函数来使用自定义的ReLU y_pred = MyReLU.apply(x.mm(w1)).mm(w2) # 计算并输出loss loss = (y_pred - y).pow(2).sum() print(t, loss.item()) # 使用autograd计算反向传播过程。 loss.backward() with torch.no_grad(): # 用梯度下降更新权重 w1 -= learning_rate * w1.grad w2 -= learning_rate * w2.grad # 在反向传播之后手动清零梯度 w1.grad.zero_() w2.grad.zero_() nn模块 PyTorch：nn 计算图和autograd是十分强大的工具，可以定义复杂的操作并自动求导；然而对于大规模的网络，autograd太过于底层。 在构建神经网络时，我们经常考虑将计算安排成层，其中一些具有可学习的参数，它们将在学习过程中进行优化。\nTensorFlow里，有类似Keras，TensorFlow-Slim和TFLearn这种封装了底层计算图的高度抽象的接口，这使得构建网络十分方便。\n在PyTorch中，包nn完成了同样的功能。nn包中定义一组大致等价于层的模块。一个模块接受输入的tesnor，计算输出的tensor，而且 还保存了一些内部状态比如需要学习的tensor的参数等。nn包中也定义了一组损失函数（loss functions），用来训练神经网络。\n这个例子中，我们用nn包实现两层的网络：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # -*- coding: utf-8 -*- import torch # N是批大小；D是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 #创建输入和输出随机张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 使用nn包将我们的模型定义为一系列的层。 # nn.Sequential是包含其他模块的模块，并按顺序应用这些模块来产生其输出。 # 每个线性模块使用线性函数从输入计算输出，并保存其内部的权重和偏差张量。 # 在构造模型之后，我们使用.to()方法将其移动到所需的设备。 model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) # nn包还包含常用的损失函数的定义； # 在这种情况下，我们将使用平均平方误差(MSE)作为我们的损失函数。 # 设置reduction='sum'，表示我们计算的是平方误差的“和”，而不是平均值; # 这是为了与前面我们手工计算损失的例子保持一致， # 但是在实践中，通过设置reduction='elementwise_mean'来使用均方误差作为损失更为常见。 loss_fn = torch.nn.MSELoss(reduction='sum') learning_rate = 1e-4 for t in range(500): # 前向传播：通过向模型传入x计算预测的y。 # 模块对象重载了__call__运算符，所以可以像函数那样调用它们。 # 这么做相当于向模块传入了一个张量，然后它返回了一个输出张量。 y_pred = model(x) # 计算并打印损失。 # 传递包含y的预测值和真实值的张量，损失函数返回包含损失的张量。 loss = loss_fn(y_pred, y) print(t, loss.item()) # 反向传播之前清零梯度 model.zero_grad() # 反向传播：计算模型的损失对所有可学习参数的导数（梯度）。 # 在内部，每个模块的参数存储在requires_grad=True的张量中， # 因此这个调用将计算模型中所有可学习参数的梯度。 loss.backward() # 使用梯度下降更新权重。 # 每个参数都是张量，所以我们可以像我们以前那样可以得到它的数值和梯度 with torch.no_grad(): for param in model.parameters(): param -= learning_rate * param.grad PyTorch：optim 到目前为止，我们已经通过手动改变包含可学习参数的张量来更新模型的权重。对于随机梯度下降(SGD/stochastic gradient descent)等简单的优化算法来说，这不是一个很大的负担，但在实践中，我们经常使用AdaGrad、RMSProp、Adam等更复杂的优化器来训练神经网络。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import torch # N是批大小；D是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生随机输入和输出张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 使用nn包定义模型和损失函数 model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) loss_fn = torch.nn.MSELoss(reduction='sum') # 使用optim包定义优化器（Optimizer）。Optimizer将会为我们更新模型的权重。 # 这里我们使用Adam优化方法；optim包还包含了许多别的优化算法。 # Adam构造函数的第一个参数告诉优化器应该更新哪些张量。 learning_rate = 1e-4 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) for t in range(500): # 前向传播：通过像模型输入x计算预测的y y_pred = model(x) # 计算并打印loss loss = loss_fn(y_pred, y) print(t, loss.item()) # 在反向传播之前，使用optimizer将它要更新的所有张量的梯度清零(这些张量是模型可学习的权重) optimizer.zero_grad() # 反向传播：根据模型的参数计算loss的梯度 loss.backward() # 调用Optimizer的step函数使它所有参数更新 optimizer.step() PyTorch：自定义nn模块 有时候需要指定比现有模块序列更复杂的模型；对于这些情况，可以通过继承nn.Module并定义forward函数，这个forward函数可以 使用其他模块或者其他的自动求导运算来接收输入tensor，产生输出tensor。\n在这个例子中，我们用自定义Module的子类构建两层网络：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import torch class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" 在构造函数中，我们实例化了两个nn.Linear模块，并将它们作为成员变量。 \"\"\" super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" 在前向传播的函数中，我们接收一个输入的张量，也必须返回一个输出张量。 我们可以使用构造函数中定义的模块以及张量上的任意的（可微分的）操作。 \"\"\" h_relu = self.linear1(x).clamp(min=0) y_pred = self.linear2(h_relu) return y_pred # N是批大小； D_in 是输入维度； # H 是隐藏层维度； D_out 是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生输入和输出的随机张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 通过实例化上面定义的类来构建我们的模型。 model = TwoLayerNet(D_in, H, D_out) # 构造损失函数和优化器。 # SGD构造函数中对model.parameters()的调用， # 将包含模型的一部分，即两个nn.Linear模块的可学习参数。 loss_fn = torch.nn.MSELoss(reduction='sum') optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) for t in range(500): # 前向传播：通过向模型传递x计算预测值y y_pred = model(x) #计算并输出loss loss = loss_fn(y_pred, y) print(t, loss.item()) # 清零梯度，反向传播，更新权重 optimizer.zero_grad() loss.backward() optimizer.step() PyTorch：控制流和权重共享 作为动态图和权重共享的一个例子，我们实现了一个非常奇怪的模型：一个全连接的ReLU网络，在每一次前向传播时，它的隐藏层的层数为随机1到4之间的数，这样可以多次重用相同的权重来计算。\n因为这个模型可以使用普通的Python流控制来实现循环，并且我们可以通过在定义转发时多次重用同一个模块来实现最内层之间的权重共享。\n我们利用Mudule的子类很容易实现这个模型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import random import torch class DynamicNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" 在构造函数中，我们构造了三个nn.Linear实例，它们将在前向传播时被使用。 \"\"\" super(DynamicNet, self).__init__() self.input_linear = torch.nn.Linear(D_in, H) self.middle_linear = torch.nn.Linear(H, H) self.output_linear = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" 对于模型的前向传播，我们随机选择0、1、2、3， 并重用了多次计算隐藏层的middle_linear模块。 由于每个前向传播构建一个动态计算图， 我们可以在定义模型的前向传播时使用常规Python控制流运算符，如循环或条件语句。 在这里，我们还看到，在定义计算图形时多次重用同一个模块是完全安全的。 这是Lua Torch的一大改进，因为Lua Torch中每个模块只能使用一次。 \"\"\" h_relu = self.input_linear(x).clamp(min=0) for _ in range(random.randint(0, 3)): h_relu = self.middle_linear(h_relu).clamp(min=0) y_pred = self.output_linear(h_relu) return y_pred # N是批大小；D是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生输入和输出随机张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 实例化上面定义的类来构造我们的模型 model = DynamicNet(D_in, H, D_out) # 构造我们的损失函数（loss function）和优化器（Optimizer）。 # 用平凡的随机梯度下降训练这个奇怪的模型是困难的，所以我们使用了momentum方法。 criterion = torch.nn.MSELoss(reduction='sum') optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9) for t in range(500): # 前向传播：通过向模型传入x计算预测的y。 y_pred = model(x) # 计算并打印损失 loss = criterion(y_pred, y) print(t, loss.item()) # 清零梯度，反向传播，更新权重 optimizer.zero_grad() loss.backward() optimizer.step() ","title":"pytorch特性","uri":"/contents/dl/pytorch%E7%89%B9%E6%80%A7/"},{"categories":["contents"],"content":"记录pytorch保存和加载模型的接口函数。\n当保存和加载模型时，需要熟悉三个核心功能：\ntorch.save：将序列化对象保存到磁盘。此函数使用Python的pickle模块进行序列化。使用此函数可以保存如模型、tensor、字典等各种对象。 torch.load：使用pickle的unpickling功能将pickle对象文件反序列化到内存。此功能还可以有助于设备加载数据。 torch.nn.Module.load_state_dict：使用反序列化函数 state_dict 来加载模型的参数字典。 什么是状态字典：state_dict? 在PyTorch中，torch.nn.Module模型的可学习参数（即权重和偏差）包含在模型的参数中，（使用model.parameters()可以进行访问）。 state_dict是Python字典对象，它将每一层映射到其参数张量。注意，只有具有可学习参数的层（如卷积层，线性层等）的模型 才具有state_dict这一项。目标优化torch.optim也有state_dict属性，它包含有关优化器的状态信息，以及使用的超参数。\n因为state_dict的对象是Python字典，所以它们可以很容易的保存、更新、修改和恢复，为PyTorch模型和优化器添加了大量模块。\n下面通过从简单模型训练一个分类器中来了解一下state_dict的使用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F # 定义模型 class TheModelClass(nn.Module): def __init__(self): super(TheModelClass, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # 初始化模型 model = TheModelClass() # 初始化优化器 optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # 打印模型的状态字典 print(\"Model's state_dict:\") for param_tensor in model.state_dict(): print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size()) # 打印优化器的状态字典 print(\"Optimizer's state_dict:\") for var_name in optimizer.state_dict(): print(var_name, \"\\t\", optimizer.state_dict()[var_name]) 保存和加载推理模型 保存 1 torch.save(model.state_dict(), PATH) 加载 1 2 3 model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.eval() 当保存好模型用来推断的时候，只需要保存模型学习到的参数，使用torch.save()函数来保存模型state_dict,它会给模型恢复提供 最大的灵活性，这就是为什么要推荐它来保存的原因。\n在 PyTorch 中最常见的模型保存使‘.pt’或者是‘.pth’作为模型文件扩展名。\n请记住，在运行推理之前，务必调用model.eval()去设置 dropout和 batch normalization 层为评估模式。如果不这么做，可能导致 模型推断结果不一致。\n注意 load_state_dict()函数只接受字典对象，而不是保存对象的路径。这就意味着在你传给load_state_dict()函数之前，你必须反序列化 你保存的state_dict。例如，你无法通过 model.load_state_dict(PATH)来加载模型。 保存/加载完整模型 保存 1 torch.save(model, PATH) 加载 1 2 3 # 模型类必须在此之前被定义 model = torch.load(PATH) model.eval() 此部分保存/加载过程使用最直观的语法并涉及最少量的代码。以 Python pickle 模块的方式来保存模型。这种方法的缺点是序列化数据受 限于某种特殊的类而且需要确切的字典结构。这是因为pickle无法保存模型类本身。相反，它保存包含类的文件的路径，该文件在加载时使用。 因此，当在其他项目使用或者重构之后，您的代码可能会以各种方式中断。\n保存和加载 Checkpoint 用于推理/继续训练 保存 1 2 3 4 5 6 7 torch.save({ 'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, ... }, PATH) 加载 1 2 3 4 5 6 7 8 9 10 11 12 model = TheModelClass(*args, **kwargs) optimizer = TheOptimizerClass(*args, **kwargs) checkpoint = torch.load(PATH) model.load_state_dict(checkpoint['model_state_dict']) optimizer.load_state_dict(checkpoint['optimizer_state_dict']) epoch = checkpoint['epoch'] loss = checkpoint['loss'] model.eval() # - or - model.train() 当保存成 Checkpoint 的时候，可用于推理或者是继续训练，保存的不仅仅是模型的 state_dict 。保存优化器的 state_dict 也很重要, 因为它包含作为模型训练更新的缓冲区和参数。你也许想保存其他项目，比如最新记录的训练损失，外部的torch.nn.Embedding层等等。\n要保存多个组件，请在字典中组织它们并使用torch.save()来序列化字典。PyTorch 中常见的保存checkpoint 是使用 .tar 文件扩展名。\n要加载项目，首先需要初始化模型和优化器，然后使用torch.load()来加载本地字典。这里,你可以非常容易的通过简单查询字典来访问你所保存的项目。\n请记住在运行推理之前，务必调用model.eval()去设置 dropout 和 batch normalization 为评估。如果不这样做，有可能得到不一致的推断结果。 如果你想要恢复训练，请调用model.train()以确保这些层处于训练模式。\n在一个文件中保存多个模型 保存 1 2 3 4 5 6 7 torch.save({ 'modelA_state_dict': modelA.state_dict(), 'modelB_state_dict': modelB.state_dict(), 'optimizerA_state_dict': optimizerA.state_dict(), 'optimizerB_state_dict': optimizerB.state_dict(), ... }, PATH) 加载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 modelA = TheModelAClass(*args, **kwargs) modelB = TheModelBClass(*args, **kwargs) optimizerA = TheOptimizerAClass(*args, **kwargs) optimizerB = TheOptimizerBClass(*args, **kwargs) checkpoint = torch.load(PATH) modelA.load_state_dict(checkpoint['modelA_state_dict']) modelB.load_state_dict(checkpoint['modelB_state_dict']) optimizerA.load_state_dict(checkpoint['optimizerA_state_dict']) optimizerB.load_state_dict(checkpoint['optimizerB_state_dict']) modelA.eval() modelB.eval() # - or - modelA.train() modelB.train() 当保存一个模型由多个torch.nn.Modules组成时，例如GAN(对抗生成网络)、sequence-to-sequence (序列到序列模型), 或者是多个模 型融合, 可以采用与保存常规检查点相同的方法。换句话说，保存每个模型的 state_dict 的字典和相对应的优化器。如前所述，可以通 过简单地将它们附加到字典的方式来保存任何其他项目，这样有助于恢复训练。\nPyTorch 中常见的保存 checkpoint 是使用 .tar 文件扩展名。\n要加载项目，首先需要初始化模型和优化器，然后使用torch.load()来加载本地字典。这里，你可以非常容易的通过简单查询字典来访问你所保存的项目。\n请记住在运行推理之前，务必调用model.eval()去设置 dropout 和 batch normalization 为评估。如果不这样做，有可能得到不一致的推断结果。 如果你想要恢复训练，请调用model.train()以确保这些层处于训练模式。\n使用在不同模型参数下的热启动模式 保存 1 torch.save(modelA.state_dict(), PATH) 加载 1 2 modelB = TheModelBClass(*args, **kwargs) modelB.load_state_dict(torch.load(PATH), strict=False) 在迁移学习或训练新的复杂模型时，部分加载模型或加载部分模型是常见的情况。利用训练好的参数，有助于热启动训练过程，并希望帮助你的模型比从头开始训练能够更快地收敛。\n无论是从缺少某些键的 state_dict 加载还是从键的数目多于加载模型的 state_dict , 都可以通过在load_state_dict()函数中将strict参数设置为 False 来忽略非匹配键的函数。\n如果要将参数从一个层加载到另一个层，但是某些键不匹配，主要修改正在加载的 state_dict 中的参数键的名称以匹配要在加载到模型中的键即可。\n通过设备保存/加载模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 保存到 CPU、加载到 CPU torch.save(model.state_dict(), PATH) device = torch.device('cpu') model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=device)) # 保存到 GPU、加载到 GPU torch.save(model.state_dict(), PATH) device = torch.device(\"cuda\") model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.to(device) # 确保在你提供给模型的任何输入张量上调用input = input.to(device) # 保存到 CPU，加载到 GPU torch.save(model.state_dict(), PATH) device = torch.device(\"cuda\") model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\")) # Choose whatever GPU device number you want model.to(device) # 确保在你提供给模型的任何输入张量上调用input = input.to(device) # 保存 torch.nn.DataParallel 模型 torch.save(model.module.state_dict(), PATH) # 加载任何你想要的设备 ","title":"pytorch-保存和加载模型","uri":"/contents/dl/pytorch-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B/"},{"categories":["contents"],"content":"\n记录pytorch加载和处理数据的接口函数。\nPyTorch提供了许多工具来简化和希望数据加载，使代码更具可读性。\n使用数据集类(datasets),转换(transforms)和数据加载器(dataloader) 下载安装包 scikit-image：用于图像的IO和变换 pandas：用于更容易地进行csv解析 读取数据集 将csv中的标注点数据读入（N，2）数组中，其中N是特征点的数量。读取数据代码如下：\n1 2 3 4 5 6 7 8 9 10 landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv') n = 65 img_name = landmarks_frame.iloc[n, 0] landmarks = landmarks_frame.iloc[n, 1:].as_matrix() landmarks = landmarks.astype('float').reshape(-1, 2) print('Image name: {}'.format(img_name)) print('Landmarks shape: {}'.format(landmarks.shape)) print('First 4 Landmarks: {}'.format(landmarks[:4])) 写一个简单的函数来展示一张图片和它对应的标注点作为例子。\n1 2 3 4 5 6 7 8 9 10 def show_landmarks(image, landmarks): \"\"\"显示带有地标的图片\"\"\" plt.imshow(image) plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r') plt.pause(0.001) # pause a bit so that plots are updated plt.figure() show_landmarks(io.imread(os.path.join('data/faces/', img_name)), landmarks) plt.show() 数据集类 torch.utils.data.Dataset是表示数据集的抽象类，因此自定义数据集应继承Dataset并覆盖以下方法__len__实现 len(dataset) 返还数据集的尺寸。 __getitem__用来获取一些索引数据，例如 dataset[i] 中的(i)。\n建立数据集类 为面部数据集创建一个数据集类。我们将在 __init__中读取csv的文件内容，在 __getitem__中读取图片。这么做是为了节省内存 空间。只有在需要用到图片的时候才读取它而不是一开始就把图片全部存进内存里。\n我们的数据样本将按这样一个字典{‘image’: image, ’landmarks’: landmarks}组织。 我们的数据集类将添加一个可选参数transform 以方便对样本进行预处理。下一节我们会看到什么时候需要用到transform参数。 __init__方法如下图所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class FaceLandmarksDataset(Dataset): \"\"\"面部标记数据集.\"\"\" def __init__(self, csv_file, root_dir, transform=None): \"\"\" csv_file（string）：带注释的csv文件的路径。 root_dir（string）：包含所有图像的目录。 transform（callable， optional）：一个样本上的可用的可选变换 \"\"\" self.landmarks_frame = pd.read_csv(csv_file) self.root_dir = root_dir self.transform = transform def __len__(self): return len(self.landmarks_frame) def __getitem__(self, idx): img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0]) image = io.imread(img_name) landmarks = self.landmarks_frame.iloc[idx, 1:] landmarks = np.array([landmarks]) landmarks = landmarks.astype('float').reshape(-1, 2) sample = {'image': image, 'landmarks': landmarks} if self.transform: sample = self.transform(sample) return sample 数据可视化 实例化这个类并遍历数据样本。我们将会打印出前四个例子的尺寸并展示标注的特征点。 代码如下图所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv', root_dir='data/faces/') fig = plt.figure() for i in range(len(face_dataset)): sample = face_dataset[i] print(i, sample['image'].shape, sample['landmarks'].shape) ax = plt.subplot(1, 4, i + 1) plt.tight_layout() ax.set_title('Sample #{}'.format(i)) ax.axis('off') show_landmarks(**sample) if i == 3: plt.show() break 数据变换 通过上面的例子我们会发现图片并不是同样的尺寸。绝大多数神经网络都假定图片的尺寸相同。因此我们需要做一些预处理。让我们创建三个转换: Rescale：缩放图片 RandomCrop：对图片进行随机裁剪。这是一种数据增强操作 ** ToTensor**：把numpy格式图片转为torch格式图片 (我们需要交换坐标轴).\n我们会把它们写成可调用的类的形式而不是简单的函数，这样就不需要每次调用时传递一遍参数。我们只需要实现__call__方法，必 要的时候实现 __init__方法。我们可以这样调用这些转换:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 class Rescale(object): \"\"\"将样本中的图像重新缩放到给定大小。. Args: output_size（tuple或int）：所需的输出大小。 如果是元组，则输出为 与output_size匹配。 如果是int，则匹配较小的图像边缘到output_size保持纵横比相同。 \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] if isinstance(self.output_size, int): if h \u003e w: new_h, new_w = self.output_size * h / w, self.output_size else: new_h, new_w = self.output_size, self.output_size * w / h else: new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) img = transform.resize(image, (new_h, new_w)) # h and w are swapped for landmarks because for images, # x and y axes are axis 1 and 0 respectively landmarks = landmarks * [new_w / w, new_h / h] return {'image': img, 'landmarks': landmarks} class RandomCrop(object): \"\"\"随机裁剪样本中的图像. Args: output_size（tuple或int）：所需的输出大小。 如果是int，方形裁剪是。 \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): self.output_size = (output_size, output_size) else: assert len(output_size) == 2 self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] new_h, new_w = self.output_size top = np.random.randint(0, h - new_h) left = np.random.randint(0, w - new_w) image = image[top: top + new_h, left: left + new_w] landmarks = landmarks - [left, top] return {'image': image, 'landmarks': landmarks} class ToTensor(object): \"\"\"将样本中的ndarrays转换为Tensors.\"\"\" def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] # 交换颜色轴因为 # numpy包的图片是: H * W * C # torch包的图片是: C * H * W image = image.transpose((2, 0, 1)) return {'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)} 接下来我们把这些转换应用到一个例子上。\n我们想要把图像的短边调整为256，然后随机裁剪(randomcrop)为224大小的正方形。也就是说，我们打算组合一个Rescale和 RandomCrop的变换。 我们可以调用一个简单的类 torchvision.transforms.Compose来实现这一操作。具体实现如下图：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 scale = Rescale(256) crop = RandomCrop(128) composed = transforms.Compose([Rescale(256), RandomCrop(224)]) # 在样本上应用上述的每个变换。 fig = plt.figure() sample = face_dataset[65] for i, tsfrm in enumerate([scale, crop, composed]): transformed_sample = tsfrm(sample) ax = plt.subplot(1, 3, i + 1) plt.tight_layout() ax.set_title(type(tsfrm).__name__) show_landmarks(**transformed_sample) plt.show() 迭代数据集 让我们把这些整合起来以创建一个带组合转换的数据集。总结一下，每次这个数据集被采样时: 及时地从文件中读取图片 对读取的图片应用转换 * 由于其中一步操作是随机的 (randomcrop) , 数据被增强了\n我们可以像之前那样使用for i in range循环来对所有创建的数据集执行同样的操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv', root_dir='data/faces/', transform=transforms.Compose([ Rescale(256), RandomCrop(224), ToTensor() ])) for i in range(len(transformed_dataset)): sample = transformed_dataset[i] print(i, sample['image'].size(), sample['landmarks'].size()) if i == 3: break 但是，对所有数据集简单的使用for循环牺牲了许多功能，尤其是: 批量处理数据 打乱数据 * 使用多线程multiprocessingworker 并行加载数据。\ntorch.utils.data.DataLoader是一个提供上述所有这些功能的迭代器。下面使用的参数必须是清楚的。一个值得关注的参数是collate_fn, 可以通过它来决定如何对数据进行批处理。但是绝大多数情况下默认值就能运行良好。\n完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 from __future__ import print_function, division import os import torch import pandas as pd #用于更容易地进行csv解析 from skimage import io, transform #用于图像的IO和变换 import numpy as np import matplotlib.pyplot as plt from torch.utils.data import Dataset, DataLoader from torchvision import transforms, utils # 忽略警告 import warnings warnings.filterwarnings(\"ignore\") plt.ion() # interactive mode landmarks_frame = pd.read_csv('/home/jiajie/Downloads/data/faces/face_landmarks.csv') n = 63 img_name = landmarks_frame.iloc[n, 0] landmarks = landmarks_frame.iloc[n, 1:].as_matrix() landmarks = landmarks.astype('float').reshape(-1, 2) print('Image name: {}'.format(img_name)) print('Landmarks shape: {}'.format(landmarks.shape)) print('First 4 Landmarks: {}'.format(landmarks[:4])) def show_landmarks(image, landmarks): \"\"\"显示带有地标的图片\"\"\" plt.imshow(image) plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r') plt.pause(0.001) # pause a bit so that plots are updated plt.figure() show_landmarks(io.imread(os.path.join('/home/jiajie/Downloads/data/faces/', img_name)), landmarks) #plt.show() #plt.pause(100) class FaceLandmarksDataset(Dataset): \"\"\"面部标记数据集.\"\"\" def __init__(self, csv_file, root_dir, transform=None): \"\"\" csv_file（string）：带注释的csv文件的路径。 root_dir（string）：包含所有图像的目录。 transform（callable， optional）：一个样本上的可用的可选变换 \"\"\" self.landmarks_frame = pd.read_csv(csv_file) self.root_dir = root_dir self.transform = transform def __len__(self): return len(self.landmarks_frame) def __getitem__(self, idx): img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0]) image = io.imread(img_name) landmarks = self.landmarks_frame.iloc[idx, 1:] landmarks = np.array([landmarks]) landmarks = landmarks.astype('float').reshape(-1, 2) sample = {'image': image, 'landmarks': landmarks} if self.transform: sample = self.transform(sample) return sample class Rescale(object): \"\"\"将样本中的图像重新缩放到给定大小。. Args: output_size（tuple或int）：所需的输出大小。 如果是元组，则输出为 与output_size匹配。 如果是int，则匹配较小的图像边缘到output_size保持纵横比相同。 \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] if isinstance(self.output_size, int): if h \u003e w: new_h, new_w = self.output_size * h / w, self.output_size else: new_h, new_w = self.output_size, self.output_size * w / h else: new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) img = transform.resize(image, (new_h, new_w)) # h and w are swapped for landmarks because for images, # x and y axes are axis 1 and 0 respectively landmarks = landmarks * [new_w / w, new_h / h] return {'image': img, 'landmarks': landmarks} class RandomCrop(object): \"\"\"随机裁剪样本中的图像. Args: output_size（tuple或int）：所需的输出大小。 如果是int，方形裁剪是。 \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): self.output_size = (output_size, output_size) else: assert len(output_size) == 2 self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] new_h, new_w = self.output_size top = np.random.randint(0, h - new_h) left = np.random.randint(0, w - new_w) image = image[top: top + new_h, left: left + new_w] landmarks = landmarks - [left, top] return {'image': image, 'landmarks': landmarks} class ToTensor(object): \"\"\"将样本中的ndarrays转换为Tensors.\"\"\" def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] # 交换颜色轴因为 # numpy包的图片是: H * W * C # torch包的图片是: C * H * W image = image.transpose((2, 0, 1)) return {'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)} transformed_dataset = FaceLandmarksDataset(csv_file='/home/jiajie/Downloads/data/faces/face_landmarks.csv', root_dir='/home/jiajie/Downloads/data/faces/', transform=transforms.Compose([ Rescale(256), RandomCrop(224), ToTensor() ])) dataloader = DataLoader(transformed_dataset, batch_size=4, shuffle=True, num_workers=4) # 辅助功能：显示批次 def show_landmarks_batch(sample_batched): \"\"\"Show image with landmarks for a batch of samples.\"\"\" images_batch, landmarks_batch = \\ sample_batched['image'], sample_batched['landmarks'] batch_size = len(images_batch) im_size = images_batch.size(2) grid_border_size = 2 grid = utils.make_grid(images_batch) plt.imshow(grid.numpy().transpose((1, 2, 0))) for i in range(batch_size): plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size, landmarks_batch[i, :, 1].numpy() + grid_border_size, s=10, marker='.', c='r') plt.title('Batch from dataloader') for i_batch, sample_batched in enumerate(dataloader): print(i_batch, sample_batched['image'].size(), sample_batched['landmarks'].size()) # 观察第4批次并停止。 if i_batch == 3: plt.figure() show_landmarks_batch(sample_batched) plt.axis('off') plt.ioff() plt.show() break 利用torchvision构造和使用数据集类(datasets),转换(transforms)和数据加载器(dataloader) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from torchvision import transforms, datasets data_transform = transforms.Compose([ transforms.RandomSizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train', transform=data_transform) dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset, batch_size=4, shuffle=True, num_workers=4) ","title":"pytorch-数据加载和处理","uri":"/contents/dl/pytorch-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%A4%84%E7%90%86/"},{"categories":["contents"],"content":"用pytorch搭建神经网络的基本步骤。\nPyTorch 自动微分 TENSOR torch.Tensor 是包的核心类。如果将其属性 .requires_grad 设置为 True，则会开始跟踪针对 tensor 的所有操作。完成计算后，您可以调用 .backward() 来自动计算所有梯度。该张量的梯度将累积到 .grad 属性中。 要停止 tensor 历史记录的跟踪，您可以调用 .detach()，它将其与计算历史记录分离，并防止将来的计算被跟踪。\n要停止跟踪历史记录（和使用内存），您还可以将代码块使用 with torch.no_grad(): 包装起来。在评估模型时，这是特别有用，因为模型在训练阶段具有 requires_grad = True 的可训练参数有利于调参，但在评估阶段我们不需要梯度。\n还有一个类对于 autograd 实现非常重要那就是 Function。Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则g rad_fn 是 None ）。\n如果你想计算导数，你可以调用 Tensor.backward()。如果 Tensor 是标量（即它包含一个元素数据），则不需要指定任何参数backward()，但是如果它有更多元素，则需要指定一个gradient 参数来指定张量的形状。\n神经网络 神经网络可以通过 torch.nn 包来构建。\n现在对于自动梯度(autograd)有一些了解，神经网络是基于自动梯度 (autograd)来定义一些模型。一个 nn.Module 包括层和一个方法 forward(input) 它会返回输出(output)。\n一个典型的神经网络训练过程包括以下几点：\n定义一个包含可训练参数的神经网络\n迭代整个输入\n通过神经网络处理输入\n计算损失(loss)\n反向传播梯度到神经网络的参数\n更新网络的参数，典型的用一个简单的更新方法：weight = weight - learning_rate *gradient\n定义神经网络 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # 1 input image channel, 6 output channels, 5x5 square convolution self.conv1 = nn.Conv2d(1,6,5) self.conv2 = nn.Conv2d(6,16,5) self.fc1 = nn.Linear(16*5*5,120) self.fc2 = nn.Linear(120,84) self.fc3 = nn.Linear(84,10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) # resize x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features net = Net() print(net) 输出结果：\n1 2 3 4 5 6 7 Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) 你刚定义了一个前馈函数，然后反向传播函数被自动通过 autograd 定义了。你可以使用任何张量操作在前馈函数上。\n一个模型可训练的参数可以通过调用 net.parameters() 返回：\n1 2 3 params = list(net.parameters()) print(len(params)) print(params[0].size()) # conv1's .weight 输出：\n1 2 10 torch.Size([6, 1, 5, 5]) 迭代处理输入 1 2 3 input = torch.randn(1, 1, 32, 32) out = net(input) print(out) 计算损失值 1 2 3 4 5 6 7 output = net(input) target = torch.randn(10) # a dummy target, for example target = target.view(1, -1) # make it the same shape as output criterion = nn.MSELoss() loss = criterion(output, target) print(loss) 当我们调用 loss.backward()，整个图都会微分，而且所有的在图中的requires_grad=True 的张量将会让他们的 grad 张量累计梯度。\n反向传播 为了实现反向传播损失，我们所有需要做的事情仅仅是使用 loss.backward()。你需要清空现存的梯度，要不然更新后的梯度会和现存的梯度累计到一起。\n1 2 3 4 5 6 7 8 9 net.zero_grad() # zeroes the gradient buffers of all parameters print('conv1.bias.grad before backward') print(net.conv1.bias.grad) loss.backward() print('conv1.bias.grad after backward') print(net.conv1.bias.grad) 更新神经网络参数 完整步骤：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim class Net(nn.Module): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # 1 input image channel, 6 output channels, 5x5 square convolution self.conv1 = nn.Conv2d(1,6,5) self.conv2 = nn.Conv2d(6,16,5) self.fc1 = nn.Linear(16*5*5,120) self.fc2 = nn.Linear(120,84) self.fc3 = nn.Linear(84,10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) # resize x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features net = Net() # create your loss criterion = nn.MSELoss() # create your optimizer optimizer = optim.SGD(net.parameters(), lr=0.01) input = torch.randn(1, 1, 32, 32) # in your training loop: optimizer.zero_grad() # zero the gradient buffers output = net(input) target = torch.randn(10) # a dummy target, for example target = target.view(1, -1) # make it the same shape as output loss = criterion(output, target) #net.zero_grad() #有了 optimizer.zero_grad() 就不需要 net.zero_grad() 了 loss.backward() optimizer.step() # Does the update ","title":"pytorch1.0-基本步骤","uri":"/contents/dl/pytorch1-0-%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4/"},{"categories":["contents"],"content":"记录构造函数的形式，用法。\n定义 类通过一个或几个特殊的成员函数来控制其对象的初始化过程，这些函数叫做构造函数。构造函数的任务是初始化类对象的数据成员，无论何时只要类的对象被创建，就会执行构造函数。\n构造函数的名字和类名相同。和其他函数不一样的是，构造函数没有返回类型，单有一个（可能为空的）参数列表和一个（可能为空的）函数体。类可以含有多个构造函数，但不同的构造函数之间必须在参数数量或参数类型上有所区别。\n默认构造函数 如果我们的类没有显式定义构造函数，编译器会隐式构造一个默认构造函数。这个默认构造函数初始化数据成员的方式：\n如果存在类内初始值，用它来初始化成员。 否则，默认初始化该成员。 但是建议自定义构造函数，理由：\n只有在类没有声明任何构造函数时，编译器才会自动生成默认的构造函数。一旦我们声明的其他构造函数，类将没有默认的构造函数。 如果定义在类内的内置类型或复合类型（比如指针或数组）的对象被默认初始化时，则它的值是未定义的。 有时候编译器无法为某些类合成默认的构造函数，比如类内含有其他类类型的数据成员且这个成员没有默认构造函数时，编译器无法初始化该成员。 形式 1 2 3 4 5 6 7 struct Sales_data{ Sales_data() = default; //默认构造函数 Sales_data() = (const std::string \u0026s): bookNo(s){} //显式初始化一个，隐式初始化两个 Sales_data() = (const std::string \u0026s, unsigned \u0026s ,double p): bookNo(s),units_sold(n),revenue(p*n){} //显式初始化三个 } ","title":"构造函数–类对象初始化","uri":"/contents/c++/construct-function/"},{"categories":["contents"],"content":"视觉 SLAM 主要分为视觉前端和优化后端。前端也称为视觉里程计(VO)。它根据相邻图像的信息,估计出粗略的相机运动,给后端提供较好的初始值。本节将记录从特征点法入手,学习如何提取、匹配图像特征点,然后估计两帧之间的相机运动和场景结构,从而实现一个基本的两帧间视觉里程计。摘自《视觉SLAM十四讲》。\n基本概念 特征点 特征点由关键点(Key-point)和描述子(Descriptor)两部分组成。比方说,当我们谈论 SIFT 特征时,是指“提取 SIFT 关键点,并计算 SIFT 描述子”两件事情。关键点是指该特征点在图像里的位置,有些特征点还具有朝向、大小等信息。描述子通常是一个向量,按照某种人为设计的方式,描述了该关键点周围像素的信息。描述子是按照“外观相似的特征应该有相似的描述子”的原则设计的。因此,只要两个特征点的描述子在向量空间上的距离相近,就可以认为它们是同样的特征点。\n特征匹配 特征匹配是视觉 SLAM 中极为关键的一步,宽泛地说,特征匹配解决了 SLAM 中的数据关联问题(data association),即确定当前看到的路标与之前看到的路标之间的对应关系。通过对图像与图像,或者图像与地图之间的描述子进行准确的匹配,我们可以为后续的姿态估计,优化等操作减轻大量负担。\n编程练习：特征提取和匹配 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 #include \u003ciostream\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; int main ( int argc, char** argv ) { if ( argc != 3 ) { cout\u003c\u003c\"usage: feature_extraction img1 img2\"\u003c\u003cendl; return 1; } //-- 读取图像 Mat img_1 = imread ( argv[1], CV_LOAD_IMAGE_COLOR ); Mat img_2 = imread ( argv[2], CV_LOAD_IMAGE_COLOR ); //-- 初始化 std::vector\u003cKeyPoint\u003e keypoints_1, keypoints_2; Mat descriptors_1, descriptors_2; Ptr\u003cFeatureDetector\u003e detector = ORB::create(); Ptr\u003cDescriptorExtractor\u003e descriptor = ORB::create(); // Ptr\u003cFeatureDetector\u003e detector = FeatureDetector::create(detector_name); // Ptr\u003cDescriptorExtractor\u003e descriptor = DescriptorExtractor::create(descriptor_name); Ptr\u003cDescriptorMatcher\u003e matcher = DescriptorMatcher::create ( \"BruteForce-Hamming\" ); //-- 第一步:检测 Oriented FAST 角点位置 detector-\u003edetect ( img_1,keypoints_1 ); detector-\u003edetect ( img_2,keypoints_2 ); //-- 第二步:根据角点位置计算 BRIEF 描述子 descriptor-\u003ecompute ( img_1, keypoints_1, descriptors_1 ); descriptor-\u003ecompute ( img_2, keypoints_2, descriptors_2 ); Mat outimg1; drawKeypoints( img_1, keypoints_1, outimg1, Scalar::all(-1), DrawMatchesFlags::DEFAULT ); imshow(\"ORB特征点\",outimg1); //-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离 vector\u003cDMatch\u003e matches; //BFMatcher matcher ( NORM_HAMMING ); matcher-\u003ematch ( descriptors_1, descriptors_2, matches ); //-- 第四步:匹配点对筛选 double min_dist=10000, max_dist=0; //找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离 for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { double dist = matches[i].distance; if ( dist \u003c min_dist ) min_dist = dist; if ( dist \u003e max_dist ) max_dist = dist; } // 仅供娱乐的写法 min_dist = min_element( matches.begin(), matches.end(), [](const DMatch\u0026 m1, const DMatch\u0026 m2) {return m1.distance\u003cm2.distance;} )-\u003edistance; max_dist = max_element( matches.begin(), matches.end(), [](const DMatch\u0026 m1, const DMatch\u0026 m2) {return m1.distance\u003cm2.distance;} )-\u003edistance; printf ( \"-- Max dist : %f \\n\", max_dist ); printf ( \"-- Min dist : %f \\n\", min_dist ); //当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限. std::vector\u003c DMatch \u003e good_matches; for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { if ( matches[i].distance \u003c= max ( 2*min_dist, 30.0 ) ) { good_matches.push_back ( matches[i] ); } } //-- 第五步:绘制匹配结果 Mat img_match; Mat img_goodmatch; drawMatches ( img_1, keypoints_1, img_2, keypoints_2, matches, img_match ); drawMatches ( img_1, keypoints_1, img_2, keypoints_2, good_matches, img_goodmatch ); imshow ( \"所有匹配点对\", img_match ); imshow ( \"优化后匹配点对\", img_goodmatch ); waitKey(0); return 0; } 相机运动估计 我们希望根据匹配的点对,估计相机的运动。\n当相机为单目时,我们只知道 2D 的像素坐标,因而问题是根据两组 2D 点估计运动。该问题用对极几何来解决。（无深度信息）\n当相机为双目、RGB-D 时,或者我们通过某种方法得到了距离信息,那问题就是根据两组 3D 点估计运动。该问题通常用 ICP 来解决。（两张深度图）\n如果我们有 3D 点和它们在相机的投影位置,也能估计相机的运动。该问题通过 PnP求解。（单张深度图）\n2D-2D: 对极几何 如下图：\n在第一帧的坐标系下,设 P 的空间位置为:\n$$ \\boldsymbol{P}=[X, Y, Z]^{T} $$\n以第一个相机坐标系作为基准，空间点对应的像素坐标：\n$$ s_{1} \\boldsymbol{p}_{1}=\\boldsymbol{K} \\boldsymbol{P}, \\quad s_{2} \\boldsymbol{p}_{2}=\\boldsymbol{K}(\\boldsymbol{R} \\boldsymbol{P}+\\boldsymbol{t}) $$\n在齐次坐标系下可以忽略掉常数项：\n$$ \\boldsymbol{p}_{1}=\\boldsymbol{K} \\boldsymbol{P}, \\quad \\boldsymbol{p}_{2}=\\boldsymbol{K}(\\boldsymbol{R} \\boldsymbol{P}+\\boldsymbol{t}) $$\n取：\n$$ \\boldsymbol{x}_{1}=\\boldsymbol{K}^{-1} \\boldsymbol{p}_{1}, \\quad \\boldsymbol{x}_{2}=\\boldsymbol{K}^{-1} \\boldsymbol{p}_{2} $$\n化简有：\n$$ \\boldsymbol{x}_{2}=\\boldsymbol{R} \\boldsymbol{x}_{1}+\\boldsymbol{t} $$\n两边同时左乘$\\boldsymbol{x}_{2}^{T} $：\n$$ \\boldsymbol{x}_{2}^{T} \\boldsymbol{t}^{\\wedge} \\boldsymbol{x}_{2}=\\boldsymbol{x}_{2}^{T} \\boldsymbol{t}^{\\wedge} \\boldsymbol{R} \\boldsymbol{x}_{1} $$\n两侧同时再左乘$\\boldsymbol{x}_{2}^{T}$：\n$$ \\boldsymbol{x}_{2}^{T} \\boldsymbol{t}^{\\wedge} \\boldsymbol{x}_{2}=\\boldsymbol{x}_{2}^{T} \\boldsymbol{t}^{\\wedge} \\boldsymbol{R} \\boldsymbol{x}_{1} $$\n观察等式左侧,$\\boldsymbol{t}^{\\wedge} \\boldsymbol{x}_{2}$是一个与 $\\boldsymbol{t}$和 $\\boldsymbol{x}_{2}$ 都垂直的向量。把它再和$\\boldsymbol{x}_{2}$ 做内积时,将得到 0。\n$$ \\boldsymbol{x}_{2}^{T} \\boldsymbol{t}^{\\wedge} \\boldsymbol{R} \\boldsymbol{x}_{1}=0 $$\n即：\n$$ \\boldsymbol{p}_{2}^{T} \\boldsymbol{K}^{-T} \\boldsymbol{t}^{\\wedge} \\boldsymbol{R} \\boldsymbol{K}^{-1} \\boldsymbol{p}_{1}=0 $$\n令：\n$$ \\boldsymbol{E}=\\boldsymbol{t}^{\\wedge} \\boldsymbol{R}, \\quad \\boldsymbol{F}=\\boldsymbol{K}^{-T} \\boldsymbol{E} \\boldsymbol{K}^{-1}, \\quad \\boldsymbol{x}_{2}^{T} \\boldsymbol{E} \\boldsymbol{x}_{1}=\\boldsymbol{p}_{2}^{T} \\boldsymbol{F} \\boldsymbol{p}_{1}=0 $$\n这两个式子都称为对极约束,它以形式简洁著名。它的几何意义是$O_{1}, P, O_{2}$ 三者共 面。对极约束中同时包含了平移和旋转。\n对极约束简洁地给出了两个匹配点的空间位置关系。于是,相机位姿估计问题变为以 下两步:\n根据配对点的像素位置,求出 $\\boldsymbol{E}$或者 $\\boldsymbol{F}$;\n根据 $\\boldsymbol{E}$ 或者 $\\boldsymbol{F}$ ,求出 $\\boldsymbol{R}$,$\\boldsymbol{t}$。\n由于 $\\boldsymbol{E}$ 和 $\\boldsymbol{E}$ 只相差了相机内参,而内参在 SLAM 中通常是已知的 ,所以实践当往往使用形式更简单的 $\\boldsymbol{E}$。\nE 具有五个自由度的事实,表明我们最少可以用五对点来求解 E。但是,E 的内在性质是一种非线性性质,在求解线性方程时会带来麻烦,因此,也可以只考虑它的尺度等价性,使用八对点来估计 E——这就是经典的八点法(Eight-point-algorithm)。\n根据已经估得的本质矩阵 E,恢复出相机的运动 R, t。这个过程是由奇异值分解(SVD)得到的。\n编程练习：对极约束求解相机运动 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 #include \u003ciostream\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/calib3d/calib3d.hpp\u003e // #include \"extra.h\" // use this if in OpenCV2 using namespace std; using namespace cv; /**************************************************** * 本程序演示了如何使用2D-2D的特征匹配估计相机运动 * **************************************************/ void find_feature_matches ( const Mat\u0026 img_1, const Mat\u0026 img_2, std::vector\u003cKeyPoint\u003e\u0026 keypoints_1, std::vector\u003cKeyPoint\u003e\u0026 keypoints_2, std::vector\u003c DMatch \u003e\u0026 matches ); void pose_estimation_2d2d ( std::vector\u003cKeyPoint\u003e keypoints_1, std::vector\u003cKeyPoint\u003e keypoints_2, std::vector\u003c DMatch \u003e matches, Mat\u0026 R, Mat\u0026 t ); // 像素坐标转相机归一化坐标 Point2d pixel2cam ( const Point2d\u0026 p, const Mat\u0026 K ); int main ( int argc, char** argv ) { if ( argc != 3 ) { cout\u003c\u003c\"usage: pose_estimation_2d2d img1 img2\"\u003c\u003cendl; return 1; } //-- 读取图像 Mat img_1 = imread ( argv[1], CV_LOAD_IMAGE_COLOR ); Mat img_2 = imread ( argv[2], CV_LOAD_IMAGE_COLOR ); vector\u003cKeyPoint\u003e keypoints_1, keypoints_2; vector\u003cDMatch\u003e matches; find_feature_matches ( img_1, img_2, keypoints_1, keypoints_2, matches ); cout\u003c\u003c\"一共找到了\"\u003c\u003cmatches.size() \u003c\u003c\"组匹配点\"\u003c\u003cendl; //-- 估计两张图像间运动 Mat R,t; pose_estimation_2d2d ( keypoints_1, keypoints_2, matches, R, t ); //-- 验证E=t^R*scale Mat t_x = ( Mat_\u003cdouble\u003e ( 3,3 ) \u003c\u003c 0, -t.at\u003cdouble\u003e ( 2,0 ), t.at\u003cdouble\u003e ( 1,0 ), t.at\u003cdouble\u003e ( 2,0 ), 0, -t.at\u003cdouble\u003e ( 0,0 ), -t.at\u003cdouble\u003e ( 1.0 ), t.at\u003cdouble\u003e ( 0,0 ), 0 ); cout\u003c\u003c\"t^R=\"\u003c\u003cendl\u003c\u003ct_x*R\u003c\u003cendl; //-- 验证对极约束 Mat K = ( Mat_\u003cdouble\u003e ( 3,3 ) \u003c\u003c 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1 ); for ( DMatch m: matches ) { Point2d pt1 = pixel2cam ( keypoints_1[ m.queryIdx ].pt, K ); Mat y1 = ( Mat_\u003cdouble\u003e ( 3,1 ) \u003c\u003c pt1.x, pt1.y, 1 ); Point2d pt2 = pixel2cam ( keypoints_2[ m.trainIdx ].pt, K ); Mat y2 = ( Mat_\u003cdouble\u003e ( 3,1 ) \u003c\u003c pt2.x, pt2.y, 1 ); Mat d = y2.t() * t_x * R * y1; cout \u003c\u003c \"epipolar constraint = \" \u003c\u003c d \u003c\u003c endl; } return 0; } void find_feature_matches ( const Mat\u0026 img_1, const Mat\u0026 img_2, std::vector\u003cKeyPoint\u003e\u0026 keypoints_1, std::vector\u003cKeyPoint\u003e\u0026 keypoints_2, std::vector\u003c DMatch \u003e\u0026 matches ) { //-- 初始化 Mat descriptors_1, descriptors_2; // used in OpenCV3 Ptr\u003cFeatureDetector\u003e detector = ORB::create(); Ptr\u003cDescriptorExtractor\u003e descriptor = ORB::create(); // use this if you are in OpenCV2 // Ptr\u003cFeatureDetector\u003e detector = FeatureDetector::create ( \"ORB\" ); // Ptr\u003cDescriptorExtractor\u003e descriptor = DescriptorExtractor::create ( \"ORB\" ); Ptr\u003cDescriptorMatcher\u003e matcher = DescriptorMatcher::create ( \"BruteForce-Hamming\" ); //-- 第一步:检测 Oriented FAST 角点位置 detector-\u003edetect ( img_1,keypoints_1 ); detector-\u003edetect ( img_2,keypoints_2 ); //-- 第二步:根据角点位置计算 BRIEF 描述子 descriptor-\u003ecompute ( img_1, keypoints_1, descriptors_1 ); descriptor-\u003ecompute ( img_2, keypoints_2, descriptors_2 ); //-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离 vector\u003cDMatch\u003e match; //BFMatcher matcher ( NORM_HAMMING ); matcher-\u003ematch ( descriptors_1, descriptors_2, match ); //-- 第四步:匹配点对筛选 double min_dist=10000, max_dist=0; //找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离 for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { double dist = match[i].distance; if ( dist \u003c min_dist ) min_dist = dist; if ( dist \u003e max_dist ) max_dist = dist; } printf ( \"-- Max dist : %f \\n\", max_dist ); printf ( \"-- Min dist : %f \\n\", min_dist ); //当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限. for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { if ( match[i].distance \u003c= max ( 2*min_dist, 30.0 ) ) { matches.push_back ( match[i] ); } } } Point2d pixel2cam ( const Point2d\u0026 p, const Mat\u0026 K ) { return Point2d ( ( p.x - K.at\u003cdouble\u003e ( 0,2 ) ) / K.at\u003cdouble\u003e ( 0,0 ), ( p.y - K.at\u003cdouble\u003e ( 1,2 ) ) / K.at\u003cdouble\u003e ( 1,1 ) ); } void pose_estimation_2d2d ( std::vector\u003cKeyPoint\u003e keypoints_1, std::vector\u003cKeyPoint\u003e keypoints_2, std::vector\u003c DMatch \u003e matches, Mat\u0026 R, Mat\u0026 t ) { // 相机内参,TUM Freiburg2 Mat K = ( Mat_\u003cdouble\u003e ( 3,3 ) \u003c\u003c 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1 ); //-- 把匹配点转换为vector\u003cPoint2f\u003e的形式 vector\u003cPoint2f\u003e points1; vector\u003cPoint2f\u003e points2; for ( int i = 0; i \u003c ( int ) matches.size(); i++ ) { points1.push_back ( keypoints_1[matches[i].queryIdx].pt ); points2.push_back ( keypoints_2[matches[i].trainIdx].pt ); } //-- 计算基础矩阵 Mat fundamental_matrix; fundamental_matrix = findFundamentalMat ( points1, points2, CV_FM_8POINT ); cout\u003c\u003c\"fundamental_matrix is \"\u003c\u003cendl\u003c\u003c fundamental_matrix\u003c\u003cendl; //-- 计算本质矩阵 Point2d principal_point ( 325.1, 249.7 );\t//相机光心, TUM dataset标定值 double focal_length = 521;\t//相机焦距, TUM dataset标定值 Mat essential_matrix; essential_matrix = findEssentialMat ( points1, points2, focal_length, principal_point ); cout\u003c\u003c\"essential_matrix is \"\u003c\u003cendl\u003c\u003c essential_matrix\u003c\u003cendl; //-- 计算单应矩阵 Mat homography_matrix; homography_matrix = findHomography ( points1, points2, RANSAC, 3 ); cout\u003c\u003c\"homography_matrix is \"\u003c\u003cendl\u003c\u003chomography_matrix\u003c\u003cendl; //-- 从本质矩阵中恢复旋转和平移信息. recoverPose ( essential_matrix, points1, points2, R, t, focal_length, principal_point ); cout\u003c\u003c\"R is \"\u003c\u003cendl\u003c\u003cR\u003c\u003cendl; cout\u003c\u003c\"t is \"\u003c\u003cendl\u003c\u003ct\u003c\u003cendl; } 三角化 在使用对极几何约束估计了相机运动后，下一步我们需要用相机的运动估计特征点的空间位置。在单目 SLAM 中,仅通过单张图像无法获得像素的深度信息,我们需要通过三角测量(Triangulation)(或三角化)的方法来估计地图点的深度。如下图：\n按照对极几何中的定义,设$\\boldsymbol{x}{1}, \\boldsymbol{x}{2}$ 为两个特征点的归一化坐标，有：\n$$ s_{1} \\boldsymbol{x}_{1}=s_{2} \\boldsymbol{R} \\boldsymbol{x}_{2}+\\boldsymbol{t} $$\n要算 $s_{2}$ ,那么先对上式两侧左乘一个 $\\boldsymbol{x}_{1}^{\\wedge}$,\n$$ s_{1} \\boldsymbol{x}_{1}^{\\wedge} \\boldsymbol{x}_{1}=0=s_{2} \\boldsymbol{x}_{1}^{\\wedge} \\boldsymbol{R} \\boldsymbol{x}_{2}+\\boldsymbol{x}_{1}^{\\wedge} \\boldsymbol{t} $$\n右侧可看成 $s_{2}$的一个方程,可以根据它直接求得 $s_{2}$ 。有了 $s_{2},s_{1}$也非常容易求出。于是,我们就得到了两个帧下的点的深度,确定了它们的空间坐标。由于噪声的存在,我们估得的 $\\boldsymbol{R}, \\boldsymbol{t}$,不一定精确使上式为零,所以更常见的做法求最小二乘解而不是零解。\n编程练习：三角测量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 #include \u003ciostream\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/calib3d/calib3d.hpp\u003e // #include \"extra.h\" // used in opencv2 using namespace std; using namespace cv; void find_feature_matches ( const Mat\u0026 img_1, const Mat\u0026 img_2, std::vector\u003cKeyPoint\u003e\u0026 keypoints_1, std::vector\u003cKeyPoint\u003e\u0026 keypoints_2, std::vector\u003c DMatch \u003e\u0026 matches ); void pose_estimation_2d2d ( const std::vector\u003cKeyPoint\u003e\u0026 keypoints_1, const std::vector\u003cKeyPoint\u003e\u0026 keypoints_2, const std::vector\u003c DMatch \u003e\u0026 matches, Mat\u0026 R, Mat\u0026 t ); void triangulation ( const vector\u003cKeyPoint\u003e\u0026 keypoint_1, const vector\u003cKeyPoint\u003e\u0026 keypoint_2, const std::vector\u003c DMatch \u003e\u0026 matches, const Mat\u0026 R, const Mat\u0026 t, vector\u003cPoint3d\u003e\u0026 points ); // 像素坐标转相机归一化坐标 Point2f pixel2cam( const Point2d\u0026 p, const Mat\u0026 K ); int main ( int argc, char** argv ) { if ( argc != 3 ) { cout\u003c\u003c\"usage: triangulation img1 img2\"\u003c\u003cendl; return 1; } //-- 读取图像 Mat img_1 = imread ( argv[1], CV_LOAD_IMAGE_COLOR ); Mat img_2 = imread ( argv[2], CV_LOAD_IMAGE_COLOR ); vector\u003cKeyPoint\u003e keypoints_1, keypoints_2; vector\u003cDMatch\u003e matches; find_feature_matches ( img_1, img_2, keypoints_1, keypoints_2, matches ); cout\u003c\u003c\"一共找到了\"\u003c\u003cmatches.size() \u003c\u003c\"组匹配点\"\u003c\u003cendl; //-- 估计两张图像间运动 Mat R,t; pose_estimation_2d2d ( keypoints_1, keypoints_2, matches, R, t ); //-- 三角化 vector\u003cPoint3d\u003e points; triangulation( keypoints_1, keypoints_2, matches, R, t, points ); //-- 验证三角化点与特征点的重投影关系 Mat K = ( Mat_\u003cdouble\u003e ( 3,3 ) \u003c\u003c 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1 ); for ( int i=0; i\u003cmatches.size(); i++ ) { Point2d pt1_cam = pixel2cam( keypoints_1[ matches[i].queryIdx ].pt, K ); Point2d pt1_cam_3d( points[i].x/points[i].z, points[i].y/points[i].z ); cout\u003c\u003c\"point in the first camera frame: \"\u003c\u003cpt1_cam\u003c\u003cendl; cout\u003c\u003c\"point projected from 3D \"\u003c\u003cpt1_cam_3d\u003c\u003c\", d=\"\u003c\u003cpoints[i].z\u003c\u003cendl; // 第二个图 Point2f pt2_cam = pixel2cam( keypoints_2[ matches[i].trainIdx ].pt, K ); Mat pt2_trans = R*( Mat_\u003cdouble\u003e(3,1) \u003c\u003c points[i].x, points[i].y, points[i].z ) + t; pt2_trans /= pt2_trans.at\u003cdouble\u003e(2,0); cout\u003c\u003c\"point in the second camera frame: \"\u003c\u003cpt2_cam\u003c\u003cendl; cout\u003c\u003c\"point reprojected from second frame: \"\u003c\u003cpt2_trans.t()\u003c\u003cendl; cout\u003c\u003cendl; } return 0; } void find_feature_matches ( const Mat\u0026 img_1, const Mat\u0026 img_2, std::vector\u003cKeyPoint\u003e\u0026 keypoints_1, std::vector\u003cKeyPoint\u003e\u0026 keypoints_2, std::vector\u003c DMatch \u003e\u0026 matches ) { //-- 初始化 Mat descriptors_1, descriptors_2; // used in OpenCV3 Ptr\u003cFeatureDetector\u003e detector = ORB::create(); Ptr\u003cDescriptorExtractor\u003e descriptor = ORB::create(); // use this if you are in OpenCV2 // Ptr\u003cFeatureDetector\u003e detector = FeatureDetector::create ( \"ORB\" ); // Ptr\u003cDescriptorExtractor\u003e descriptor = DescriptorExtractor::create ( \"ORB\" ); Ptr\u003cDescriptorMatcher\u003e matcher = DescriptorMatcher::create(\"BruteForce-Hamming\"); //-- 第一步:检测 Oriented FAST 角点位置 detector-\u003edetect ( img_1,keypoints_1 ); detector-\u003edetect ( img_2,keypoints_2 ); //-- 第二步:根据角点位置计算 BRIEF 描述子 descriptor-\u003ecompute ( img_1, keypoints_1, descriptors_1 ); descriptor-\u003ecompute ( img_2, keypoints_2, descriptors_2 ); //-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离 vector\u003cDMatch\u003e match; // BFMatcher matcher ( NORM_HAMMING ); matcher-\u003ematch ( descriptors_1, descriptors_2, match ); //-- 第四步:匹配点对筛选 double min_dist=10000, max_dist=0; //找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离 for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { double dist = match[i].distance; if ( dist \u003c min_dist ) min_dist = dist; if ( dist \u003e max_dist ) max_dist = dist; } printf ( \"-- Max dist : %f \\n\", max_dist ); printf ( \"-- Min dist : %f \\n\", min_dist ); //当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限. for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { if ( match[i].distance \u003c= max ( 2*min_dist, 30.0 ) ) { matches.push_back ( match[i] ); } } } void pose_estimation_2d2d ( const std::vector\u003cKeyPoint\u003e\u0026 keypoints_1, const std::vector\u003cKeyPoint\u003e\u0026 keypoints_2, const std::vector\u003c DMatch \u003e\u0026 matches, Mat\u0026 R, Mat\u0026 t ) { // 相机内参,TUM Freiburg2 Mat K = ( Mat_\u003cdouble\u003e ( 3,3 ) \u003c\u003c 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1 ); //-- 把匹配点转换为vector\u003cPoint2f\u003e的形式 vector\u003cPoint2f\u003e points1; vector\u003cPoint2f\u003e points2; for ( int i = 0; i \u003c ( int ) matches.size(); i++ ) { points1.push_back ( keypoints_1[matches[i].queryIdx].pt ); points2.push_back ( keypoints_2[matches[i].trainIdx].pt ); } //-- 计算基础矩阵 Mat fundamental_matrix; fundamental_matrix = findFundamentalMat ( points1, points2, CV_FM_8POINT ); cout\u003c\u003c\"fundamental_matrix is \"\u003c\u003cendl\u003c\u003c fundamental_matrix\u003c\u003cendl; //-- 计算本质矩阵 Point2d principal_point ( 325.1, 249.7 );\t//相机主点, TUM dataset标定值 int focal_length = 521;\t//相机焦距, TUM dataset标定值 Mat essential_matrix; essential_matrix = findEssentialMat ( points1, points2, focal_length, principal_point ); cout\u003c\u003c\"essential_matrix is \"\u003c\u003cendl\u003c\u003c essential_matrix\u003c\u003cendl; //-- 计算单应矩阵 Mat homography_matrix; homography_matrix = findHomography ( points1, points2, RANSAC, 3 ); cout\u003c\u003c\"homography_matrix is \"\u003c\u003cendl\u003c\u003chomography_matrix\u003c\u003cendl; //-- 从本质矩阵中恢复旋转和平移信息. recoverPose ( essential_matrix, points1, points2, R, t, focal_length, principal_point ); cout\u003c\u003c\"R is \"\u003c\u003cendl\u003c\u003cR\u003c\u003cendl; cout\u003c\u003c\"t is \"\u003c\u003cendl\u003c\u003ct\u003c\u003cendl; } void triangulation ( const vector\u003c KeyPoint \u003e\u0026 keypoint_1, const vector\u003c KeyPoint \u003e\u0026 keypoint_2, const std::vector\u003c DMatch \u003e\u0026 matches, const Mat\u0026 R, const Mat\u0026 t, vector\u003c Point3d \u003e\u0026 points ) { Mat T1 = (Mat_\u003cfloat\u003e (3,4) \u003c\u003c 1,0,0,0, 0,1,0,0, 0,0,1,0); Mat T2 = (Mat_\u003cfloat\u003e (3,4) \u003c\u003c R.at\u003cdouble\u003e(0,0), R.at\u003cdouble\u003e(0,1), R.at\u003cdouble\u003e(0,2), t.at\u003cdouble\u003e(0,0), R.at\u003cdouble\u003e(1,0), R.at\u003cdouble\u003e(1,1), R.at\u003cdouble\u003e(1,2), t.at\u003cdouble\u003e(1,0), R.at\u003cdouble\u003e(2,0), R.at\u003cdouble\u003e(2,1), R.at\u003cdouble\u003e(2,2), t.at\u003cdouble\u003e(2,0) ); Mat K = ( Mat_\u003cdouble\u003e ( 3,3 ) \u003c\u003c 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1 ); vector\u003cPoint2f\u003e pts_1, pts_2; for ( DMatch m:matches ) { // 将像素坐标转换至相机坐标 pts_1.push_back ( pixel2cam( keypoint_1[m.queryIdx].pt, K) ); pts_2.push_back ( pixel2cam( keypoint_2[m.trainIdx].pt, K) ); } Mat pts_4d; cv::triangulatePoints( T1, T2, pts_1, pts_2, pts_4d ); // 转换成非齐次坐标 for ( int i=0; i\u003cpts_4d.cols; i++ ) { Mat x = pts_4d.col(i); x /= x.at\u003cfloat\u003e(3,0); // 归一化 Point3d p ( x.at\u003cfloat\u003e(0,0), x.at\u003cfloat\u003e(1,0), x.at\u003cfloat\u003e(2,0) ); points.push_back( p ); } } Point2f pixel2cam ( const Point2d\u0026 p, const Mat\u0026 K ) { return Point2f ( ( p.x - K.at\u003cdouble\u003e(0,2) ) / K.at\u003cdouble\u003e(0,0), ( p.y - K.at\u003cdouble\u003e(1,2) ) / K.at\u003cdouble\u003e(1,1) ); } 3D-2D: PnP PnP(Perspective-n-Point)是求解 3D 到 2D 点对运动的方法。它描述了当我们知道n 个 3D 空间点以及它们的投影位置时,如何估计相机所在的位姿。PnP 问题有很多种求解方法,例如用三对点估计位姿的 P3P,直接线性变换(DLT),EPnP(Efficient PnP),UPnP 等等。此外,还能用非线性优化的方式,构建最 小二乘问题并迭代求解,也就是 Bundle Adjustment。\n直接线性变换(Direct Linear Transform, DLT) 考虑某个空间点 $\\boldsymbol{P}$ ,它的齐次坐标为 $\\boldsymbol{P}=(X, Y, Z, 1)^{T}$ 。在图像 $\\boldsymbol{I_{1}}$ 中,投影到特征点$\\boldsymbol{x}_{1}=\\left(u_{1}, v_{1}, 1\\right)^{T}$ (以归一化平面齐次坐标表示)。此时相机的位姿 $\\boldsymbol{R}, \\boldsymbol{t}$是未知的。与单应矩阵的求解类似,我们定义增广矩阵 $[\\boldsymbol{R} | \\boldsymbol{t}]$ 为一个 3 × 4 的矩阵,包含了旋转与平移信息 。\n$$ s\\left(\\begin{array}{c}{u_{1}} \\\\ {v_{1}} \\\\ {1}\\end{array}\\right)=\\left(\\begin{array}{cccc}{t_{1}} \u0026 {t_{2}} \u0026 {t_{3}} \u0026 {t_{4}} \\\\ {t_{5}} \u0026 {t_{6}} \u0026 {t_{7}} \u0026 {t_{8}} \\\\ {t_{9}} \u0026 {t_{10}} \u0026 {t_{11}} \u0026 {t_{12}}\\end{array}\\right)\\left(\\begin{array}{l}{X} \\\\ {Y} \\\\ {Z} \\\\ {1}\\end{array}\\right) $$\n用最后一行把 s 消去,得到两个约束:\n$$ u_{1}=\\frac{t_{1} X+t_{2} Y+t_{3} Z+t_{4}}{t_{9} X+t_{10} Y+t_{11} Z+t_{12}} \\quad v_{1}=\\frac{t_{5} X+t_{6} Y+t_{7} Z+t_{8}}{t_{9} X+t_{10} Y+t_{11} Z+t_{12}} $$\n定义：\n$$ t_{1}=\\left(t_{1}, t_{2}, t_{3}, t_{4}\\right)^{T}, t_{2}=\\left(t_{5}, t_{6}, t_{7}, t_{8}\\right)^{T}, t_{3}=\\left(t_{9}, t_{10}, t_{11}, t_{12}\\right)^{T} $$\n于是有:\n$$ \\begin{array}{l}{\\boldsymbol{t}_{1}^{T} \\boldsymbol{P}-\\boldsymbol{t}_{3}^{T} \\boldsymbol{P} u_{1}=0} \\ {\\boldsymbol{t}_{2}^{T} \\boldsymbol{P}-\\boldsymbol{t}_{3}^{T} \\boldsymbol{P} v_{1}=0}\\end{array} $$\n假设一共有 N 个特征点,可以列出线性方程组:\n$$ \\left(\\begin{array}{ccc}{\\boldsymbol{P}_{1}^{T}} \u0026 {0} \u0026 {-u_{1} \\boldsymbol{P}_{1}^{T}} \\\\ {0} \u0026 {\\boldsymbol{P}_{1}^{T}} \u0026 {-v_{1} \\boldsymbol{P}_{1}^{T}} \\\\ {\\vdots} \u0026 {\\vdots} \u0026 {\\vdots} \\\\ {\\boldsymbol{P}_{N}^{T}} \u0026 {0} \u0026 {-u_{N} \\boldsymbol{P}_{N}^{T}} \\\\ {0} \u0026 {\\boldsymbol{P}_{N}^{T}} \u0026 {-v_{N} \\boldsymbol{P}_{N}^{T}}\\end{array}\\right)\\left(\\begin{array}{l}{t_{1}} \\\\ {t_{2}} \\\\ {t_{3}}\\end{array}\\right)=0 $$\n由于 $\\boldsymbol{t}$一共有 12 维,因此最少通过六对匹配点,即可实现矩阵 $\\boldsymbol{T}$ 的线性求解,这种方法(也)称为直接线性变换(Direct Linear Transform, DLT)。在 DLT 求解可以由 QR 分解完成 ,相当于把结果从矩阵空间重新投影到 SE(3) 流形上,转换成旋转和平移两部分。\nBundle Adjustment 前面说的线性方法,往往是先求相机位姿,再求空间点位置,而非线性优化则是把它们都看成优化变量,放在一起优化。这是一种非常通用的求解方式,我们可以用它对 PnP 或 ICP 给出的结果进行优化。在 PnP 中,这个 Bundle Adjustment 问题,是一个最小化重投影误差(Reprojection error)的问题。\n考虑 n 个三维空间点 P 和它们的投影 p,我们希望计算相机的位姿 $\\boldsymbol{R}, \\boldsymbol{t}$它的李代数表示为 $\\boldsymbol{\\xi}$。假设某空间点坐标为：$\\boldsymbol{P}_{i}=\\left[X_{i}, Y_{i}, Z_{i}\\right]^{T}$ ,其投影的像素坐标为 $\\boldsymbol{u}_{i}=\\left[u_{i}, v_{i}\\right]^{T}$ 。\n像素位置与空间点位置的关系如下:\n$$ s_{i}\\left[\\begin{array}{c}{u_{i}} \\\\ {v_{i}} \\\\ {1}\\end{array}\\right]=\\boldsymbol{K} \\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right)\\left[\\begin{array}{c}{X_{i}} \\\\ {Y_{i}} \\\\ {Z_{i}} \\\\ {1}\\end{array}\\right] $$\n写成矩阵形式就是:\n$$ s_{i} \\boldsymbol{u}_{i}=\\boldsymbol{K} \\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right) \\boldsymbol{P}_{i} $$\n我们把误差求和,构建最小二乘问题,然后寻找最好的相机位姿,使它最小化:\n$$ \\xi^{*}=\\arg \\min _{\\xi} \\frac{1}{2} \\sum_{i=1}^{n}\\left|u_{i}-\\frac{1}{s_{i}} K \\exp \\left(\\xi^{\\wedge}\\right) P_{i}\\right|_{2}^{2} $$ 该问题的误差项,是将像素坐标(观测到的投影位置)与 3D 点按照当前估计的位姿进行投影得到的位置相比较得到的误差,所以称之为重投影误差。可以通过 G-N, L-M 等优化算法进行求解。\n编程练习：PnP 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 #include \u003ciostream\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/calib3d/calib3d.hpp\u003e #include \u003cEigen/Core\u003e #include \u003cEigen/Geometry\u003e #include \u003cg2o/core/base_vertex.h\u003e #include \u003cg2o/core/base_unary_edge.h\u003e #include \u003cg2o/core/block_solver.h\u003e #include \u003cg2o/core/optimization_algorithm_levenberg.h\u003e #include \u003cg2o/solvers/csparse/linear_solver_csparse.h\u003e #include \u003cg2o/types/sba/types_six_dof_expmap.h\u003e #include \u003cchrono\u003e using namespace std; using namespace cv; void find_feature_matches ( const Mat\u0026 img_1, const Mat\u0026 img_2, std::vector\u003cKeyPoint\u003e\u0026 keypoints_1, std::vector\u003cKeyPoint\u003e\u0026 keypoints_2, std::vector\u003c DMatch \u003e\u0026 matches ); // 像素坐标转相机归一化坐标 Point2d pixel2cam ( const Point2d\u0026 p, const Mat\u0026 K ); void bundleAdjustment ( const vector\u003cPoint3f\u003e points_3d, const vector\u003cPoint2f\u003e points_2d, const Mat\u0026 K, Mat\u0026 R, Mat\u0026 t ); int main ( int argc, char** argv ) { if ( argc != 5 ) { cout\u003c\u003c\"usage: pose_estimation_3d2d img1 img2 depth1 depth2\"\u003c\u003cendl; return 1; } //-- 读取图像 Mat img_1 = imread ( argv[1], CV_LOAD_IMAGE_COLOR ); Mat img_2 = imread ( argv[2], CV_LOAD_IMAGE_COLOR ); vector\u003cKeyPoint\u003e keypoints_1, keypoints_2; vector\u003cDMatch\u003e matches; find_feature_matches ( img_1, img_2, keypoints_1, keypoints_2, matches ); cout\u003c\u003c\"一共找到了\"\u003c\u003cmatches.size() \u003c\u003c\"组匹配点\"\u003c\u003cendl; // 建立3D点 Mat d1 = imread ( argv[3], CV_LOAD_IMAGE_UNCHANGED ); // 深度图为16位无符号数，单通道图像 Mat K = ( Mat_\u003cdouble\u003e ( 3,3 ) \u003c\u003c 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1 ); vector\u003cPoint3f\u003e pts_3d; vector\u003cPoint2f\u003e pts_2d; for ( DMatch m:matches ) { ushort d = d1.ptr\u003cunsigned short\u003e (int ( keypoints_1[m.queryIdx].pt.y )) [ int ( keypoints_1[m.queryIdx].pt.x ) ]; if ( d == 0 ) // bad depth continue; float dd = d/5000.0; Point2d p1 = pixel2cam ( keypoints_1[m.queryIdx].pt, K ); pts_3d.push_back ( Point3f ( p1.x*dd, p1.y*dd, dd ) ); pts_2d.push_back ( keypoints_2[m.trainIdx].pt ); } cout\u003c\u003c\"3d-2d pairs: \"\u003c\u003cpts_3d.size() \u003c\u003cendl; Mat r, t; solvePnP ( pts_3d, pts_2d, K, Mat(), r, t, false ); // 调用OpenCV 的 PnP 求解，可选择EPNP，DLS等方法 Mat R; cv::Rodrigues ( r, R ); // r为旋转向量形式，用Rodrigues公式转换为矩阵 cout\u003c\u003c\"R=\"\u003c\u003cendl\u003c\u003cR\u003c\u003cendl; cout\u003c\u003c\"t=\"\u003c\u003cendl\u003c\u003ct\u003c\u003cendl; cout\u003c\u003c\"calling bundle adjustment\"\u003c\u003cendl; bundleAdjustment ( pts_3d, pts_2d, K, R, t ); } void find_feature_matches ( const Mat\u0026 img_1, const Mat\u0026 img_2, std::vector\u003cKeyPoint\u003e\u0026 keypoints_1, std::vector\u003cKeyPoint\u003e\u0026 keypoints_2, std::vector\u003c DMatch \u003e\u0026 matches ) { //-- 初始化 Mat descriptors_1, descriptors_2; // used in OpenCV3 Ptr\u003cFeatureDetector\u003e detector = ORB::create(); Ptr\u003cDescriptorExtractor\u003e descriptor = ORB::create(); // use this if you are in OpenCV2 // Ptr\u003cFeatureDetector\u003e detector = FeatureDetector::create ( \"ORB\" ); // Ptr\u003cDescriptorExtractor\u003e descriptor = DescriptorExtractor::create ( \"ORB\" ); Ptr\u003cDescriptorMatcher\u003e matcher = DescriptorMatcher::create ( \"BruteForce-Hamming\" ); //-- 第一步:检测 Oriented FAST 角点位置 detector-\u003edetect ( img_1,keypoints_1 ); detector-\u003edetect ( img_2,keypoints_2 ); //-- 第二步:根据角点位置计算 BRIEF 描述子 descriptor-\u003ecompute ( img_1, keypoints_1, descriptors_1 ); descriptor-\u003ecompute ( img_2, keypoints_2, descriptors_2 ); //-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离 vector\u003cDMatch\u003e match; // BFMatcher matcher ( NORM_HAMMING ); matcher-\u003ematch ( descriptors_1, descriptors_2, match ); //-- 第四步:匹配点对筛选 double min_dist=10000, max_dist=0; //找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离 for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { double dist = match[i].distance; if ( dist \u003c min_dist ) min_dist = dist; if ( dist \u003e max_dist ) max_dist = dist; } printf ( \"-- Max dist : %f \\n\", max_dist ); printf ( \"-- Min dist : %f \\n\", min_dist ); //当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限. for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { if ( match[i].distance \u003c= max ( 2*min_dist, 30.0 ) ) { matches.push_back ( match[i] ); } } } Point2d pixel2cam ( const Point2d\u0026 p, const Mat\u0026 K ) { return Point2d ( ( p.x - K.at\u003cdouble\u003e ( 0,2 ) ) / K.at\u003cdouble\u003e ( 0,0 ), ( p.y - K.at\u003cdouble\u003e ( 1,2 ) ) / K.at\u003cdouble\u003e ( 1,1 ) ); } // 在这个图优化中,节点和边的选择为: // 1.节点:第二个相机的位姿节点 ξ ∈ se(3),以及所有特征点的空间位置 P ∈ R 3 。 // 2.边:每个 3D 点在第二个相机中的投影 void bundleAdjustment ( const vector\u003c Point3f \u003e points_3d, const vector\u003c Point2f \u003e points_2d, const Mat\u0026 K, Mat\u0026 R, Mat\u0026 t ) { // 初始化g2o typedef g2o::BlockSolver\u003c g2o::BlockSolverTraits\u003c6,3\u003e \u003e Block; // pose 维度为 6, landmark 维度为 3 Block::LinearSolverType* linearSolver = new g2o::LinearSolverCSparse\u003cBlock::PoseMatrixType\u003e(); // 线性方程求解器 Block* solver_ptr = new Block ( linearSolver ); // 矩阵块求解器 g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg ( solver_ptr ); g2o::SparseOptimizer optimizer; optimizer.setAlgorithm ( solver ); // vertex g2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap(); // camera pose Eigen::Matrix3d R_mat; R_mat \u003c\u003c R.at\u003cdouble\u003e ( 0,0 ), R.at\u003cdouble\u003e ( 0,1 ), R.at\u003cdouble\u003e ( 0,2 ), R.at\u003cdouble\u003e ( 1,0 ), R.at\u003cdouble\u003e ( 1,1 ), R.at\u003cdouble\u003e ( 1,2 ), R.at\u003cdouble\u003e ( 2,0 ), R.at\u003cdouble\u003e ( 2,1 ), R.at\u003cdouble\u003e ( 2,2 ); pose-\u003esetId ( 0 ); pose-\u003esetEstimate ( g2o::SE3Quat ( R_mat, Eigen::Vector3d ( t.at\u003cdouble\u003e ( 0,0 ), t.at\u003cdouble\u003e ( 1,0 ), t.at\u003cdouble\u003e ( 2,0 ) ) ) ); optimizer.addVertex ( pose ); int index = 1; for ( const Point3f p:points_3d ) // landmarks { g2o::VertexSBAPointXYZ* point = new g2o::VertexSBAPointXYZ(); point-\u003esetId ( index++ ); point-\u003esetEstimate ( Eigen::Vector3d ( p.x, p.y, p.z ) ); point-\u003esetMarginalized ( true ); // g2o 中必须设置 marg 参见第十讲内容 optimizer.addVertex ( point ); } // parameter: camera intrinsics g2o::CameraParameters* camera = new g2o::CameraParameters ( K.at\u003cdouble\u003e ( 0,0 ), Eigen::Vector2d ( K.at\u003cdouble\u003e ( 0,2 ), K.at\u003cdouble\u003e ( 1,2 ) ), 0 ); camera-\u003esetId ( 0 ); optimizer.addParameter ( camera ); // edges index = 1; for ( const Point2f p:points_2d ) { //投影边方程 g2o::EdgeProjectXYZ2UV* edge = new g2o::EdgeProjectXYZ2UV(); edge-\u003esetId ( index ); // 空间点位置 edge-\u003esetVertex ( 0, dynamic_cast\u003cg2o::VertexSBAPointXYZ*\u003e ( optimizer.vertex ( index ) ) ); // 位姿 edge-\u003esetVertex ( 1, pose ); edge-\u003esetMeasurement ( Eigen::Vector2d ( p.x, p.y ) ); edge-\u003esetParameterId ( 0,0 ); edge-\u003esetInformation ( Eigen::Matrix2d::Identity() ); optimizer.addEdge ( edge ); index++; } chrono::steady_clock::time_point t1 = chrono::steady_clock::now(); optimizer.setVerbose ( true ); optimizer.initializeOptimization(); optimizer.optimize ( 100 ); chrono::steady_clock::time_point t2 = chrono::steady_clock::now(); chrono::duration\u003cdouble\u003e time_used = chrono::duration_cast\u003cchrono::duration\u003cdouble\u003e\u003e ( t2-t1 ); cout\u003c\u003c\"optimization costs time: \"\u003c\u003ctime_used.count() \u003c\u003c\" seconds.\"\u003c\u003cendl; cout\u003c\u003cendl\u003c\u003c\"after optimization:\"\u003c\u003cendl; cout\u003c\u003c\"T=\"\u003c\u003cendl\u003c\u003cEigen::Isometry3d ( pose-\u003eestimate() ).matrix() \u003c\u003cendl; } 3D-3D: ICP 假设我们有一组配对好的 3D 点(比如我们对两个 RGB-D 图像进行了匹配):\n$$ \\boldsymbol{P}=(\\boldsymbol{p}_{1}, \\ldots, \\boldsymbol{p}_{n}), \\quad \\boldsymbol{P}^{\\prime}=(\\boldsymbol{p}_{1}^{\\prime}, \\ldots, \\boldsymbol{p}_{n}^{\\prime}) $$\n现在,想要找一个欧氏变换 $\\boldsymbol{R}, \\boldsymbol{t}$，使得:\n$$ \\forall i, \\boldsymbol{p}_{i}=\\boldsymbol{R} \\boldsymbol{p}_{i}^{\\prime}+\\boldsymbol{t} $$\n这个问题可以用迭代最近点(Iterative Closest Point, ICP)求解。ICP 的求解也分为两种方式:利用线性代数的求解(主要是 SVD),以及利用非线性优化方式的求解(类似于 Bundle Adjustment)。\nSVD方法 我们先定义第 i对点的误差项:\n$$ e_{i}=p_{i}-\\left(R p_{i}^{\\prime}+t\\right) $$\n然后,构建最小二乘问题,求使误差平方和达到极小的 $\\boldsymbol{R}, \\boldsymbol{t} $：\n$$ \\min _{\\boldsymbol{R}, \\boldsymbol{t}} J=\\frac{1}{2} \\sum_{i=1}^{n}\\left|\\left(\\boldsymbol{p}_{i}-\\left(\\boldsymbol{R} \\boldsymbol{p}_{i}^{\\prime}+\\boldsymbol{t}\\right)\\right)\\right|_{2}^{2} $$\n首先,定义两组点的质心:\n$$ \\boldsymbol{p}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\boldsymbol{p}_{i}\\right), \\quad \\boldsymbol{p}^{\\prime}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\boldsymbol{p}_{i}^{\\prime}\\right) $$\n随后,在误差函数中,我们作如下的处理:\n$$ \\begin{aligned} \\frac{1}{2} \\sum_{i=1}^{n}\\left|\\boldsymbol{p}_{i}-\\left(\\boldsymbol{R} \\boldsymbol{p}_{i}^{\\prime}+\\boldsymbol{t}\\right)\\right|^{2} \u0026=\\frac{1}{2} \\sum_{i=1}^{n}\\left|\\boldsymbol{p}_{i}-\\boldsymbol{R} \\boldsymbol{p}_{i}^{\\prime}-\\boldsymbol{t}-\\boldsymbol{p}+\\boldsymbol{R} \\boldsymbol{p}^{\\prime}+\\boldsymbol{p}-\\boldsymbol{R} \\boldsymbol{p}^{\\prime}\\right|^{2} \\\\ \u0026=\\frac{1}{2} \\sum_{i=1}^{n}\\left|\\left(\\boldsymbol{p}_{i}-\\boldsymbol{p}-\\boldsymbol{R}\\left(\\boldsymbol{p}_{i}^{\\prime}-\\boldsymbol{p}^{\\prime}\\right)\\right)+\\left(\\boldsymbol{p}-\\boldsymbol{R} \\boldsymbol{p}^{\\prime}-\\boldsymbol{t}\\right)\\right|^{2} \\\\ \u0026=\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\left|\\boldsymbol{p}_{i}-\\boldsymbol{p}-\\boldsymbol{R}\\left(\\boldsymbol{p}_{i}^{\\prime}-\\boldsymbol{p}^{\\prime}\\right)\\right|^{2}+\\left|\\boldsymbol{p}-\\boldsymbol{R} \\boldsymbol{p}^{\\prime}-\\boldsymbol{t}\\right|^{2}+\\right.\\\\ \u0026\\left.2\\left(\\boldsymbol{p}_{i}-\\boldsymbol{p}-\\boldsymbol{R}\\left(\\boldsymbol{p}_{i}^{\\prime}-\\boldsymbol{p}^{\\prime}\\right)\\right)^{T}\\left(\\boldsymbol{p}-\\boldsymbol{R} \\boldsymbol{p}^{\\prime}-\\boldsymbol{t}\\right)\\right) \\end{aligned} $$\n注意到交叉项部分中,$\\left(\\boldsymbol{p}_{i}-\\boldsymbol{p}-\\boldsymbol{R}\\left(\\boldsymbol{p}_{i}^{\\prime}-\\boldsymbol{p}^{\\prime}\\right)\\right)$ 在求和之后是为零的,因此优化目标函 数可以简化为:\n$$ \\min _{\\boldsymbol{R}, \\boldsymbol{t}} J=\\frac{1}{2} \\sum_{i=1}^{n}\\left|\\boldsymbol{p}_{i}-\\boldsymbol{p}-\\boldsymbol{R}\\left(\\boldsymbol{p}_{i}^{\\prime}-\\boldsymbol{p}^{\\prime}\\right)\\right|^{2}+\\left|\\boldsymbol{p}-\\boldsymbol{R} \\boldsymbol{p}^{\\prime}-\\boldsymbol{t}\\right|^{2} $$\nICP 可以分为以下三个步骤求解:\n1 . 计算两组点的质心位置$\\boldsymbol{p},\\boldsymbol{p}^{\\prime}$ ,然后计算每个点的去质心坐标:\n$$ \\boldsymbol{q}_{i}=\\boldsymbol{p}_{i}-\\boldsymbol{p}, \\quad \\boldsymbol{q}_{i}^{\\prime}=\\boldsymbol{p}_{i}^{\\prime}-\\boldsymbol{p}^{\\prime} $$\n2 . 根据以下优化问题计算旋转矩阵:\n$$ \\boldsymbol{R}^{*}=\\arg \\min _{\\boldsymbol{R}} \\frac{1}{2} \\sum_{i=1}^{n}\\left|\\boldsymbol{q}_{i}-\\boldsymbol{R} \\boldsymbol{q}_{i}^{\\prime}\\right|^{2} $$\n3 . 根据第二步的 $\\boldsymbol{R}$,计算 $\\boldsymbol{t}$:\n展开关于 $\\boldsymbol{R}$的误差项,得:\n$$ \\frac{1}{2} \\sum_{i=1}^{n}\\left|\\boldsymbol{q}_{i}-\\boldsymbol{R} \\boldsymbol{q}_{i}^{\\prime}\\right|^{2}=\\frac{1}{2} \\sum_{i=1}^{n} \\boldsymbol{q}_{i}^{T} \\boldsymbol{q}_{i}+\\boldsymbol{q}_{i}^{\\prime T} \\boldsymbol{R}^{T} \\boldsymbol{R} \\boldsymbol{q}_{i}^{\\prime}-2 \\boldsymbol{q}_{i}^{T} \\boldsymbol{R} \\boldsymbol{q}_{i}^{\\prime} $$\n注意到第一项和 $\\boldsymbol{R}$ 无关,第二项由于 $\\boldsymbol{R}^{T} \\boldsymbol{R}=\\boldsymbol{I}$,亦与 R 无关。因此,实际上优化目标函数变为:\n$$ \\sum_{i=1}^{n}-\\boldsymbol{q}_{i}^{T} \\boldsymbol{R} \\boldsymbol{q}_{i}^{\\prime}=\\sum_{i=1}^{n}-\\operatorname{tr}\\left(\\boldsymbol{R} \\boldsymbol{q}_{i}^{\\prime} \\boldsymbol{q}_{i}^{T}\\right)=-\\operatorname{tr}\\left(\\boldsymbol{R} \\sum_{i=1}^{n} \\boldsymbol{q}_{i}^{\\prime} \\boldsymbol{q}_{i}^{T}\\right) $$\n由SVD分解，定义：\n$$ \\boldsymbol{W}=\\sum_{i=1}^{n} \\boldsymbol{q}_{i}^{\\prime} \\boldsymbol{q}_{i}^{T} $$\n对 $\\boldsymbol{W}$ 进行 SVD 分解:\n$$ \\boldsymbol{W}=\\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T} $$\n当 $\\boldsymbol{W}$ 满秩时,$\\boldsymbol{R}$ 为:\n$$ \\boldsymbol{R}=\\boldsymbol{U} \\boldsymbol{V}^{T} $$\n非线性优化方法 目标函数可以写成: $$ \\min _{\\xi}=\\frac{1}{2} \\sum_{i=1}^{n}\\left|\\left(\\boldsymbol{p}_{i}-\\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right) \\boldsymbol{p}_{i}^{\\prime}\\right)\\right|_{2}^{2} $$\n编程练习：求解 ICP 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 #include \u003ciostream\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/calib3d/calib3d.hpp\u003e #include \u003cEigen/Core\u003e #include \u003cEigen/Geometry\u003e #include \u003cEigen/SVD\u003e #include \u003cg2o/core/base_vertex.h\u003e #include \u003cg2o/core/base_unary_edge.h\u003e #include \u003cg2o/core/block_solver.h\u003e #include \u003cg2o/core/optimization_algorithm_gauss_newton.h\u003e #include \u003cg2o/solvers/eigen/linear_solver_eigen.h\u003e #include \u003cg2o/types/sba/types_six_dof_expmap.h\u003e #include \u003cchrono\u003e using namespace std; using namespace cv; void find_feature_matches ( const Mat\u0026 img_1, const Mat\u0026 img_2, std::vector\u003cKeyPoint\u003e\u0026 keypoints_1, std::vector\u003cKeyPoint\u003e\u0026 keypoints_2, std::vector\u003c DMatch \u003e\u0026 matches ); // 像素坐标转相机归一化坐标 Point2d pixel2cam ( const Point2d\u0026 p, const Mat\u0026 K ); void pose_estimation_3d3d ( const vector\u003cPoint3f\u003e\u0026 pts1, const vector\u003cPoint3f\u003e\u0026 pts2, Mat\u0026 R, Mat\u0026 t ); void bundleAdjustment( const vector\u003cPoint3f\u003e\u0026 points_3d, const vector\u003cPoint3f\u003e\u0026 points_2d, Mat\u0026 R, Mat\u0026 t ); // g2o edge class EdgeProjectXYZRGBDPoseOnly : public g2o::BaseUnaryEdge\u003c3, Eigen::Vector3d, g2o::VertexSE3Expmap\u003e { public: EIGEN_MAKE_ALIGNED_OPERATOR_NEW; EdgeProjectXYZRGBDPoseOnly( const Eigen::Vector3d\u0026 point ) : _point(point) {} virtual void computeError() { const g2o::VertexSE3Expmap* pose = static_cast\u003cconst g2o::VertexSE3Expmap*\u003e ( _vertices[0] ); // measurement is p, point is p' _error = _measurement - pose-\u003eestimate().map( _point ); } virtual void linearizeOplus() { g2o::VertexSE3Expmap* pose = static_cast\u003cg2o::VertexSE3Expmap *\u003e(_vertices[0]); g2o::SE3Quat T(pose-\u003eestimate()); Eigen::Vector3d xyz_trans = T.map(_point); double x = xyz_trans[0]; double y = xyz_trans[1]; double z = xyz_trans[2]; _jacobianOplusXi(0,0) = 0; _jacobianOplusXi(0,1) = -z; _jacobianOplusXi(0,2) = y; _jacobianOplusXi(0,3) = -1; _jacobianOplusXi(0,4) = 0; _jacobianOplusXi(0,5) = 0; _jacobianOplusXi(1,0) = z; _jacobianOplusXi(1,1) = 0; _jacobianOplusXi(1,2) = -x; _jacobianOplusXi(1,3) = 0; _jacobianOplusXi(1,4) = -1; _jacobianOplusXi(1,5) = 0; _jacobianOplusXi(2,0) = -y; _jacobianOplusXi(2,1) = x; _jacobianOplusXi(2,2) = 0; _jacobianOplusXi(2,3) = 0; _jacobianOplusXi(2,4) = 0; _jacobianOplusXi(2,5) = -1; } bool read ( istream\u0026 in ) {} bool write ( ostream\u0026 out ) const {} protected: Eigen::Vector3d _point; }; int main ( int argc, char** argv ) { if ( argc != 5 ) { cout\u003c\u003c\"usage: pose_estimation_3d3d img1 img2 depth1 depth2\"\u003c\u003cendl; return 1; } //-- 读取图像 Mat img_1 = imread ( argv[1], CV_LOAD_IMAGE_COLOR ); Mat img_2 = imread ( argv[2], CV_LOAD_IMAGE_COLOR ); vector\u003cKeyPoint\u003e keypoints_1, keypoints_2; vector\u003cDMatch\u003e matches; find_feature_matches ( img_1, img_2, keypoints_1, keypoints_2, matches ); cout\u003c\u003c\"一共找到了\"\u003c\u003cmatches.size() \u003c\u003c\"组匹配点\"\u003c\u003cendl; // 建立3D点 Mat depth1 = imread ( argv[3], CV_LOAD_IMAGE_UNCHANGED ); // 深度图为16位无符号数，单通道图像 Mat depth2 = imread ( argv[4], CV_LOAD_IMAGE_UNCHANGED ); // 深度图为16位无符号数，单通道图像 Mat K = ( Mat_\u003cdouble\u003e ( 3,3 ) \u003c\u003c 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1 ); vector\u003cPoint3f\u003e pts1, pts2; for ( DMatch m:matches ) { ushort d1 = depth1.ptr\u003cunsigned short\u003e ( int ( keypoints_1[m.queryIdx].pt.y ) ) [ int ( keypoints_1[m.queryIdx].pt.x ) ]; ushort d2 = depth2.ptr\u003cunsigned short\u003e ( int ( keypoints_2[m.trainIdx].pt.y ) ) [ int ( keypoints_2[m.trainIdx].pt.x ) ]; if ( d1==0 || d2==0 ) // bad depth continue; Point2d p1 = pixel2cam ( keypoints_1[m.queryIdx].pt, K ); Point2d p2 = pixel2cam ( keypoints_2[m.trainIdx].pt, K ); float dd1 = float ( d1 ) /5000.0; float dd2 = float ( d2 ) /5000.0; pts1.push_back ( Point3f ( p1.x*dd1, p1.y*dd1, dd1 ) ); pts2.push_back ( Point3f ( p2.x*dd2, p2.y*dd2, dd2 ) ); } cout\u003c\u003c\"3d-3d pairs: \"\u003c\u003cpts1.size() \u003c\u003cendl; Mat R, t; pose_estimation_3d3d ( pts1, pts2, R, t ); cout\u003c\u003c\"ICP via SVD results: \"\u003c\u003cendl; cout\u003c\u003c\"R = \"\u003c\u003cR\u003c\u003cendl; cout\u003c\u003c\"t = \"\u003c\u003ct\u003c\u003cendl; cout\u003c\u003c\"R_inv = \"\u003c\u003cR.t() \u003c\u003cendl; cout\u003c\u003c\"t_inv = \"\u003c\u003c-R.t() *t\u003c\u003cendl; cout\u003c\u003c\"calling bundle adjustment\"\u003c\u003cendl; bundleAdjustment( pts1, pts2, R, t ); // verify p1 = R*p2 + t for ( int i=0; i\u003c5; i++ ) { cout\u003c\u003c\"p1 = \"\u003c\u003cpts1[i]\u003c\u003cendl; cout\u003c\u003c\"p2 = \"\u003c\u003cpts2[i]\u003c\u003cendl; cout\u003c\u003c\"(R*p2+t) = \"\u003c\u003c R * (Mat_\u003cdouble\u003e(3,1)\u003c\u003cpts2[i].x, pts2[i].y, pts2[i].z) + t \u003c\u003cendl; cout\u003c\u003cendl; } } void find_feature_matches ( const Mat\u0026 img_1, const Mat\u0026 img_2, std::vector\u003cKeyPoint\u003e\u0026 keypoints_1, std::vector\u003cKeyPoint\u003e\u0026 keypoints_2, std::vector\u003c DMatch \u003e\u0026 matches ) { //-- 初始化 Mat descriptors_1, descriptors_2; // used in OpenCV3 Ptr\u003cFeatureDetector\u003e detector = ORB::create(); Ptr\u003cDescriptorExtractor\u003e descriptor = ORB::create(); // use this if you are in OpenCV2 // Ptr\u003cFeatureDetector\u003e detector = FeatureDetector::create ( \"ORB\" ); // Ptr\u003cDescriptorExtractor\u003e descriptor = DescriptorExtractor::create ( \"ORB\" ); Ptr\u003cDescriptorMatcher\u003e matcher = DescriptorMatcher::create(\"BruteForce-Hamming\"); //-- 第一步:检测 Oriented FAST 角点位置 detector-\u003edetect ( img_1,keypoints_1 ); detector-\u003edetect ( img_2,keypoints_2 ); //-- 第二步:根据角点位置计算 BRIEF 描述子 descriptor-\u003ecompute ( img_1, keypoints_1, descriptors_1 ); descriptor-\u003ecompute ( img_2, keypoints_2, descriptors_2 ); //-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离 vector\u003cDMatch\u003e match; // BFMatcher matcher ( NORM_HAMMING ); matcher-\u003ematch ( descriptors_1, descriptors_2, match ); //-- 第四步:匹配点对筛选 double min_dist=10000, max_dist=0; //找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离 for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { double dist = match[i].distance; if ( dist \u003c min_dist ) min_dist = dist; if ( dist \u003e max_dist ) max_dist = dist; } printf ( \"-- Max dist : %f \\n\", max_dist ); printf ( \"-- Min dist : %f \\n\", min_dist ); //当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限. for ( int i = 0; i \u003c descriptors_1.rows; i++ ) { if ( match[i].distance \u003c= max ( 2*min_dist, 30.0 ) ) { matches.push_back ( match[i] ); } } } Point2d pixel2cam ( const Point2d\u0026 p, const Mat\u0026 K ) { return Point2d ( ( p.x - K.at\u003cdouble\u003e ( 0,2 ) ) / K.at\u003cdouble\u003e ( 0,0 ), ( p.y - K.at\u003cdouble\u003e ( 1,2 ) ) / K.at\u003cdouble\u003e ( 1,1 ) ); } void pose_estimation_3d3d ( const vector\u003cPoint3f\u003e\u0026 pts1, const vector\u003cPoint3f\u003e\u0026 pts2, Mat\u0026 R, Mat\u0026 t ) { Point3f p1, p2; // center of mass int N = pts1.size(); for ( int i=0; i\u003cN; i++ ) { p1 += pts1[i]; p2 += pts2[i]; } p1 = Point3f( Vec3f(p1) / N); p2 = Point3f( Vec3f(p2) / N); vector\u003cPoint3f\u003e q1 ( N ), q2 ( N ); // remove the center for ( int i=0; i\u003cN; i++ ) { q1[i] = pts1[i] - p1; q2[i] = pts2[i] - p2; } // compute q1*q2^T Eigen::Matrix3d W = Eigen::Matrix3d::Zero(); for ( int i=0; i\u003cN; i++ ) { W += Eigen::Vector3d ( q1[i].x, q1[i].y, q1[i].z ) * Eigen::Vector3d ( q2[i].x, q2[i].y, q2[i].z ).transpose(); } cout\u003c\u003c\"W=\"\u003c\u003cW\u003c\u003cendl; // SVD on W Eigen::JacobiSVD\u003cEigen::Matrix3d\u003e svd ( W, Eigen::ComputeFullU|Eigen::ComputeFullV ); Eigen::Matrix3d U = svd.matrixU(); Eigen::Matrix3d V = svd.matrixV(); cout\u003c\u003c\"U=\"\u003c\u003cU\u003c\u003cendl; cout\u003c\u003c\"V=\"\u003c\u003cV\u003c\u003cendl; Eigen::Matrix3d R_ = U* ( V.transpose() ); Eigen::Vector3d t_ = Eigen::Vector3d ( p1.x, p1.y, p1.z ) - R_ * Eigen::Vector3d ( p2.x, p2.y, p2.z ); // convert to cv::Mat R = ( Mat_\u003cdouble\u003e ( 3,3 ) \u003c\u003c R_ ( 0,0 ), R_ ( 0,1 ), R_ ( 0,2 ), R_ ( 1,0 ), R_ ( 1,1 ), R_ ( 1,2 ), R_ ( 2,0 ), R_ ( 2,1 ), R_ ( 2,2 ) ); t = ( Mat_\u003cdouble\u003e ( 3,1 ) \u003c\u003c t_ ( 0,0 ), t_ ( 1,0 ), t_ ( 2,0 ) ); } void bundleAdjustment ( const vector\u003c Point3f \u003e\u0026 pts1, const vector\u003c Point3f \u003e\u0026 pts2, Mat\u0026 R, Mat\u0026 t ) { // 初始化g2o typedef g2o::BlockSolver\u003c g2o::BlockSolverTraits\u003c6,3\u003e \u003e Block; // pose维度为 6, landmark 维度为 3 Block::LinearSolverType* linearSolver = new g2o::LinearSolverEigen\u003cBlock::PoseMatrixType\u003e(); // 线性方程求解器 Block* solver_ptr = new Block( linearSolver ); // 矩阵块求解器 g2o::OptimizationAlgorithmGaussNewton* solver = new g2o::OptimizationAlgorithmGaussNewton( solver_ptr ); g2o::SparseOptimizer optimizer; optimizer.setAlgorithm( solver ); // vertex g2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap(); // camera pose pose-\u003esetId(0); pose-\u003esetEstimate( g2o::SE3Quat( Eigen::Matrix3d::Identity(), Eigen::Vector3d( 0,0,0 ) ) ); optimizer.addVertex( pose ); // edges int index = 1; vector\u003cEdgeProjectXYZRGBDPoseOnly*\u003e edges; for ( size_t i=0; i\u003cpts1.size(); i++ ) { EdgeProjectXYZRGBDPoseOnly* edge = new EdgeProjectXYZRGBDPoseOnly( Eigen::Vector3d(pts2[i].x, pts2[i].y, pts2[i].z) ); edge-\u003esetId( index ); edge-\u003esetVertex( 0, dynamic_cast\u003cg2o::VertexSE3Expmap*\u003e (pose) ); edge-\u003esetMeasurement( Eigen::Vector3d( pts1[i].x, pts1[i].y, pts1[i].z) ); edge-\u003esetInformation( Eigen::Matrix3d::Identity()*1e4 ); optimizer.addEdge(edge); index++; edges.push_back(edge); } chrono::steady_clock::time_point t1 = chrono::steady_clock::now(); optimizer.setVerbose( true ); optimizer.initializeOptimization(); optimizer.optimize(10); chrono::steady_clock::time_point t2 = chrono::steady_clock::now(); chrono::duration\u003cdouble\u003e time_used = chrono::duration_cast\u003cchrono::duration\u003cdouble\u003e\u003e(t2-t1); cout\u003c\u003c\"optimization costs time: \"\u003c\u003ctime_used.count()\u003c\u003c\" seconds.\"\u003c\u003cendl; cout\u003c\u003cendl\u003c\u003c\"after optimization:\"\u003c\u003cendl; cout\u003c\u003c\"T=\"\u003c\u003cendl\u003c\u003cEigen::Isometry3d( pose-\u003eestimate() ).matrix()\u003c\u003cendl; } ","title":"视觉里程计1","uri":"/contents/slam-sfm/visual-odometer1/"},{"categories":["contents"],"content":"主要记录两个非线性优化的方法：Gauss-Newton, Levenburg-Marquadt下降策略，以及学习 Ceres 库和 g2o 库的基本使用方法。摘自《视觉SLAM十四讲》\nSLAM 模型的运动方程和观测方程 经典 SLAM 模型由一个状态方程和一个运动方程构成：\n$$ \\boldsymbol{x}_{k}=f\\left(\\boldsymbol{x}_{k-1}, \\boldsymbol{u}_{k}\\right)+\\boldsymbol{w}_{k} $$\n$$ \\boldsymbol{z}_{k, j}=h\\left(\\boldsymbol{y}_{j}, \\boldsymbol{x}_{k}\\right)+\\boldsymbol{v}_{k, j} $$\n状态方程中，这里的 $\\boldsymbol{x}_{k}$乃是相机的位姿。k时刻的相机位姿由k-1时刻相机位姿，运动传感器的数据和噪声决定。我们可以使用变换矩阵或李代数表示它。观测方程中， ${\\boldsymbol{z}_{k, j}}$ 对应$\\boldsymbol{x}_{k}$对路标$\\boldsymbol{y}_{j}$的一次观测，对应到图像上的像素位置。对于针孔摄像机，观测方程可以表示成:\n$$ s \\boldsymbol{z}_{k, j}=\\boldsymbol{K} \\exp \\left(\\boldsymbol{\\xi}^{\\wedge}\\right) \\boldsymbol{y}_{j} $$\n这里 $\\boldsymbol{K}$为相机内参,s 为像素点的距离。同时这里$\\boldsymbol{z}_{k, j}$和 $\\boldsymbol{y}_{j}$都必须以齐次坐标来描述,且中间有一次齐次到非齐次的转换。\n由于受到噪声的影响，我们希望通过带噪声的数据 $\\boldsymbol{z}$ 和 $\\boldsymbol{u}$,推断位姿 $\\boldsymbol{x}$和地图 $\\boldsymbol{y}$。\n非线性优化 在非线性优化中,我们把所有待估计的变量放在一个“状态变量”中:\n$$ \\boldsymbol{x}=({\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{N}, \\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{M}}) $$\n对机器人状态的估计,就是求已知输入数据 $\\boldsymbol{u}$ 和观测数据$\\boldsymbol{ z }$的条件下,计算状态$\\boldsymbol{ x}$ 的条件概率分布: $$ P(\\boldsymbol{x} | \\boldsymbol{z}, \\boldsymbol{u}) $$\n类似于$\\boldsymbol{x}$,这里 $\\boldsymbol{u}$ 和$\\boldsymbol{z}$ 也是对所有数据的统称。特别地,当我们没有测量运动的传感器,只有一张张的图像时,即只考虑观测方程带来的数据时,相当于估计 $P(\\boldsymbol{x} | \\boldsymbol{z})$ 的条件概率分布。如果忽略图像在时间上的联系,把它们看作一堆彼此没有关系的图片,该问题也称为 Structure from Motion(SfM),即如何从许多图像中重建三维空间结构 。在这种情况下,SLAM 可以看作是图像具有时间先后顺序的,需要实时求解一个 SfM 问题。\n直接求后验分布是困难的,但是求一个状态最优估计,使得在该状态下,后验概率最大化(Maximize a Posterior,MAP),则是可行的:\n$$ x^{*}_{M A P}=\\arg \\max P(x | z)=\\arg \\max P(z | x) P(x) $$\n当不知道机器人位姿大概在什么地方,此时就没有了先验。那么,可以求解x 的最大似然估计(Maximize Likelihood Estimation, MLE):\n$$ \\boldsymbol{x}^{*}_{M L E}=\\arg \\max P(\\boldsymbol{z} | \\boldsymbol{x}) $$\n在这里，最大似然估计,可以理解成:“在什么样的状态下,最可能产生现在观测到的数据”。 在求解最大似然估计的时候，最终解的形式可以表示为一个最小二乘问题，度量函数选择误差的平方之和：\n$$ J(\\boldsymbol{x})=\\sum_{k} e_{v, k}^{T} \\boldsymbol{R}_{k}^{-1} \\boldsymbol{e}_{v, k}+\\sum_{k} \\sum_{j} \\boldsymbol{e}_{y, k, j}^{T} \\boldsymbol{Q}_{k, j}^{-1} \\boldsymbol{e}_{y, k, j} $$\n上式的最优解等价于状态的最大似然估计。但由于噪声的存在，当我们把估计的轨迹与地图代入 SLAM 的运动、观测方程中时,它们并不会完美的成立。因此我们把状态的估计值进行微调,使得整体的误差下降一些。当然这个下降也有限度,它一般会到达一个极小值。这就是一个典型非线性优化的过程。下面记录两个非线性优化的方法：Gauss-Newton, Levenburg-Marquadt下降策略。\n设$\\boldsymbol{f}$ 是任意一个非线性函数,我们设它有 m 维:$f(x) \\in \\mathbb{R}^{m}$。一个简单的最小二乘问题是:\n$$ \\min _{x} \\frac{1}{2}|f(\\boldsymbol{x})|_{2}^{2} $$\nGauss-Newton Gauss Newton 是最优化算法里面最简单的方法之一。它的思想是将 $f(\\boldsymbol{x})$进行一阶的泰勒展开：\n$$ f(\\boldsymbol{x}+\\Delta \\boldsymbol{x}) \\approx f(\\boldsymbol{x})+\\boldsymbol{J}(\\boldsymbol{x}) \\Delta \\boldsymbol{x} $$\n这里$J(\\boldsymbol{x})$为$f(\\boldsymbol{x})$关于 x 的导数,实际上是一个 m × n 的矩阵,也是一个雅可比矩阵。根据前面的框架,当前的目标是为了寻找下降矢量 $\\Delta \\boldsymbol{x}$,使得$|f(\\boldsymbol{x}+\\Delta \\boldsymbol{x})|^{2}$达到最小。为了求 $\\Delta \\boldsymbol{x}$,,我们需要解一个线性的最小二乘问题:\n$$ \\Delta \\boldsymbol{x}^{*}=\\arg \\min _{\\Delta \\boldsymbol{x}} \\frac{1}{2}|f(\\boldsymbol{x})+\\boldsymbol{J}(\\boldsymbol{x}) \\Delta \\boldsymbol{x}|^{2} $$\n求上式关于 $\\Delta \\boldsymbol{x}$的导数,并令其为零,可以得到如下方程组:\n$$ J(\\boldsymbol{x})^{T} \\boldsymbol{J}(\\boldsymbol{x}) \\Delta \\boldsymbol{x}=-\\boldsymbol{J}(\\boldsymbol{x})^{T} f(\\boldsymbol{x}) $$\nGauss-Newton 的算法步骤可以写成:\n给定初始值 $\\boldsymbol{x}_{0}$ 。\n对于第 k 次迭代,求出当前的雅可比矩阵$J\\left(x_{k}\\right)$和误差$f\\left(\\boldsymbol{x}_{k}\\right)$。\n求解增量方程:$\\boldsymbol{H} \\Delta \\boldsymbol{x}_{k}=\\boldsymbol{g}$\n若$\\Delta \\boldsymbol{x}_{k}$ 足够小,则停止。否则,令$\\boldsymbol{x}_{k+1}=\\boldsymbol{x}_{k}+\\Delta \\boldsymbol{x}_{k}$,返回 2.\nLevenberg-Marquadt 由于 Gauss-Newton 方法中采用的近似二阶泰勒展开只能在展开点附近有较好的近似 效果,所以我们很自然地想到应该给 ∆x 添加一个信赖区域(Trust Region),不能让它太大而使得近似不准确。非线性优化种有一系列这类方法,这类方法也被称之为信赖区域方法 (Trust Region Method)。在信赖区域里边,我们认为近似是有效的;出了这个区域,近似可能会出问题。\n一个比较好的方法是根据我们的近似模型跟实际函数之间的差异来确定这个范围:如果差异小,我们就让范围尽可能大;如果差异大,我们就缩小这个近似范围。\n$$ \\rho=\\frac{f(\\boldsymbol{x}+\\Delta \\boldsymbol{x})-f(\\boldsymbol{x})}{\\boldsymbol{J}(\\boldsymbol{x}) \\Delta \\boldsymbol{x}} $$\n$\\rho$ 的分子是实际函数下降的值,分母是近似模型下降的值。如果 $\\rho$ 接近于 1,则近似是好的。如果 $\\rho$太小,说明实际减小的值远少于近似减小的值,则认为近似比较差,需要缩小近似范围。反之,如果 $\\rho$比较大,则说明实际下降的比预计的更大,我们可以放大近似范围。\nLevenberg-Marquadt的算法步骤可以写成:\n给定初始值 $\\boldsymbol{x}_{0}$ ,以及初始优化半径 $\\mu$。\n对于第 k 次迭代,求解:\n$$ \\min _{\\Delta \\boldsymbol{x}_{k}} \\frac{1}{2}\\left|f\\left(\\boldsymbol{x}_{k}\\right)+\\boldsymbol{J}\\left(\\boldsymbol{x}_{k}\\right) \\Delta \\boldsymbol{x}_{k}\\right|^{2}, \\quad \\text { s.t. }\\left|\\boldsymbol{D} \\Delta \\boldsymbol{x}_{k}\\right|^{2} \\leq \\mu $$\n这里 $\\mu$ 是信赖区域的半径,$D$将在后文说明。\n计算 $\\rho$。\n若 $\\rho$ \u003e 34 ,则 $\\mu$ = 2$\\mu$;\n若 $\\rho$ \u003c 14 ,则 $\\mu$ = 0.5$\\mu$;\n如果 $\\rho$ 大于某阈值,认为近似可行。令$\\boldsymbol{x}_{k+1}=\\boldsymbol{x}_{k}+\\Delta \\boldsymbol{x}_{k}$ 。\n判断算法是否收敛。如不收敛则返回 2,否则结束。\n上式用 拉格朗日乘子将它转化为一个无约束优化问题:\n$$ \\min _{\\Delta \\boldsymbol{x}_{k}} \\frac{1}{2}\\left|f\\left(\\boldsymbol{x}_{k}\\right)+\\boldsymbol{J}\\left(\\boldsymbol{x}_{k}\\right) \\Delta \\boldsymbol{x}_{k}\\right|^{2}+\\frac{\\lambda}{2}|\\boldsymbol{D} \\Delta \\boldsymbol{x}|^{2} $$\n类似于 Gauss-Newton 中的做法,把它展开后,我们发现 该问题的核心仍是计算增量的线性方程: $$ \\left(\\boldsymbol{H}+\\lambda \\boldsymbol{D}^{T} \\boldsymbol{D}\\right) \\Delta \\boldsymbol{x}=\\boldsymbol{g} $$ 可以看到,增量方程相比于 Gauss-Newton,多了一项$\\lambda \\boldsymbol{D}^{T} \\boldsymbol{D}$。如果考虑它的简化形式,即$\\boldsymbol{D}=\\boldsymbol{I}$,那么相当于求解: $$ (\\boldsymbol{H}+\\lambda \\boldsymbol{I}) \\Delta \\boldsymbol{x}=\\boldsymbol{g} $$ 我们看到,当参数 $\\lambda$比较小时,$\\boldsymbol{H}$占主要地位,这说明二次近似模型在该范围内是比较好的,L-M 方法更接近于 G-N 法。另一方面,当 λ 比较大时,λI 占据主要地位,L-M更接近于一阶梯度下降法(即最速下降),这说明附近的二次近似不够好。L-M 的求解方式,可在一定程度上避免线性方程组的系数矩阵的非奇异和病态问题,提供更稳定更准确的增量 $\\Delta \\boldsymbol{x}$。\n无论是 G-N 还是 L-M,在做最优化计算的时候,都需要提供变量的初始值。实际上非线性优化的所有迭代求解方案,都需要用户来提供一个良好的初始值。由于目标函数太复杂,导致在求解空间上的变化难以琢磨,对问题提供不同的初始值往往会导致不同的计算结果。这种情况是非线性优化的通病:大多数算法都容易陷入局部极小值。因此,无论是哪类科学问题,我们提供初始值都应该有科学依据,例如视觉 SLAM 问题中,我们会用 ICP,PnP 之类的算法提供优化初始值。总之,一个良好的初始值对最优化问题非常重要!\n代码实践 使用 Ceres 拟合曲线 编译Ceres有两个注意的点：当初编译sophus要求eigen版本是3.3，编译Ceres时要求是3.2。 gcc版本我之前的是4.9，要升级为gcc-5 用Ceres拟合曲线： $$ y=\\exp \\left(a x^{2}+b x+c\\right)+w $$ 其中 a, b, c 为曲线的参数,w 为高斯噪声。我们故意选择了这样一个非线性模型,以使问题不至于太简单。现在,假设我们有 N 个关于 x, y 的观测数据点,想根据这些数据点求出曲线的参数。那么,可以求解下面的最小二乘问题以估计曲线参数: $$ \\min {a, b, c} \\frac{1}{2} \\sum{i=1}^{N}\\left|y_{i}-\\exp \\left(a x_{i}^{2}+b x_{i}+c\\right)\\right|^{2} $$\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 #include \u003ciostream\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003cceres/ceres.h\u003e #include \u003cchrono\u003e using namespace std; // 代价函数的计算模型 struct CURVE_FITTING_COST { CURVE_FITTING_COST ( double x, double y ) : _x ( x ), _y ( y ) {} // 残差的计算 template \u003ctypename T\u003e bool operator() ( const T* const abc, // 模型参数，有3维 T* residual ) const // 残差 { residual[0] = T ( _y ) - ceres::exp ( abc[0]*T ( _x ) *T ( _x ) + abc[1]*T ( _x ) + abc[2] ); // y-exp(ax^2+bx+c) return true; } const double _x, _y; // x,y数据 }; int main ( int argc, char** argv ) { double a=1.0, b=2.0, c=1.0; // 真实参数值 int N=100; // 数据点 double w_sigma=1.0; // 噪声Sigma值 cv::RNG rng; // OpenCV随机数产生器 double abc[3] = {0,0,0}; // abc参数的估计值 vector\u003cdouble\u003e x_data, y_data; // 数据 cout\u003c\u003c\"generating data: \"\u003c\u003cendl; for ( int i=0; i\u003cN; i++ ) { double x = i/100.0; x_data.push_back ( x ); y_data.push_back ( exp ( a*x*x + b*x + c ) + rng.gaussian ( w_sigma ) ); cout\u003c\u003cx_data[i]\u003c\u003c\" \"\u003c\u003cy_data[i]\u003c\u003cendl; } // 构建最小二乘问题 ceres::Problem problem; for ( int i=0; i\u003cN; i++ ) { problem.AddResidualBlock ( // 向问题中添加误差项 // 使用自动求导，模板参数：误差类型，输出维度，输入维度，维数要与前面struct中一致 new ceres::AutoDiffCostFunction\u003cCURVE_FITTING_COST, 1, 3\u003e ( new CURVE_FITTING_COST ( x_data[i], y_data[i] ) ), nullptr, // 核函数，这里不使用，为空 abc // 待估计参数 ); } // 配置求解器 ceres::Solver::Options options; // 这里有很多配置项可以填 options.linear_solver_type = ceres::DENSE_QR; // 增量方程如何求解 options.minimizer_progress_to_stdout = true; // 输出到cout ceres::Solver::Summary summary; // 优化信息 chrono::steady_clock::time_point t1 = chrono::steady_clock::now(); ceres::Solve ( options, \u0026problem, \u0026summary ); // 开始优化 chrono::steady_clock::time_point t2 = chrono::steady_clock::now(); chrono::duration\u003cdouble\u003e time_used = chrono::duration_cast\u003cchrono::duration\u003cdouble\u003e\u003e( t2-t1 ); cout\u003c\u003c\"solve time cost = \"\u003c\u003ctime_used.count()\u003c\u003c\" seconds. \"\u003c\u003cendl; // 输出结果 cout\u003c\u003csummary.BriefReport() \u003c\u003cendl; cout\u003c\u003c\"estimated a,b,c = \"; for ( auto a:abc ) cout\u003c\u003ca\u003c\u003c\" \"; cout\u003c\u003cendl; return 0; } CMakeLists.txt:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cmake_minimum_required( VERSION 2.8 ) project( ceres_curve_fitting ) set( CMAKE_BUILD_TYPE \"Release\" ) set( CMAKE_CXX_FLAGS \"-std=c++11 -O3\" ) # 添加cmake模块以使用ceres库 list( APPEND CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake_modules ) # 寻找Ceres库并添加它的头文件 find_package( Ceres REQUIRED ) include_directories( ${CERES_INCLUDE_DIRS} ) # OpenCV find_package( OpenCV REQUIRED ) include_directories( ${OpenCV_DIRS} ) add_executable( curve_fitting main.cpp ) # 与Ceres和OpenCV链接 target_link_libraries( curve_fitting ${CERES_LIBRARIES} ${OpenCV_LIBS} ) 基于图优化的库:g2o 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 #include \u003ciostream\u003e #include \u003cg2o/core/base_vertex.h\u003e #include \u003cg2o/core/base_unary_edge.h\u003e #include \u003cg2o/core/block_solver.h\u003e #include \u003cg2o/core/optimization_algorithm_levenberg.h\u003e #include \u003cg2o/core/optimization_algorithm_gauss_newton.h\u003e #include \u003cg2o/core/optimization_algorithm_dogleg.h\u003e #include \u003cg2o/solvers/dense/linear_solver_dense.h\u003e #include \u003cEigen/Core\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003ccmath\u003e #include \u003cchrono\u003e using namespace std; // 曲线模型的顶点，模板参数：优化变量维度和数据类型 class CurveFittingVertex: public g2o::BaseVertex\u003c3, Eigen::Vector3d\u003e { public: EIGEN_MAKE_ALIGNED_OPERATOR_NEW virtual void setToOriginImpl() // 重置 { _estimate \u003c\u003c 0,0,0; } virtual void oplusImpl( const double* update ) // 更新 { _estimate += Eigen::Vector3d(update); } // 存盘和读盘：留空 virtual bool read( istream\u0026 in ) {} virtual bool write( ostream\u0026 out ) const {} }; // 误差模型 模板参数：观测值维度，类型，连接顶点类型 class CurveFittingEdge: public g2o::BaseUnaryEdge\u003c1,double,CurveFittingVertex\u003e { public: EIGEN_MAKE_ALIGNED_OPERATOR_NEW CurveFittingEdge( double x ): BaseUnaryEdge(), _x(x) {} // 计算曲线模型误差 void computeError() { const CurveFittingVertex* v = static_cast\u003cconst CurveFittingVertex*\u003e (_vertices[0]); const Eigen::Vector3d abc = v-\u003eestimate(); _error(0,0) = _measurement - std::exp( abc(0,0)*_x*_x + abc(1,0)*_x + abc(2,0) ) ; } virtual bool read( istream\u0026 in ) {} virtual bool write( ostream\u0026 out ) const {} public: double _x; // x 值， y 值为 _measurement }; int main( int argc, char** argv ) { double a=1.0, b=2.0, c=1.0; // 真实参数值 int N=100; // 数据点 double w_sigma=1.0; // 噪声Sigma值 cv::RNG rng; // OpenCV随机数产生器 double abc[3] = {0,0,0}; // abc参数的估计值 vector\u003cdouble\u003e x_data, y_data; // 数据 cout\u003c\u003c\"generating data: \"\u003c\u003cendl; for ( int i=0; i\u003cN; i++ ) { double x = i/100.0; x_data.push_back ( x ); y_data.push_back ( exp ( a*x*x + b*x + c ) + rng.gaussian ( w_sigma ) ); cout\u003c\u003cx_data[i]\u003c\u003c\" \"\u003c\u003cy_data[i]\u003c\u003cendl; } // 构建图优化，先设定g2o typedef g2o::BlockSolver\u003c g2o::BlockSolverTraits\u003c3,1\u003e \u003e Block; // 每个误差项优化变量维度为3，误差值维度为1 Block::LinearSolverType* linearSolver = new g2o::LinearSolverDense\u003cBlock::PoseMatrixType\u003e(); // 线性方程求解器 Block* solver_ptr = new Block( linearSolver ); // 矩阵块求解器 // 梯度下降方法，从GN, LM, DogLeg 中选 g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg( solver_ptr ); // g2o::OptimizationAlgorithmGaussNewton* solver = new g2o::OptimizationAlgorithmGaussNewton( solver_ptr ); // g2o::OptimizationAlgorithmDogleg* solver = new g2o::OptimizationAlgorithmDogleg( solver_ptr ); g2o::SparseOptimizer optimizer; // 图模型 optimizer.setAlgorithm( solver ); // 设置求解器 optimizer.setVerbose( true ); // 打开调试输出 // 往图中增加顶点 CurveFittingVertex* v = new CurveFittingVertex(); v-\u003esetEstimate( Eigen::Vector3d(0,0,0) ); v-\u003esetId(0); optimizer.addVertex( v ); // 往图中增加边 for ( int i=0; i\u003cN; i++ ) { CurveFittingEdge* edge = new CurveFittingEdge( x_data[i] ); edge-\u003esetId(i); edge-\u003esetVertex( 0, v ); // 设置连接的顶点 edge-\u003esetMeasurement( y_data[i] ); // 观测数值 edge-\u003esetInformation( Eigen::Matrix\u003cdouble,1,1\u003e::Identity()*1/(w_sigma*w_sigma) ); // 信息矩阵：协方差矩阵之逆 optimizer.addEdge( edge ); } // 执行优化 cout\u003c\u003c\"start optimization\"\u003c\u003cendl; chrono::steady_clock::time_point t1 = chrono::steady_clock::now(); optimizer.initializeOptimization(); optimizer.optimize(100); chrono::steady_clock::time_point t2 = chrono::steady_clock::now(); chrono::duration\u003cdouble\u003e time_used = chrono::duration_cast\u003cchrono::duration\u003cdouble\u003e\u003e( t2-t1 ); cout\u003c\u003c\"solve time cost = \"\u003c\u003ctime_used.count()\u003c\u003c\" seconds. \"\u003c\u003cendl; // 输出优化值 Eigen::Vector3d abc_estimate = v-\u003eestimate(); cout\u003c\u003c\"estimated model: \"\u003c\u003cabc_estimate.transpose()\u003c\u003cendl; return 0; } CMakeLists.txt\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 cmake_minimum_required( VERSION 2.8 ) project( g2o_curve_fitting ) set( CMAKE_BUILD_TYPE \"Release\" ) set( CMAKE_CXX_FLAGS \"-std=c++11 -O3\" ) # 添加cmake模块以使用ceres库 list( APPEND CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake_modules ) # 寻找G2O find_package( G2O REQUIRED ) include_directories( ${G2O_INCLUDE_DIRS} \"/usr/include/eigen3\" ) # OpenCV find_package( OpenCV REQUIRED ) include_directories( ${OpenCV_DIRS} ) add_executable( curve_fitting main.cpp ) # 与G2O和OpenCV链接 target_link_libraries( curve_fitting ${OpenCV_LIBS} g2o_core g2o_stuff ) ","title":"非线性优化","uri":"/contents/slam-sfm/nonlinear-optimization/"},{"categories":["contents"],"content":"在SLAM中常需要估计一个相机的位置和姿态，这是个优化问题，需要对相机位姿求导，而李代数可以方便地表示相机位姿的导数。本笔记记录李群$SO(3),SE(3)$和李代数$so(3),se(3)$的对应关系以及李代数的求导表示。最后是李代数的编程练习。\n知识点总结 Sophus编程练习 Sophus 可以下载github源码然后cmake编译：\n1 2 3 4 5 6 7 8 git clone git://github.com/strasdat/Sophus.git cd Sophus git checkout a621ff mkdir svs_build cd svs_build cmake .. -DCMAKE_INSTALL_PREFIX:PATH=$HOME/svslocal make -j4 make install cpp代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 #include \u003ciostream\u003e #include \u003ccmath\u003e #include \u003cEigen/Core\u003e #include \u003cEigen/Geometry\u003e #include \"sophus/so3.h\" #include \"sophus/se3.h\" using namespace std; int main(int argc , char **argv) { Eigen::Matrix3d R = Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d(0,0,1)).toRotationMatrix();//z轴旋转pi/2 Sophus::SO3 SO3_R(R); Sophus::SO3 SO3_v(0,0,M_PI/2); Eigen::Quaterniond q(R); Sophus::SO3 SO3_q(q); cout\u003c\u003c\"SO(3) from matrix:\"\u003c\u003cSO3_R\u003c\u003cendl; cout\u003c\u003c\"SO(3) from vector:\"\u003c\u003cSO3_v\u003c\u003cendl; cout\u003c\u003c\"SO(3) from quaternion:\"\u003c\u003cSO3_q\u003c\u003cendl; //使用对数映射获取它的李代数 Eigen::Vector3d so3 = SO3_R.log(); cout\u003c\u003c\"李代数so3:\"\u003c\u003cso3.transpose()\u003c\u003cendl; //hat为向量到反对称矩阵 cout\u003c\u003c\"so3 hat:\"\u003c\u003cSophus::SO3::hat(so3)\u003c\u003cendl; //vee 为反对称到向量 cout\u003c\u003c\"so3 hat vee:\"\u003c\u003cSophus::SO3::vee(Sophus::SO3::hat(so3)).transpose()\u003c\u003cendl; //增加绕动模型 Eigen::Vector3d update_so3(1e-4,0,0); Sophus::SO3 SO3_updated = Sophus::SO3::exp(update_so3)*SO3_R;//左乘更新 cout\u003c\u003c\"SO3 updated:\"\u003c\u003cSO3_updated\u003c\u003cendl; //对SE(3)的操作 Eigen::Vector3d t(1,0,0); Sophus::SE3 SE3_Rt(R,t); Sophus::SE3 SE3_qt(q,t); cout\u003c\u003c\"SE3 from R,t:\"\u003c\u003cendl\u003c\u003cSE3_Rt\u003c\u003cendl; cout\u003c\u003c\"SE3 from q,t:\"\u003c\u003cendl\u003c\u003cSE3_qt\u003c\u003cendl; typedef Eigen::Matrix\u003cdouble,6,1\u003e Vector6d; Vector6d se3 = SE3_Rt.log(); cout\u003c\u003c\"李代数se3:\"\u003c\u003cse3.transpose()\u003c\u003cendl; cout\u003c\u003c\"se3 hat:\"\u003c\u003cSophus::SE3::hat(se3)\u003c\u003cendl; cout\u003c\u003c\"se3 hat vee:\"\u003c\u003cSophus::SE3::vee(Sophus::SE3::hat(se3)).transpose()\u003c\u003cendl; Vector6d update_se3 ; update_se3.setZero(); update_se3(0,0) = 1e-4d; cout\u003c\u003c\"update_se3 is:\"\u003c\u003cupdate_se3.transpose()\u003c\u003cendl; Sophus::SE3 SE3_updated = Sophus::SE3::exp(update_se3)* SE3_Rt; cout\u003c\u003c\"SE3 updated:\"\u003c\u003cendl\u003c\u003cSE3_updated.matrix()\u003c\u003cendl; return 0; } CMakeLists.txt\n1 2 3 4 5 6 7 8 9 10 11 cmake_minimum_required(VERSION 2.6) project(usesophus) include_directories( \"/usr/include/eigen3\" ) find_package( Sophus REQUIRED ) include_directories( ${Sophus_INCLUDE_DIRS} ) add_executable(usesophus main.cpp) target_link_libraries( usesophus ${Sophus_LIBRARIES} ) install(TARGETS usesophus RUNTIME DESTINATION bin) ","title":"李群与李代数","uri":"/contents/slam-sfm/slam3/"},{"categories":["contents"],"content":"笔记记录：\n笔记内容有向量内积，向量外积，欧氏变换，旋转向量，欧拉角，旋转矩阵，四元数以及它们的转换关系，代码是Eigen库的基本使用。\n笔记 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 #include \u003ciostream\u003e #include \u003cctime\u003e #include \u003ccmath\u003e #include \u003cEigen/Core\u003e #include \u003cEigen/Dense\u003e #include \u003cEigen/Geometry\u003e using namespace std; #define MATRIX_SIZE 50 int main(int argc, char** argv) { ///Eigen 基本操作 Eigen::Matrix\u003cfloat,2,3\u003e matrix_23 ; Eigen::Vector3d v_3d; //实质是Eigen::Matrix\u003cdouble,3,1\u003e matrix_23 \u003c\u003c 1,2,3,4,5,6; //赋值操作 v_3d \u003c\u003c 3,2,1; Eigen::Matrix\u003cdouble,2,1\u003e result = matrix_23.cast\u003cdouble\u003e() * v_3d; cout\u003c\u003c result \u003c\u003cendl; Eigen::Matrix3d matrix_33 = Eigen::Matrix3d::Random(); cout\u003c\u003c \"matrix_33 is :\" \u003c\u003c matrix_33 \u003c\u003cendl; cout\u003c\u003c \"matrix_33 transpose is :\" \u003c\u003c matrix_33.transpose() \u003c\u003cendl; //转置 cout\u003c\u003c \"matrix_33 sum is :\" \u003c\u003c matrix_33.sum() \u003c\u003cendl; //各元素的和 cout\u003c\u003c \"matrix_33 trace is :\"\u003c\u003c matrix_33.trace() \u003c\u003cendl; //矩阵的迹 cout\u003c\u003c \"matrix_33 inverse is :\"\u003c\u003c matrix_33.inverse() \u003c\u003cendl; //逆 cout\u003c\u003c \"matrix_33 determinant is ::\"\u003c\u003c matrix_33.determinant() \u003c\u003cendl; //行列式 //特征值和特征向量 Eigen::SelfAdjointEigenSolver\u003cEigen::Matrix3d\u003e eigen_solver (matrix_33.transpose() * matrix_33);//实对称矩阵 cout\u003c\u003c \"Eigen values = \"\u003c\u003c eigen_solver.eigenvalues() \u003c\u003c endl; //特征值 cout\u003c\u003c \"Eigen vectors = \"\u003c\u003c eigen_solver.eigenvectors() \u003c\u003c endl; //特征向量 //解方程，对比求逆和矩阵分解的速度 Eigen::Matrix\u003cdouble,MATRIX_SIZE,MATRIX_SIZE\u003e matrix_NN; matrix_NN = Eigen::MatrixXd::Random(MATRIX_SIZE,MATRIX_SIZE); Eigen::Matrix \u003c double ,MATRIX_SIZE,1\u003e v_Nd; v_Nd = Eigen::MatrixXd::Random(MATRIX_SIZE,1); //求逆 clock_t time_stt = clock(); Eigen::Matrix\u003cdouble,MATRIX_SIZE,1\u003e x = matrix_NN.inverse() * v_Nd; cout\u003c\u003c \" inverse time use is :\"\u003c\u003c 1000*(clock()-time_stt)/(double)CLOCKS_PER_SEC \u003c\u003c \"ms\" \u003c\u003cendl; //QR分解 time_stt = clock(); x = matrix_NN.colPivHouseholderQr().solve(v_Nd); cout\u003c\u003c \"Qr time use is :\"\u003c\u003c 1000*(clock()-time_stt)/(double)CLOCKS_PER_SEC \u003c\u003c \"ms\" \u003c\u003cendl; ///Eigen 中四元数，欧拉角，旋转矩阵，旋转向量的转换 Eigen::Matrix3d rotation_matrix = Eigen::Matrix3d::Identity(); Eigen::AngleAxisd rotation_vector(M_PI/4,Eigen::Vector3d(0,0,1)); cout.precision(3); rotation_matrix = rotation_vector.toRotationMatrix(); //旋转向量可以转换为旋转矩阵 cout\u003c\u003c\"rotation_matrix is :\"\u003c\u003crotation_matrix\u003c\u003cendl; ///AngleAxis 旋转向量进行坐标变换 Eigen::Vector3d v(1,0,0); Eigen::Vector3d v_rotated = rotation_vector * v; cout\u003c\u003c\"(1,0,0) after rotated:\"\u003c\u003cv_rotated.transpose()\u003c\u003cendl; /// 旋转矩阵进行坐标变换 v_rotated = rotation_matrix * v; cout\u003c\u003c\"(1,0,0) after rotation = \"\u003c\u003cv_rotated.transpose(); /// 欧拉角： 可以直接将旋转矩阵转换为欧拉角 Eigen::Vector3d euler_angles = rotation_matrix.eulerAngles(2,1,0); //(2,1,0)表示ZYX 的旋转顺序 cout\u003c\u003c\"yaw pitch roll = \"\u003c\u003ceuler_angles.transpose()\u003c\u003cendl; ///四元数 可以将旋转向量转换为四元数 Eigen::Quaterniond q = Eigen::Quaterniond (rotation_vector); cout\u003c\u003c\"Quaterniond = \\n\"\u003c\u003cq.coeffs()\u003c\u003cendl;//coeffs 表示顺序是(x,y,z,w),w为实部，前三者为虚部。 /// 也可以将旋转矩阵赋予给四元数 q = Eigen::Quaterniond(rotation_matrix); cout\u003c\u003c\"Quaterniond = \\n\"\u003c\u003cq.coeffs()\u003c\u003cendl; ///四元数旋转一个矩阵 v_rotated = q*v; //注意数学形式为 qvq^{-1} cout\u003c\u003c\"(1,0,0) after rotated :\"\u003c\u003cv_rotated.transpose()\u003c\u003cendl; ///使用欧氏变换 Eigen::Isometry3d T_o = Eigen::Isometry3d::Identity(); T_o.rotate (rotation_vector); T_o.pretranslate(Eigen::Vector3d(1,3,4)); cout\u003c\u003c \"Transform matrix = \\n\"\u003c\u003c T_o.matrix()\u003c\u003cendl; Eigen::Vector3d v_transformed = T_o*v; cout\u003c\u003c\"v transformed :\"\u003c\u003cv_transformed.transpose()\u003c\u003cendl; ///仿射变换 Eigen::Affine3d T_a = Eigen::Affine3d::Identity(); T_a.rotate (rotation_vector); T_a.prescale(0.5); T_a.pretranslate(Eigen::Vector3d(1,3,4)); cout\u003c\u003c \"Transform matrix = \\n\"\u003c\u003c T_a.matrix()\u003c\u003cendl; Eigen::Vector3d v_transformed1 = T_a*v; cout\u003c\u003c\"v transformed :\"\u003c\u003cv_transformed1.transpose()\u003c\u003cendl; ///射影变换 /* Eigen::Projective3d T_p; T_p.rotate (rotation_vector); T_p.prescale(0.5); T_p.pretranslate(Eigen::Vector3d(1,3,4)); cout\u003c\u003c \"Transform matrix = \\n\"\u003c\u003c T_p.matrix()\u003c\u003cendl; Eigen::Vector3d v_transformed2 = T_p*v; cout\u003c\u003c\"v transformed :\"\u003c\u003cv_transformed2.transpose()\u003c\u003cendl; */ return 0; } ","title":"三维空间刚体运动","uri":"/contents/slam-sfm/slam2/"},{"categories":["contents"],"content":"笔记记录：\nSLAM基本框架\n如何用cmake编译cpp源文件\nSLAM基本框架 SLAM要解决两个问题：定位和建图。一个基本的SLAM框架包括：传感器信息读取，视觉里程计，后端优化，回环检测和建图。视觉里程计称为整个框架的前端，它的任务是估算相邻图像中相机的运动轨迹，构建局部地图。由于估算会有误差积累（累计漂移）导致估算的轨迹不再准确，这时候需要回环检测负责把“机器人回到原始位置”的事情检测出来，视觉里程计和回环检测会把数据传递给后端优化，校正整个轨迹的形状，最后构建全局地图。整个框架的框图如下： 以下是阅读 《视觉SLAM十四讲》 做的笔记：第一张是相机分类，后两张是SLAM基本框架各部分的大致内容和联系：\n用cmake编译cpp源文件 例子会用到opencv和cmake.两者在Linux下安装步骤看这里。 为什么要用cmake? 《CMake Practice》 里这样说：\n1.开放源代码，使用类 BSD 许可发布。\n2.跨平台，并可生成 native 编译配置文件，在 Linux/Unix 平台，生成 makefile，在苹果平台，可以生成 xcode，在 Windows 平台，可以生成 MSVC 的工程文件。\n3.能够管理大型项目，KDE4 就是最好的证明。\n4.简化编译构建过程和编译过程。Cmake 的工具链非常简单：cmake+make。\n5.高效虑，按照 KDE 官方说法，CMake 构建 KDE4 的 kdelibs 要比使用 autotools 来构建 KDE3.5.6 的 kdelibs 快 40% ，主要是因为 Cmake 在工具链中没有 libtool。\n6.可扩展，可以为 cmake 编写特定功能的模块，扩充 cmake 功能。\n下面用cmake编译 mian.cpp 构建工程，需要创建一个文件夹，并把两个文件拷贝进去：\nmain.cpp\nCMakeLists.txt\nmain.cpp在本例子要实现读取指定文件夹图片，将每张图片重命名为数字序列（如0001.png,0002.png…）,并将图片存储在新的文件夹下，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include \u003copencv2/opencv.hpp\u003e #include \u003ciostream\u003e #include \u003cstdio.h\u003e using namespace std; using namespace cv; int main() { String Pattern = \"/home/gaki/image/\"; String OutputDir = Pattern+\"output/\";//注意在该目录创建一个*output*文件夹，重命名的图片将储存在这个。 char newName[10]; Mat OutputImage; vector\u003cString\u003e FileName;\tglob(Pattern, FileName, false); for(int i=0;i\u003cFileName.size();i++) { if(FileName[i].find(\".png\")!=String::npos) { OutputImage = imread(FileName[i]); imshow(\"image\",OutputImage); waitKey(1000); sprintf(newName,\"%04d.png\",i); imwrite(OutputDir+newName,OutputImage); cout\u003c\u003c\"images found !\"\u003c\u003cendl; cout\u003c\u003c\"writed in\"\u003c\u003cOutputDir+newName\u003c\u003cendl; } else{ cout\u003c\u003c\"not a png image found\"\u003c\u003cendl; } } return 0; } CMakeList.txt如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #这是注释 cmake_minimum_required(VERSION 2.8) #cmake最低版本要求，低于这个版本编译不会通过 project(practice1) #工程名称，可以随意起名字 #设置源文件列表，当需要编译多个源文件时这样的形式是方便的，如set(SRC_LIST a.cpp b.cpp c.cpp) #SRC_LIST作为变量，main.cpp作为一个值赋给SRC_LIST这个变量 set(SRC_LIST main.cpp) find_package(OpenCV REQUIRED) include_directories(${OpenCV_INCLUDE_DIRS}) #${ }表示取变量的值 add_executable(${PROJECT_NAME} ${SRC_LIST}) #生成一个可执行文件，相关源文件是变量SRC_LIST中的值 target_link_libraries(${PROJECT_NAME} ${OpenCV_LIBS}) 一个最简的CMakeLists.txt如下：\n1 2 project(practice1) add_executable(practice1 main.cpp) 其他语句依实际构建工程需要添加。\n编译 将以上两个文件放在随意一个文件夹下，比如我的在/home/gaki/slampractice/p1，首先终端进入该文件夹： 1 cd /home/gaki/slampractice/p1 新建 build 文件夹并且进入 build 文件夹：\n1 mkdir build \u0026\u0026 cd build 构建工程：(注意cmake 后面有两个 .)\n1 cmake .. 注意到 build 目录下生成了 Makefile ,然后进行工程实际构建:\n1 make 此时 build 目录中生成可执行文件 practice1 ,运行：\n1 ./practice1 此时 output 文件夹下输出重命名后的图片，程序顺利执行～\n","title":"SLAM的基本框架","uri":"/contents/slam-sfm/slam1/"},{"categories":["contents"],"content":"有这样一个说法，计算机视觉研究方向可以分为两个主要方向：基于学习的方法和基于几何的方法。基于学习的方法目前最火的属于Deep Learning,基于几何的方法最火的非SLAM莫属了。这里的学习笔记主要记录在公众号“计算机视觉life”中学习的专题《从零开始一起学习SLAM》和高翔博士的《视觉SLAM十四讲》，小白我也是从零基础刚开始学习，希望笔记能一直记录下去吧~\n很多小伙伴入门SLAM用的是高翔的《视觉SLAM十四讲》，需要下载高清电子版的话可以戳这里，提取码是：onkn。\n点击这里，可以下载几篇SLAM领域的综述论文，提取码：79um。目前只看了一篇中文的综述（惭愧），其他的等学习了一部分基础知识后再回来总结。\n**《基于单目视觉的同时定位与地图构建方法综述》**概要 目前用在SLAM上的Sensor主要分两大类，激光雷达和摄像头，而基于单目视觉的同时定位与地图构建技术（V-SLAM）属于使用摄像头做Sensor一大类，这篇论文简述了V-SLAM中的基于单目视觉的SLAM的基本原理（具体看论文去~）。代表性的单目V-SLAM系统为：\n基于滤波器的V-SLAM:论文介绍了MonoSLAM和MSCKF两个系统。\n基于关键帧BA（BA指的是集束调整：bundle adjustment）：论文重点介绍了ORB-SLAM。\n基于直接跟踪的V-SLAM：论文介绍了DTAM和LSD-SLAM。\n基于滤波器和基于关键帧 BA 的 V-SLAM 通常都需要在图像中提取并匹配特征点, 因此对环境特征的丰富程度和图像质量(如模糊程度、图像噪声等)十分敏感. 相比之下, 直接跟踪法(Direct Tracking)不依赖于特征点的提取和匹配, 而是直接通过比较像素颜色来求解相机运动, 因此通常在特征缺失、图像模糊等情况下有更好的鲁棒性.\n此外论文对这三种单目V-SLAM系统的优缺点做了详细分析。\n近年研究热点与发展趋势 缓解特征依赖 V-SLAM 最大的局限在于过于依赖场景特征. 基于直接跟踪的方法通过直接对比像素颜色, 避免了对特征缺失/图像模糊非常敏感的特征提取和匹配过程, 从而很大程度上缓解了特征依赖. 然而, 稠密或半稠密的直接跟踪会引入很大的计算量, 若要运行在计算性能较低的移动设备上, 就需要将图像降采样至很小的分辨率, 那么必然会降低跟踪精度。\nV-SLAM 对场景特征的依赖, 本质上是由于使用了过于底层的局部特征(点特征), 如果能利用边缘、 平面等更为高层的图像信息, 也能有效地缓解特征依赖。\n稠密三维重建 基于单目摄像头的稠密三维重建的难点在于需要实时恢复稠密的深度图, 这一过程通常都需要引入很大的计算量, 关键是如何权衡重建精度和计算效率。\n上述方法虽然都能实时重建出稠密的三维信息, 但大多依赖于 GPU 并行计算. 然而在很多 AR 应用中, 往往 GPU 需要用来绘制虚拟物体. 因此如何进一步提高效率, 只用 CPU 就能恢复稠密或半稠密的三维信息, 仍值得进一步研究。\n多传感器融合 基于单一传感器的定位方案不可避免地都有各自的固有局限: 仅基于图像的 V-SLAM 依赖场景纹理特征; 仅基于 IMU 的定位通常有严重的误差累积; 仅基于深度的 SLAM 依赖于场景几何特征, 且设备获取深度的精度和范围受限于设备的成本和功耗. 只有将不同传感器数据融合起来, 才能互相取长补短, 达到最高的精度和鲁棒性. 如今大多数移动设备都配有单目摄像头和 IMU, 有的甚至配有双目、 鱼眼或深度摄像头, 如何融合这些多传感器数据成为近年来的一个研究热点。\n","title":"SLAM论文综述","uri":"/contents/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/slam-total/"},{"categories":["contents"],"content":"\nSFM（structure-from-motion）算法是一种基于各种收集到的无序图片进行三维重建的离线算法。顾名思义是从运动中（不同时间拍摄的图片集）恢复物体的三维结构，这需要估计出图片的$R,t$，结合相机内参重建稀疏点云。\nSFM在多视图三维重建步骤中的对应位置 如上图，完整的多视图三维重建：重建稀疏点云–重建稠密点云–Mesh–纹理Texture。SFM负责重建稀疏点云这一部分。这需要从多张视图中估计出照片的旋转平移矩阵$R,t$，结合相机内参恢复物体稀疏点云结构。\n算法的关键是获得两张图片中的对应点，然后估计基础矩阵$F$，再估计本征矩阵$E$，再通过SVD分解求得较好的$R,t$，得到物体的三维点，最后将多个稀疏点云融合在一起，这里一个常用的算法是Bundle Adjustment(BA)。\n算法原理 首先得了解物体在针孔相机模型下，由世界坐标系投影到像素坐标系的数学模型。图像中的像素坐标点$\\mathbf{x}=\\left[\\begin{array}{l}{x} \\\\ {y}\\end{array}\\right]$和它在真实世界下的世界坐标点$\\mathbf{X}=\\left[\\begin{array}{l}{X} \\\\ {Y} \\\\ {Z}\\end{array}\\right]$的对应关系为：\n$$ \\left[\\begin{array}{l}{x} \\\\ {y} \\\\ {1}\\end{array}\\right]=\\left[\\begin{array}{lll}{f} \u0026 {0} \u0026 {0} \\\\ {0} \u0026 {f} \u0026 {0} \\\\ {0} \u0026 {0} \u0026 {1}\\end{array}\\right]\\left[\\begin{array}{cccc}{1} \u0026 {0} \u0026 {0} \u0026 {0} \\\\ {0} \u0026 {1} \u0026 {0} \u0026 {0} \\\\ {0} \u0026 {0} \u0026 {1} \u0026 {0}\\end{array}\\right]\\left[\\begin{array}{cc}{\\mathbf{R}} \u0026 {\\mathbf{t}} \\\\ {\\mathbf{0}^{T}} \u0026 {1}\\end{array}\\right]\\left[\\begin{array}{c}{X} \\\\ {Y} \\\\ {Z} \\\\ {1}\\end{array}\\right] $$\n用矩阵形式表示： $$ \\mathbf{x}=\\mathbf{K}[\\mathbf{R} | \\mathbf{t}] \\mathbf{X} $$ 或者： $$ \\mathbf{x}=\\mathbf{P} \\mathbf{X} $$ 对于同一个世界坐标系下的点，在多个相机坐标系下成像，即为：\n$$ \\begin{aligned} \\mathbf{x}_{1} \u0026=\\mathbf{K}\\left[\\mathbf{R}_{1} | \\mathbf{t}_{1}\\right] \\mathbf{X} \\\\ \\mathbf{x}_{2} \u0026=\\mathbf{K}\\left[\\mathbf{R}_{2} | \\mathbf{t}_{2}\\right] \\mathbf{X} \\\\ \\mathbf{x}_{3} \u0026=\\mathbf{K}\\left[\\mathbf{R}_{3} | \\mathbf{t}_{3}\\right] \\mathbf{X} \\end{aligned} $$\n如果能准确找到这些对应点，就可以准确计算出各相机的$R,t$，但事实上只能用估计的方法求得较好的对应点。一个思路是提取各图像中物体的特征点，常用的算法是SIFT，因其具有尺度和旋转不变性，再去做匹配，再用RANSC算法优化改善匹配对。之后利用F矩阵和E矩阵可以算出相机的$R,t$，再通过三角化得到稀疏点云，如下图： SIFT特征提取 参考这篇文章。\n特征匹配 第二步是匹配和建立track,图像对两两匹配,一般采用欧式距离.有两种方法：\n粗暴匹配,对所有特征点都穷举计算距离\n邻近搜索,建立KD树,缩小搜索范围,能提高效率,但也有可能不是最优,所以邻域取值是关键,越大越准确,越大计算量越大\n这里参考的是这篇文章。\n然而初选的匹配对可能还是不可靠，需要用几何约束去检测。这个测试是基于事实的，假设一个静止场景，不是所有的匹配特征点在实际场景中是符合物理规律的。那么就需要计算对极几何，F矩阵可以把两张图片之间的像素坐标联系起来，并包含相机的内参信息。每一个符合的匹配对像素坐标都需要满足：\n$$ \\mathbf{x}_{1}^{T} \\mathbf{F} \\mathbf{x}_{2}=0 $$\n然而像这种F矩阵计算出有很多噪声数据，需要用RANSAC(随机抽样一致性)算法进行滤波，用直接线性变换(DLT)(需要八组对应点)来进行RANSACA假设，剔除不满足基础矩阵的匹配对。 用RANSAC去估计基础矩阵F的思路是：\n多次迭代以下流程：\n选8个点\n用DLT算法计算F\n记录内点的数目\n选取内点最多的对应F\n这时候可以找到比较好的匹配点对了。\n特征分解本征矩阵得到R,t 基础矩阵F和本征矩阵E的关系是：\n$$ \\begin{array}{l}{\\mathbf{x}_{1}^{T} \\mathbf{F} \\mathbf{x}_{2}=0} \\\\ {\\mathbf{E}=\\mathbf{K}_{\\mathbf{I}}^{T} \\mathbf{F} \\mathbf{K}_{2}}\\end{array} $$\n$R,t$可以由本征矩阵E经过SVD分解求得。\n这里有一个问题，给定一个本征矩阵$\\mathbf{E}=\\mathbf{U} \\operatorname{diag}(1,1,0) \\mathbf{V}^{T}$ 和第一个相机矩阵 $\\mathbf{P}_{1}=[\\mathbf{I} | \\mathbf{0}]$ ，第二个相机矩阵有以下几种选择：\n$$ \\begin{array}{l}{\\mathbf{P}_{2}=\\left[\\mathbf{U} \\mathbf{W} \\mathbf{V}^{T} |+\\mathbf{u}_{3}\\right]} \\\\ {\\mathbf{P}_{2}=\\left[\\mathbf{U} \\mathbf{W} \\mathbf{V}^{T} |-\\mathbf{u}_{3}\\right]} \\\\ {\\mathbf{P}_{2}=\\left[\\mathbf{U} \\mathbf{W}^{T} \\mathbf{V}^{T} |+\\mathbf{u}_{3}\\right]} \\\\ {\\mathbf{P}_{2}=\\left[\\mathbf{U} \\mathbf{W}^{T} \\mathbf{V}^{T} |-\\mathbf{u}_{3}\\right]}\\end{array} \\quad \\mathbf{W}=\\left[\\begin{array}{ccc}{0} \u0026 {-1} \u0026 {0} \\\\ {1} \u0026 {0} \u0026 {0} \\\\ {0} \u0026 {0} \u0026 {1}\\end{array}\\right] $$\n我们希望得到上图中，图一的情况，这种情况下，两个相机的“从光心到主点的射线”与“三维物体点到光心的连线”的夹角都小于90度。 公式表示为： 选择$(\\mathbf{X}-\\mathbf{C}) \\cdot \\mathbf{R}(3, :)^{T}\u003e0 ?$的对应的$P_2$即可。这时候两幅图像的$R,t$均求得。\n点云融合 由上面计算出来的$R,t$和相机内参，可以恢复出物体的稀疏点云结构，如何将点云融合呢，如果求出的$R,t$是一个准确解，这时直接讲各部分点云通过$R,t$变换到同一基准下就可以完成融合的过程。单上面求解出来的$R,t$仍然不够准确，这时候可以通过Bundle Adjustment（BA），这是一个非线性优化的过程，目的是使重建误差降低到最小，通过调整POSE和三维点使反向投影差最小，如果相机没有标定,还应该将焦距也参与平差。BA最小化以下目标函数：\n$$ \\min \\sum_{i} \\sum_{j}\\left(\\tilde{\\mathbf{x}}_{i}^{j}-\\mathbf{K}\\left[\\mathbf{R}_{i} | \\mathbf{t}_{i}\\right] \\mathbf{X}^{j}\\right)^{2} $$\n多幅图像的计算方法，依次迭代上面的流程，求得比较准确的$R,t$后，即可进行点云的融合，到此完成稀疏点云的重建过程。\n","title":"SFM算法原理初简介","uri":"/contents/slam-sfm/sfm-intro/"},{"categories":["contents"],"content":"记录openMVG，openMVS的编译，使用流程。\n三维重建流程 三维重建的流程大致如下：首先，通过多角度拍摄或者从视频中提取得到一组图像序列，将这些图像序列作为整个系统的输入；随后，在多视角的图像中，根据纹理特征提取出稀疏特征点（称为点云），通过这些特征点估计相机位置和参数；在得到相机参数并完成特征点匹配后，我们就可以获得更稠密的点云（这些点可以附带颜色，从远处看就像还原了物体本身一样，但从近处能明显看出它们只是一些点）；最后根据这些点重建物体表面，并进行纹理映射，就还原出三维场景和物体了。\n概括起来就是：图像获取-\u003e特征匹配-\u003e深度估计-\u003e稀疏点云-\u003e相机参数估计-\u003e稠密点云-\u003e表面重建-\u003e纹理映射\n详细见这篇文章。\nopenMVG搭配openMVS 刚好完成从原始图像到稀疏点云、重建稠密点云、重建表面和纹理映射的流程。以下是两个开源系统的编译和使用流程。\n编译 openMVG 跟着官方文档走就可以了，没有什么坑，注意依赖库的版本就行。\n或者看这里：\ngit clone --recursive https://github.com/openMVG/openMVG.git sudo apt-get install libpng-dev libjpeg-dev libtiff-dev libxxf86vm1 libxxf86vm-dev libxi-dev libxrandr-dev sudo apt-get install graphviz cd path/to/openMVG mkdir build cd build cmake -DCMAKE_BUILD_TYPE=RELEASE -DOpenMVG_BUILD_TESTS=ON -DOpenMVG_BUILD_EXAMPLES=ON . ../src/ make -j12 make test sudo make install 编译openMVS 主要参考官方文档\n文章参考:\nOpenMVG与OpenMVS安装配置、简单使用\nubuntu16.04下安装openMVG+openMVS +三维重建测试 Ubuntu使用OpenMVG和OpenMVS进行三维重建\n有几个坑，需要注意：\n官方文档第四行有一句：main_path=‘pwd’，不用管行命令（直接跳过），在cmake的时候执行cmake . ../openMVS -DCMAKE_BUILD_TYPE=Release -DVCG_ROOT=\"$main_path/vcglib\"时，把$main_path替换成你安装vcglib的对应路径就好。 Eigen必须是3.2.X版本，如果按照官方文档安装貌似装的是最新版本，可以自己下载源码安装3.2.X版本。 在后面进行MVS的测试是，需要把openMVS_build/bin的文件复制到linux下面/user/bin文件夹里面；才能运行MVS重建命令。 使用流程 openMVG的sfM用例 见官方文档\n比如我的流程是：\nopenMVG_main_SfMInit_ImageListing -d ~/3d-reco/openMVG/src/openMVG/exif/sensor_width_database/sensor_width_camera_database.txt -i ~/datasets/ImageDataset_SceauxCastle-master/images -o ~/datasets/ImageDataset_SceauxCastle-master/matches/ openMVG_main_ComputeFeatures -i ~/datasets/ImageDataset_SceauxCastle-master/matches/sfm_data.json -o ~/datasets/ImageDataset_SceauxCastle-master/matches/ openMVG_main_ComputeMatches -n ANNL2 -f 1 -i ~/datasets/ImageDataset_SceauxCastle-master/matches/sfm_data.json -o ~/datasets/ImageDataset_SceauxCastle-master/matches/ openMVG_main_GlobalSfM -i ~/datasets/ImageDataset_SceauxCastle-master/matches/sfm_data.json -m ~/datasets/ImageDataset_SceauxCastle-master/matches/ -o ~/datasets/ImageDataset_SceauxCastle-master/outReconstruction/ 在openMVG_main_ComputeMatches步骤时我遇到invalid match files的错误，通过更换matching的方法，即使用指令openMVG_main_ComputeMatches -n ANNL2 -f 1就可以执行下面的重建步骤。\n另外，若使用openMVG_main_GlobalSfM，可能也会遇到invalid match files的问题，这是因为上一步骤可能生成的是matches.f.bin，而openMVG_main_GlobalSfM需要的是matches.e.bin，这时将matches.f.bin重命名为matches.e.bin即可。\nopenMVS流程 见官方文档\n我的流程：\nsudo openMVG_main_openMVG2openMVS -i ~/datasets/ImageDataset_SceauxCastle-master/reconstruction_global/robust.bin -o ~/datasets/ImageDataset_SceauxCastle-master/reconstruction_global/global_scene.mvs DensifyPointCloud global_scene.mvs ReconstructMesh -d 4 global_scene_dense.mvs RefineMesh --resolution-level=4 global_scene_dense_mesh.mvs TextureMesh global_scene_dense_mesh_refine.mvs 以上。\n","title":"三维重建系统搭建-openMVG和openMVS","uri":"/contents/slam-sfm/openmvg-openmvs/"},{"categories":["contents"],"content":"记录信号检测与估值的学习笔记，以后不知道用不用得着，但先保存下来吧。\n","title":"信号检测与估值笔记","uri":"/contents/day-day-up/signal-detect-and-estimate/"},{"categories":["contents"],"content":"之前想在win10系统基础上安装Ubuntu 18.04，折腾了两天才装上。这里把遇到的一些坑记录下来，防止以后要重装系统时忘记了。\n概述 折腾了这么久，最后发现是有几个概念当初不清楚，把这几个概念搞明白就知道怎么做了。\n系统引导方式：Legacy和UEFI 概念了解戳这里：Legacy和UEFI 分区表类型：MBR和GPT 概念了解戳这里：MBR和GPT 首先，我是想在台式机上，在已经安装好win10的基础上，再安装Ubuntu 18.04.1 LTS，但在分区过程中出现报错：starting sector number, 4532393984 exceeds the msdos-partition-table-imposed maximum ，在一个解答中发现，由于我的台式机只有一块4TB的大磁盘，而且当初安装win10时用的是MBR分区表格式，而这个格式可利用的磁盘空间不能超过2TB，而且这种格式在一块磁盘上只允许有4个主分区（或者3个主分区加一个扩展分区，在扩展分区上可以创建逻辑分区）。在win10的磁盘管理器中可以看到有一块未利用的磁盘空间（在最后面）。当在安装ubuntu时划分的空间再加上win10所占的空间超过2TB时就会报错。如果一块磁盘的大小不超过2TB，用MBR分区表格式，只要主分区数不超过4个就没问题，一旦超过，得换用GPT分区表格式了。无奈win10用的是MBR格式装的，就把win10删了，将磁盘转为GPT格式，再重装双系统。（后来发现，可以在DiskGenius中将MBR转换为GPT，虽然有风险，但总比删掉好。不管哪种方式，记得把重要数据做好备份）。\n我最终成功在4TB大磁盘上安装双系统的方式是：UEFI+GPT。安装前记得确保自己的磁盘是GPT格式的，并且支持UEFI引导方式。\n我先安装好ubuntu，再安装win10。（我觉得安装顺序没有多大差别，把各个分区分配好就行。）\n安装ubuntu 跟着这篇文章做就好，不重复造轮子了。但注意，把U盘插在USB2.0口上安装，并且在刚进入安装界面时，选择 install ubuntu 的地方， 按’e’键进入edit mode ， 找到\"quiet splash —\"，把“—”换成“acpi=off”,然后F10继续安装；然后跟着上面的文章的步骤安装。\n安装时出现的报错： 我在安装ubuntu时，把启动盘插到USB3.0口上最终无法识别安装盘，插到USB2.0口上就好了。 安装过程出现 acpi error，解决方法参考：这篇文章 报错：无法将grub-efi-amd64-signed软件包安装到/target，这时未完全安装完，不要关机，得先安装boot修复工具： sudo add-apt-repository ppa:yannubuntu/boot-repair sudo apt-get update sudo apt-get install -y boot-repair \u0026\u0026 boot-repair 然后根据引导操作修复就可以了，修复结束后重启,这时候可以把安装盘拔掉了，重启后可以进入ubuntu系统。\n安装win10 我用的是老毛桃装机软件刻录的启动盘，选的是UEFI方式做的启动盘，制作方式上网搜一下就好。老毛桃教程戳这里。不管用什么刻录软件，只要保证在同一块硬盘上安装的双系统都是 GPT+UEFI 方式就好。\n","title":"UEFI模式下Windows10上安装Ubuntu 18.04.1 LTS双系统","uri":"/contents/tools/ubuntu-and-windows/"},{"categories":["contents"],"content":"在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，何恺明等人提出了残差网络（ResNet）。本节记录残差网络和它的变式：稠密网络。摘自《动手学深度学习》\n残差网络（ResNet） 网络模型 残差块 下图的右图是ResNet的基础块，即残差块（residual block）。在残差块中，输入可通过跨层的数据线路更快地向前传播。 ResNet沿用了VGG全3×3卷积层的设计。残差块里首先有2个有相同输出通道数的3×3卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。\nResNet模型 ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。\nGoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\npython实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import d2lzh as d2l from mxnet import gluon, init, nd from mxnet.gluon import nn # 残差块的实现如下。它可以设定输出通道数、是否使用额外的1×1卷积层来修改通道数以及卷积层的步幅。 class Residual(nn.Block): # 本类已保存在d2lzh包中方便以后使用 def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs): super(Residual, self).__init__(**kwargs) self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1, strides=strides) self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2D(num_channels, kernel_size=1, strides=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm() self.bn2 = nn.BatchNorm() def forward(self, X): Y = nd.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y)) if self.conv3: X = self.conv3(X) return nd.relu(Y + X) # ResNet模型实现 net = nn.Sequential() net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3), nn.BatchNorm(), nn.Activation('relu'), nn.MaxPool2D(pool_size=3, strides=2, padding=1)) def resnet_block(num_channels, num_residuals, first_block=False): blk = nn.Sequential() for i in range(num_residuals): if i == 0 and not first_block: blk.add(Residual(num_channels, use_1x1conv=True, strides=2)) else: blk.add(Residual(num_channels)) # 注意，这里对第一个模块做了特别处理。因为前面的最大池化层已经将输入宽高减半，就不需要使用1x1卷积了。 return blk net.add(resnet_block(64, 2, first_block=True), resnet_block(128, 2), resnet_block(256, 2), resnet_block(512, 2)) # 最后，与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。 net.add(nn.GlobalAvgPool2D(), nn.Dense(10)) # 获取数据和训练模型 lr, num_epochs, batch_size, ctx = 0.05, 5, 256, d2l.try_gpu() net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier()) trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr}) train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs) 稠密连接网络（DenseNet） 图中将部分前后相邻的运算抽象为模块𝐴和模块𝐵。与ResNet的主要区别在于，DenseNet里模块𝐵的输出不是像ResNet那样和模块𝐴的输出相加，而是在通道维上连结。这样模块𝐴的输出可以直接传入模块𝐵后面的层。在这个设计里，模块𝐴直接跟模块𝐵后面的所有层连接在了一起。这也是它被称为“稠密连接”的原因。\nDenseNet的主要构建模块是稠密块（dense block）和过渡层（transition layer）。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。\n模型结构 稠密块 DenseNet使用了ResNet改良版的“批量归一化、激活和卷积”结构。稠密块由多个conv_block组成，每块使用相同的输出通道数。但在前向计算时，我们将每块的输入和输出在通道维上连结。\n过渡层 由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡层用来控制模型复杂度。它通过1×1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。\nDenseNet模型 DenseNet首先使用同ResNet一样的单卷积层和最大池化层。类似于ResNet接下来使用的4个残差块，DenseNet使用的是4个稠密块。同ResNet一样，我们可以设置每个稠密块使用多少个卷积层。 ResNet里通过步幅为2的残差块在每个模块之间减小高和宽。这里我们则使用过渡层来减半高和宽，并减半通道数。最后，同ResNet一样，最后接上全局池化层和全连接层来输出。\npython实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 import d2lzh as d2l from mxnet import gluon, init, nd from mxnet.gluon import nn # DenseNet使用了ResNet改良版的“批量归一化、激活和卷积”结构，我们首先在conv_block函数里实现这个结构。 def conv_block(num_channels): blk = nn.Sequential() blk.add(nn.BatchNorm(), nn.Activation('relu'), nn.Conv2D(num_channels, kernel_size=3, padding=1)) return blk # 稠密块由多个conv_block组成，每块使用相同的输出通道数。但在前向计算时，我们将每块的输入和输出在通道维上连结。 class DenseBlock(nn.Block): def __init__(self, num_convs, num_channels, **kwargs): super(DenseBlock, self).__init__(**kwargs) self.net = nn.Sequential() for _ in range(num_convs): self.net.add(conv_block(num_channels)) def forward(self, X): for blk in self.net: Y = blk(X) X = nd.concat(X, Y, dim=1) # 在通道维上将输入和输出连结 return X # 卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。稠密块中的每个卷积层的输出通道数最后会累加到稠密块的最后输出通道那里（连结） # 通过1×1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。（加在稠密块后面） def transition_block(num_channels): blk = nn.Sequential() blk.add(nn.BatchNorm(), nn.Activation('relu'), nn.Conv2D(num_channels, kernel_size=1), nn.AvgPool2D(pool_size=2, strides=2)) return blk # DenseNet模型，DenseNet首先使用同ResNet一样的单卷积层和最大池化层。 net = nn.Sequential() net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3), nn.BatchNorm(), nn.Activation('relu'), nn.MaxPool2D(pool_size=3, strides=2, padding=1)) # 类似于ResNet接下来使用的4个残差块，DenseNet使用的是4个稠密块。同ResNet一样，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与上一节的ResNet-18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。 # ResNet里通过步幅为2的残差块在每个模块之间减小高和宽。这里我们则使用过渡层来减半高和宽，并减半通道数。 num_channels, growth_rate = 64, 32 # num_channels为当前的通道数 num_convs_in_dense_blocks = [4, 4, 4, 4] for i, num_convs in enumerate(num_convs_in_dense_blocks): net.add(DenseBlock(num_convs, growth_rate)) # 上一个稠密块的输出通道数 num_channels += num_convs * growth_rate # 在稠密块之间加入通道数减半的过渡层 if i != len(num_convs_in_dense_blocks) - 1: num_channels //= 2 net.add(transition_block(num_channels)) # 同ResNet一样，最后接上全局池化层和全连接层来输出。 net.add(nn.BatchNorm(), nn.Activation('relu'), nn.GlobalAvgPool2D(), nn.Dense(10)) lr, num_epochs, batch_size, ctx = 0.1, 5, 256, d2l.try_gpu() net.initialize(ctx=ctx, init=init.Xavier()) trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr}) train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs) ","title":"残差网络结构","uri":"/contents/dl/resnet-densenet/"},{"categories":["contents"],"content":"记录卷积神经网络（CNN）的卷积层，卷积核，池化层等概念和网络输入输出维度的计算方法。摘自《动手学深度学习》\n卷积层的概念 卷积神经网络（convolutional neural network）是含有卷积层（convolutional layer）的神经网络。\n虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。因为假如使用卷积运算学出的核数组按上下、左右翻转，在利用该核数组做卷积运算时，依然会得出和互相关一样的输出（负负得正和正正得正）。以后说卷积其实指的是互相关运算。\n卷积层的运算 最简单的二维卷积层 如上图，输入是一个高和宽均为3的二维数组（通道为1）。我们将该数组的形状记为3×3或（3，3）。核数组的高和宽分别为2。该数组在卷积计算中又称卷积核或过滤器（filter）。卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即2×2。图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：0×0+1×1+3×2+4×3=19。\n二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。\n多输入通道和多输出通道 前面用到的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3×ℎ×𝑤$的多维数组。我们将大小为3的这一维称为通道（channel）维。\n多输入通道 如上图：当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。假设输入数据的通道数为$𝑐_𝑖$，那么卷积核的输入通道数同样为$𝑐_𝑖$。设卷积核窗口形状为$𝑘_ℎ×𝑘_𝑤$。当$𝑐_𝑖=1$时，我们知道卷积核只包含一个形状为$𝑘_ℎ×𝑘_𝑤$的二维数组。当$𝑐_𝑖\u003e1$时，我们将会为每个输入通道各分配一个形状为$𝑘_ℎ×𝑘_𝑤$的核数组。把这$𝑐_𝑖$个数组在输入通道维上连结，即得到一个形状为$𝑐_𝑖×𝑘_ℎ×𝑘_𝑤$的卷积核。由于输入和卷积核各有$𝑐_𝑖$个通道，我们可以在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这$𝑐_𝑖$个互相关运算的二维输出按通道相加，得到一个二维数组。这就是含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出。\n多输出通道 当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。设卷积核输入通道数和输出通道数分别为$𝑐_𝑖$和$𝑐_𝑜$，高和宽分别为$𝑘_ℎ$和$𝑘_𝑤$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$𝑐_𝑖×𝑘_ℎ×𝑘_𝑤$的核数组。将它们在输出通道维上连结，卷积核的形状即$𝑐_𝑜×𝑐_𝑖×𝑘_ℎ×𝑘_𝑤$。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。\n总结就是：输入的通道数$c_i$取决于卷积核的通道数，输出的通道数$c_o$取决于卷积核的个数。\n1×1卷积层 最后我们讨论卷积窗口形状为1×1（$𝑘_ℎ=𝑘_𝑤=1$）的多通道卷积层。我们通常称之为1×1卷积层，并将其中的卷积运算称为1×1卷积。因为使用了最小窗口，1×1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，1×1卷积的主要计算发生在通道维上。图中展示了使用输入通道数为3、输出通道数为2的1×1卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×1卷积层的作用与全连接层等价。\n特征图和感受野 二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。如图： 输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为2×2的输出记为$Y$，并考虑一个更深的卷积神经网络：将$Y$与另一个形状为2×2的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。\n填充和步幅 填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。卷积神经网络经常使用奇数高宽的卷积核，如1、3、5和7，使得两端上的填充个数相等。\n在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。我们可以使用更大步幅。下图展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了3行，而在输出第一行第二个元素时卷积窗口向右滑动了2列。当卷积窗口在输入上再向右滑动2列时，由于输入元素无法填满窗口，无结果输出。 卷积运算的输出形状的计算公式： $$ \\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor. $$ 该输出的通道数取决于参与运算的卷积核的个数。\n池化层 但实际图像里，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出Y中的不同位置，进而对后面的模式识别造成不便。而池化（pooling）层，它的提出是为了缓解卷积层对位置的过度敏感性。\n同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最大池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。\n二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$𝑝×𝑞$的池化层称为$𝑝×𝑞$池化层，其中的池化运算叫作$𝑝×𝑞$池化。\n填充和步幅 同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。\n多通道 在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。这意味着池化层的输出通道数与输入通道数相等。\n池化层小结 最大池化和平均池化分别取池化窗口中输入元素的最大值和平均值作为输出。 池化层的一个主要作用是缓解卷积层对位置的过度敏感性。 可以指定池化层的填充和步幅。 池化层的输出通道数跟输入通道数相同。 ","title":"卷积神经网络的卷积层和池化层","uri":"/contents/dl/cnn-layer/"},{"categories":["contents"],"content":"本节介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易 。内容摘自《动手学深度学习》\n通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。\n批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。\n原理 对全连接层做批量归一化 我们先考虑如何对全连接层做批量归一化。通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为$𝑢$，权重参数和偏差参数分别为$𝑊$和$𝑏$，激活函数为$\\phi$。设批量归一化的运算符为$BN$。那么，使用批量归一化的全连接层的输出为: $$ \\phi(\\text{BN}(\\boldsymbol{x})), $$ 其中批量归一化输入$\\boldsymbol{x}$由仿射变换 $$ \\boldsymbol{x} = \\boldsymbol{W\\boldsymbol{u} + \\boldsymbol{b}} $$ 得到。考虑一个由𝑚个样本组成的小批量，仿射变换的输出为一个新的小批量$\\mathcal{B} = {\\boldsymbol{x}^{(1)}, \\ldots, \\boldsymbol{x}^{(m)} }$。它们正是批量归一化层的输入。对于小批量$\\mathcal{B}$中任意样本$\\boldsymbol{x}^{(i)} \\in \\mathbb{R}^d, 1 \\leq i \\leq m$，批量归一化层的输出同样是$𝑑$维向量: $$ \\boldsymbol{y}^{(i)} = \\text{BN}(\\boldsymbol{x}^{(i)}), $$ 并由以下几步求得。首先，对小批量 $\\mathcal{B}$ 求均值和方差：\n$$ \\boldsymbol{\\mu}_\\mathcal{B} \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m} \\boldsymbol{x}^{(i)}, $$\n$$ \\boldsymbol{\\sigma}_\\mathcal{B}^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B})^2, $$\n其中的平方计算是按元素求平方。接下来，使用按元素开方和按元素除法对$\\boldsymbol{x}^{(i)}$标准化：\n$$ \\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B}}{\\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}}, $$\n这里 $\\epsilon \u003e 0$ 是一个很小的常数，保证分母大于0。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 $\\boldsymbol{\\gamma}$ 和偏移（shift）参数 $\\boldsymbol{\\beta}$。这两个参数和$\\boldsymbol{x}^{(i)}$形状相同，皆为𝑑维向量。它们与$\\boldsymbol{x}^{(i)}$分别做按元素乘法（符号$\\odot$）和加法计算：\n$$ {\\boldsymbol{y}}^{(i)} \\leftarrow \\boldsymbol{\\gamma} \\odot \\hat{\\boldsymbol{x}}^{(i)} + \\boldsymbol{\\beta}. $$\n至此，我们得到了$\\boldsymbol{x}^{(i)}$的批量归一化的输出$\\boldsymbol{y}^{(i)}$。 值得注意的是，可学习的拉伸和偏移参数保留了不对$\\hat{\\boldsymbol{x}}^{(i)}$做批量归一化的可能：此时只需学出$\\boldsymbol{\\gamma} = \\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}$和$\\boldsymbol{\\beta} = \\boldsymbol{\\mu}_\\mathcal{B}$。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。\n对卷积层做批量归一化 对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。设小批量中有𝑚个样本。在单个通道上，假设卷积计算输出的高和宽分别为𝑝和𝑞。我们需要对该通道中𝑚×𝑝×𝑞个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中𝑚×𝑝×𝑞个元素的均值和方差。\n预测时的批量归一化 使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。\npython实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import d2lzh as d2l from mxnet import autograd, gluon, init, nd from mxnet.gluon import nn def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum): # 通过autograd来判断当前模式是训练模式还是预测模式 if not autograd.is_training(): # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差 X_hat = (X - moving_mean) / nd.sqrt(moving_var + eps) else: assert len(X.shape) in (2, 4) if len(X.shape) == 2: # 使用全连接层的情况，计算特征维上的均值和方差（这里指的是每个特征上都统计（小批量样本的）均值和方差） mean = X.mean(axis=0) var = ((X - mean) ** 2).mean(axis=0) else: # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持X的形状以便后面可以做广播运算（同上） mean = X.mean(axis=(0, 2, 3), keepdims=True) var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True) # 训练模式下用当前的均值和方差做标准化 X_hat = (X - mean) / nd.sqrt(var + eps) # 更新移动平均的均值和方差 moving_mean = momentum * moving_mean + (1.0 - momentum) * mean moving_var = momentum * moving_var + (1.0 - momentum) * var Y = gamma * X_hat + beta # 拉伸和偏移 return Y, moving_mean, moving_var 接下来，我们自定义一个BatchNorm层。它保存参与求梯度和迭代的拉伸参数gamma和偏移参数beta，同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。BatchNorm实例所需指定的num_features参数对于全连接层来说应为输出个数，对于卷积层来说则为输出通道数。该实例所需指定的num_dims参数对于全连接层和卷积层来说分别为2和4。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BatchNorm(nn.Block): def __init__(self, num_features, num_dims, **kwargs): super(BatchNorm, self).__init__(**kwargs) if num_dims == 2: shape = (1, num_features) else: shape = (1, num_features, 1, 1) # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1 self.gamma = self.params.get('gamma', shape=shape, init=init.One()) self.beta = self.params.get('beta', shape=shape, init=init.Zero()) # 不参与求梯度和迭代的变量，全在内存上初始化成0 self.moving_mean = nd.zeros(shape) self.moving_var = nd.zeros(shape) def forward(self, X): # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上 if self.moving_mean.context != X.context: self.moving_mean = self.moving_mean.copyto(X.context) self.moving_var = self.moving_var.copyto(X.context) # 保存更新过的moving_mean和moving_var Y, self.moving_mean, self.moving_var = batch_norm( X, self.gamma.data(), self.beta.data(), self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9) return Y ","title":"批标准化","uri":"/contents/dl/barch-normal/"},{"categories":["contents"],"content":"异常处理机制为程序中异常检测和异常处理这两部分的协作提供支持。\n概念 throw表达式\n异常检测部分使用throw表达式来表示它遇到了无法处理的问题。我们说throw引发了异常。\ntry语句块\n异常处理部分使用try语句块处理异常。try语句块以关键字try开头，并以一个或多个catch子句结束。\n标准异常 1 2 3 4 5 6 7 8 9 10 exception //最常见的问题 runtime_error //只有在运行时才能检测出的问题 range_error //运行时错误： 生成的结果超出了有意义的值域范围 overflow_error //运行时错误: 计算上溢 underflow_error //运行时错误: 计算下溢 logic_error //程序逻辑错误 domain_error //逻辑错误： 参数对应的结果值不存在 invalid_argument //逻辑错误： 无效参数 length_error //逻辑错误： 试图创建一个超出该类型最大长度的对象 out_of_range //逻辑错误： 使用一个超出有效范围的值 我们只能以默认初始化的方式初始化 exception ，bad_alloc , bad_cast ，不允许为这些对象提供初始值。而其他异常类型必须用 string 对象或者C风格字符串初始化。\n例子 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #include \"stdafx.h\" #include \u003ciostream\u003e using namespace std; int main() { int num1, num2; while (cin \u003e\u003e num1 \u003e\u003e num2) { try { if (num2 == 0) { throw range_error(\"除数不能为0\"); } cout \u003c\u003c num1 / num2 \u003c\u003c endl; } catch (range_error err) { cout \u003c\u003c err.what() \u003c\u003c \"\\n Try again?Enter y or n\" \u003c\u003c endl; char c; cin \u003e\u003e c; if (c == 'n') break; } } } 每个标准库都定义了名为 what 的成员函数，这些函数没有参数，返回值是C风格字符串（const char*）。\n异常发生时,寻找处理代码的过程与调用函数的过程恰好相反，异常抛出时，首先搜索抛出该异常的函数，若没找到匹配的catch子句，终止该函数，并在调用该函数的函数中继续寻找。以此类推。当最终没有找到，系统会调用 terminate 函数并终止程序执行。同理，如果一段程序没有try语句且发生异常，系统会调用terminate函数并终止当前程序的执行。\n","title":"异常处理","uri":"/contents/c++/exception/"},{"categories":["contents"],"content":"谨以此文纪念逝去的本科四年时光。\n那时我们有梦 关于文学，关于爱情 关于穿越世界的旅行 如今我们深夜饮酒 杯子碰在一起 都是梦破碎的声音 ​\t北岛《波兰来客》\n大学四年下来有多少遗憾，数不清了，人大多数是后知后觉吧，很多现在觉得“如果当时怎么怎么做就好了”的想法，受限于当时的认知水平和眼界是万万想不到的。本人又是很喜欢钻牛角尖的主，害怕一件事情做不好干脆在起步就犹豫不前了，现在知道要完成一件事情不是线性发展的过程，更多是一个充满反馈环的曲线：起步–入门–认知提升–明确路线–继续深入–认知提升–明确路线…加上坚持不懈，在一个领域上的积累只是时间和学习难度的问题了。这些写在前面，愿我后三年脚步不停，立下的flag都能实现。\n回想大学几年，大一好奇心十足加入吉他社，捣鼓一阵子发现自己五音不全又是笨手笨脚的就放弃了，加了英语社跟着早读小半个学期无疾而终，科协划水，倒是支教坚持了一学期，徒步旅行，社会拓展三天两夜，大二学专业课不适应成绩下降，又迷上打游戏（不过质量效应三部曲真心好玩，回忆满满hhh），大三开始为我的均分害怕开始认真学习，担心，不安持续了一年，站在保研的边缘线，不敢往外校跑，又害怕考研，到最后大四初绩点出来，早已经没有其他选择，后悔遗憾也好，自我满足也好，再提也没有意义。大四一年实验室打杂，算是有一些心得吧，希望我能从现在开始端正心态，认真科研，用力生活。毕竟很多事情不经努力根本没有选择的机会，至于往事，既是过往，皆为序曲了吧。\n四年来去了南京，武汉，长沙，贵阳，黔东南，兰州，西宁，青海湖，杭州。还心念着四川的牛背山拍星空，新疆西藏也还没有机会去。\n毕业旅行去了想去的城市，见了相见的姑娘，喝了酒，把该说的话说清楚，愿她过的快活如意，学业进步。\n至于毕业，没什么感觉，无非把行李搬了一个校区，和我玩的好的哥们，大多数都留校读研了，往后三年相聚不难。\n学好专业知识，锻炼好身体，培养持续的业余兴趣，做好自己。愿三年后不再是个虚胖子，不管在学术上，还是身体上，还是思想上。\n","title":"写在本科毕业此时","uri":"/contents/broken-thoughts/graduate/"},{"categories":["contents"],"content":"记录应对过拟合的两种策略：权重衰减 (weight decay) 和丢弃法（dropout），并在实验“softmax回归分类”中探究这两种策略和它们的组合在应对过拟合方面的有效性。摘自《动手学深度学习》\n过拟合现象，即模型的训练误差远小于它在测试集上的误差。虽然增大训练数据集可能会减轻过拟合，但是获取额外的训练数据往往代价高昂。权重衰减和丢弃法可以有效应对过拟合的现象。\n权重衰减简介 权重衰减等价于𝐿2范数正则化，正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。我们先描述𝐿2范数正则化，再解释它为何又称权重衰减。\n𝐿2范数正则化 𝐿2范数正则化在模型原损失函数基础上添加𝐿2范数惩罚项，从而得到训练所需要最小化的函数。𝐿2范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。 以一个线性回归的损失函数为例子：\n$$ \\ell(w_1, w_2, b) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right)^2 $$\n将权重参数用向量$\\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为:\n$$ \\ell(w_1, w_2, b) + \\frac{\\lambda}{2n} |\\boldsymbol{w}|^2, $$\n其中超参数$\\lambda \u003e 0$。当权重参数均为0时，惩罚项最小。当𝜆较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将上面“线性回归”中权重𝑤1和𝑤2的迭代方式更改为：\n$$ \\begin{aligned} w_1 \u0026\\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right)w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_1^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\\\ w_2 \u0026\\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right)w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_2^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right). \\end{aligned} $$\n可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。\nweight-decay的python实现 在损失函数中增加一个惩罚项即可：\n1 2 def l2_penalty(w): return (w**2).sum() / 2 然后损失函数中：\n1 l = loss(net(X, w, b), y) + lambd * l2_penalty(w) 丢弃法简介 丢弃法有一些不同的变体。本节中提到的丢弃法特指倒置丢弃法（inverted dropout）。当对网络某一层使用丢弃法时，该层的单元将有一定概率被丢弃掉。以一个单隐藏层的多层感知机为例子，其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（i=1,…,5）的计算表达式为:\n$$ h_i = \\phi\\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\\right), $$\n这里$\\phi$是激活函数，$x_1, \\ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \\ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$， 那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$:\n$$ h_i’ = \\frac{\\xi_i}{1-p} h_i. $$\n由于$E(\\xi_i) = 1-p$，因此:\n$$ E(h_i’) = \\frac{E(\\xi_i)}{1-p}h_i = h_i. $$\n即丢弃法不改变其输入的期望值。由于在训练中隐藏层神经元的丢弃是随机的,输出层的计算无法过度依赖隐藏层中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。\ndropout的python实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import d2lzh as d2l from mxnet import autograd, gluon, init, nd from mxnet.gluon import loss as gloss, nn def dropout(X, drop_prob): assert 0 \u003c= drop_prob \u003c= 1 keep_prob = 1 - drop_prob # 这种情况下把全部元素都丢弃 if keep_prob == 0: return X.zeros_like() mask = nd.random.uniform(0, 1, X.shape) \u003c keep_prob return mask * X / keep_prob num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256 W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1)) b1 = nd.zeros(num_hiddens1) W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2)) b2 = nd.zeros(num_hiddens2) W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs)) b3 = nd.zeros(num_outputs) params = [W1, b1, W2, b2, W3, b3] for param in params: param.attach_grad() drop_prob1, drop_prob2 = 0.2, 0.5 def net(X): X = X.reshape((-1, num_inputs)) H1 = (nd.dot(X, W1) + b1).relu() if autograd.is_training(): # 只在训练模型时使用丢弃法 H1 = dropout(H1, drop_prob1) # 在第一层全连接后添加丢弃层 H2 = (nd.dot(H1, W2) + b2).relu() if autograd.is_training(): H2 = dropout(H2, drop_prob2) # 在第二层全连接后添加丢弃层 return nd.dot(H2, W3) + b3 num_epochs, lr, batch_size = 5, 0.5, 256 loss = gloss.SoftmaxCrossEntropyLoss() train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr) 权重衰减和丢弃法应对过拟合的效果 本实验中利用softmax回归分类探究权重衰减和丢弃法应对过拟合的效果。在这里我们增加两个隐藏层。下面定义的模型将全连接层和激活函数ReLU串起来，并对每个激活函数的输出使用丢弃法。我们可以分别设置各个层的丢弃概率。通常的建议是把靠近输入层的丢弃概率设得小一点。损失函数增加$L_2$范数惩罚项。利用Gluon可以直接在构造Trainer实例时通过wd参数来指定权重衰减超参数。默认下，Gluon会对权重和偏差同时衰减。我们可以分别对权重和偏差构造Trainer实例，从而只对权重衰减。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 import d2lzh as d2l from mxnet import autograd, gluon, init, nd from mxnet.gluon import loss as gloss, nn from matplotlib import pyplot as plt drop_prob1, drop_prob2 = 0.2, 0.5 num_epochs, lr, batch_size = 20, 0.5, 256 lambd = 1e-6 def semilogy(train_loss, train_epoch, test_loss = None, test_epoch = None , title = None,num = None): plt.figure(num) plt.title(title) plt.xlabel('epoch') plt.ylabel('train_loss') plt.plot(train_epoch, train_loss) if test_epoch and test_loss: plt.plot(test_epoch, test_loss, linestyle=':') plt.pause(0.1) def l2_penalty(w): return (w**2).sum() / 2 def train(net, train_iter, test_iter, loss, num_epochs, batch_size, trainer_w=None , trainer_b=None): \"\"\"Train and evaluate a model with CPU.\"\"\" train_ls, test_ls = [], [] for epoch in range(num_epochs): train_l_sum, train_acc_sum, n = 0.0, 0.0, 0 for X, y in train_iter: with autograd.record(): y_hat = net(X) l = loss(y_hat, y).sum() l.backward() trainer_w.step(batch_size) trainer_b.step(batch_size) y = y.astype('float32') train_l_sum += l.asscalar() train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar() n += y.size train_ls.append(train_l_sum / n) test_l_sum ,n1 = 0.0 ,0.0 for X1,y1 in test_iter: y1_hat = net(X1) l1 = loss(y1_hat,y1).sum() test_l_sum += l1.asscalar() n1 += y1.size test_ls.append(test_l_sum / n1) test_acc = d2l.evaluate_accuracy(test_iter, net) print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f' % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc)) semilogy(train_ls,range(1, num_epochs + 1),test_ls,range(1, num_epochs + 1),\"loss\",1) plt.waitforbuttonpress() loss = gloss.SoftmaxCrossEntropyLoss() train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) net = nn.Sequential() net.add(nn.Dense(256, activation=\"relu\"), nn.Dropout(drop_prob1), # 在第一个全连接层后添加丢弃层 nn.Dense(256, activation=\"relu\"), nn.Dropout(drop_prob2), # 在第二个全连接层后添加丢弃层 nn.Dense(10)) net.initialize(init.Normal(sigma=0.01)) trainer_w = gluon.Trainer(net.collect_params('.*weight'), 'sgd', {'learning_rate': lr, 'wd': lambd}) trainer_b = gluon.Trainer(net.collect_params('.*bias'), 'sgd', {'learning_rate': lr}) train(net, train_iter, test_iter, loss, num_epochs, batch_size, trainer_w , trainer_b) 以下是实验改动： 1 . 原始情况如上面的代码，隐藏层使用丢弃法，不使用权重衰减，迭代周期为5. 结果：\n1 2 3 4 5 epoch 1, loss 1.1250, train acc 0.566, test acc 0.772 epoch 2, loss 0.6197, train acc 0.770, test acc 0.840 epoch 3, loss 0.4963, train acc 0.816, test acc 0.847 epoch 4, loss 0.4487, train acc 0.836, test acc 0.858 epoch 5, loss 0.4156, train acc 0.849, test acc 0.867 蓝色折线是train_ls,橙色折线是tesr_ls.\n2 . 把本节中的两个丢弃概率超参数对调.\n1 2 3 4 5 epoch 1, loss 1.1963, train acc 0.532, test acc 0.772 epoch 2, loss 0.6132, train acc 0.772, test acc 0.797 epoch 3, loss 0.5203, train acc 0.808, test acc 0.824 epoch 4, loss 0.4819, train acc 0.824, test acc 0.855 epoch 5, loss 0.4541, train acc 0.832, test acc 0.853 精度稍微下降，通常的建议是把靠近输入层的丢弃概率设得小一点。 3 . 增大迭代周期数，比较使用丢弃法与不使用丢弃法的结果. 使用丢弃法，迭代周期为20 不使用丢弃法，迭代周期为20 不使用dropout的训练loss更低，但测试loss更高。就是过拟合现象更加明显了。\n4 . 增加模型复杂度，比如增加一层隐藏层单,迭代周期为了明显期间，设为20，丢弃概率由低层到高层依次为：0.2,0.4,0.6.\n增加隐藏层的单元，比如每一层由256改为512，丢弃概率由低到高为：0.2,0.5.\n损失有所降低。\n5 . 不使用丢弃法，使用权重衰减，epoch为20.\n同时使用丢弃法和权重衰减，epoch为20.\n","title":"应对过拟合的一些策略","uri":"/contents/dl/tips-for-overfit/"},{"categories":["contents"],"content":"本文记录两视图的对极几何的概念和基本矩阵的概念，以及给定第一幅视图的像点$\\mathbf{x}$，它怎么约束第二幅视 图的对应点$\\mathbf{x}^{\\prime}$的位置的问题。\n概念 对极几何是两幅视图之间内在的射影几何，它独立于景物结构，只依赖于摄像机的内参数和相对姿态。对极几何可以由3×3矩阵$F$表示，称为基本矩阵，它是一个秩为2的矩阵，如果一个三维空间点$\\mathbf{X}$在第一，第二幅视图中的像点为$\\mathbf{x}$，$\\mathbf{x}^{\\prime}$，则这两个图像点满足关系$\\mathbf{x}^{\\prime T}F\\mathbf{x}=0$ 两幅视图之间的对极几何是图像平面与以基线（连接两摄像机中心的直线）为轴的平面束（极平面束）的交的几何。如上图，确定了两相机的光心$\\mathbf{c},\\mathbf{c}^{\\prime}$和像点$\\mathbf{x}$，由于空间点$\\mathbf{X}$在过$\\mathbf{c},\\mathbf{x}$的射线上，该射线被投影到第二幅视图中被影像成一条直线（极线）$\\mathbf{I}^{\\prime}$，空间点$\\mathbf{X}$对应的第二幅视图像点$\\mathbf{x}^{\\prime}$（极点）就落在直线$\\mathbf{I}^{\\prime}$上。用公式表示为： $$ \\mathbf{I}^{\\prime}=F \\mathbf{x} $$ $F$为基本矩阵。\n基本矩阵$F$的代数形式 F 表征着$\\mathbf{I}^{\\prime}$与$\\mathbf{x}$的关系。在摄像机矩阵$P$作用下，从$\\mathbf{x}$反向投影的射线通过解方程$P\\mathbf{X}=\\mathbf{x}$得到，其解的形式为： $$ \\mathbf{X}(\\lambda)=P^{+} \\mathbf{x}+\\lambda \\mathbf{C} $$ 其中$P^{+}$是$P$的伪逆，则$P P^{+}=I$，该射线上有两个特殊的点$P^{+} \\mathbf{x}$和$\\mathbf{C}$分别被映射到第二幅视图上的点$P^{\\prime} P^{+} \\mathbf{x}$和$P^{\\prime} \\mathbf{C}$上($P^{\\prime} $是第二幅视图的摄像机矩阵)，故$\\mathbf{I}^{\\prime}$可以表示为： $$ \\mathbf{I}^{\\prime}=\\left(P^{\\prime} \\mathbf{C}\\right) \\times\\left(P^{\\prime} P^{+} \\mathbf{x}\\right)=F\\mathbf{x} $$ 由于 $$ \\mathrm{e}^{\\prime} = \\left(P^{\\prime} \\mathbf{C}\\right) $$ 故\n$$ F=\\left[\\mathrm{e}^{\\prime}\\right]_{ \\times} P^{\\prime} P^{+} $$\n其中$\\left[\\mathrm{e}^{\\prime}\\right]_{ \\times}$表示$\\mathrm{e}^{\\prime}$的反对称矩阵。\n由$F$恢复摄像机矩阵 上面的$F$可以由摄像机矩阵$P$和$P^{\\prime} $唯一确定，但反过来并不成立。给定$F$仅能在射影变换意义下恢复摄像机矩阵。\n如果$H$是一个3维射影变换的一个4×4的矩阵，那么对应于摄像机矩阵$(P,P^{\\prime})$和$(PH,P^{\\prime}H)$的基本矩阵是相同的。\n摄像机矩阵的规范形式 对应于摄像机矩阵，对$P=[I | 0]$和$P^{\\prime}=[M | \\mathrm{m}]$的基本矩阵是$[\\mathbf{m}]_{ \\times} M$\n给定$F$后摄像机射影多义性 令$F$为基本矩阵而$\\left(P, P^{\\prime}\\right)$和$\\left(\\tilde{P}, \\tilde{P}^{\\prime}\\right)$都是于基本矩阵$F$对应的两组摄像机矩阵对，则存在一个非奇异矩阵$H$，使得$\\widehat{\\mathrm{P}}=\\mathrm{PH}$和$\\tilde{P}^{\\prime}=P^{\\prime} H$。令$P=\\widetilde{P}=[\\mathrm{I} | \\mathbf{0}]$，$P^{\\prime}=[A | \\mathbf{a}]$，$\\tilde{P}^{\\prime}=[\\tilde{A} | \\tilde{\\mathbf{a}}]$，此时$F$可以分解为两种不同的形式：\n$$ F=[\\mathrm{a}]_{ \\times}A $$\n$$ F=[\\tilde{\\mathrm{a}}]_{\\times} \\tilde{A} $$\n这两种形式的对应关系为：$\\widetilde{\\mathbf{a}}=k \\mathbf{a}$和$\\tilde{A}=k^{-1}\\left(A+\\mathbf{a}\\mathbf{v}^{T}\\right)$，其中$k$为非零常数，$\\mathbf{v}$是三维矢量。\n给定$F$求规范摄像机对 可以选择为两种形式：\n$\\mathrm{P}=[I| \\mathbf{0}]$ 和 $P^{\\prime}=\\left[\\left[\\mathrm{e}^{\\prime}\\right]_{ \\times} F | \\mathrm{e}^{\\prime}\\right]$\n$\\mathrm{P}=[I | \\mathbf{0}]$和 $P^{\\prime}=\\left[\\left[\\mathrm{e}^{\\prime}\\right]_{ \\times} F+\\mathrm{e}^{\\prime} \\mathbf{v}^{\\mathrm{T}} | \\lambda \\mathrm{e}^{\\prime}\\right]$\n公式的推导我直接放图了： ","title":"对极几何和基本矩阵F","uri":"/contents/multi-view-geometry/epipolar-geametry-and-f/"},{"categories":["contents"],"content":"记录模型欠拟合和过拟合的原因和实验现象。摘自《动手学深度学习》\n网络欠拟合和过拟合\n概念 在讨论欠拟合和过拟合之前首先需要区分几个概念：\n1 . 训练误差和泛化误差\n训练误差（training error）和泛化误差（generalization error）。前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。\n2 . 模型选择\n在机器学习中，通常需要评估若干候选模型的表现并从中选择模型。这一过程称为模型选择（model selection）。可供选择的候选模型可以是有着不同超参数的同类模型。以多层感知机为例，我们可以选择隐藏层的个数，以及每个隐藏层中隐藏单元个数和激活函数。\n3 . 验证数据集\n从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。但由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。然而在实际应用中，由于数据不容易获取，测试数据极少只使用一次就丢弃。一种改善的方法是$K$折交叉验证（$K$-fold cross-validation）。在$K$折交叉验证中，我们把原始训练数据集分割成$K$个不重合的子数据集，然后我们做$K$次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他$K−1$个子数据集来训练模型。在这$K$次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这$K$次训练误差和验证误差分别求平均。\n欠拟合和过拟合是什么 在模型训练中经常出现的两类典型问题：一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。\n虽然有很多因素可能导致这两种拟合问题，在主要两个因素是：模型复杂度和训练数据集大小。\n出现过拟合的情况：模型复杂度太高，训练数据集太小 出现欠拟合的情况：模型复杂度太低 说一个模型的复杂度高低是和训练数据集的数据特征以及数据规模有关的。如果用一个三阶多项式模型来拟合一个线性模型生成的数据，可以说模型复杂度太高了。在实验中发现，此时虽然网络仍然可以较好拟合出线性模型生成的数据的权重（将高阶权重拟合趋向于0），但此时网络对噪声更加敏感了。当训练数据集规模小于网络模型中的参数时候，这使模型显得过于复杂，以至于容易被训练数据中的噪声影响。而且网络倾向于记住数据的每个特征，这时候容易发生过拟合现象（对训练集的特征学的太好了，泛化能力低），此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。相对于欠拟合，我觉得过拟合更容易发生，因为容易设计高复杂度的网络，这时候网络性能瓶颈在于数据规模。 下面代码使用多项式拟合实验来测试模型复杂度和训练数据集大小对欠拟合和过拟合的影响，欠拟合是设计一个线性网络拟合一个多项式网络（模型复杂度太低），过拟合是让输入数据规模减小（训练数据集太小）。 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 from mxnet import autograd, gluon, nd from mxnet.gluon import data as gdata, loss as gloss ,nn from matplotlib import pyplot as plt n_train, n_test, true_w, true_b = 100, 100, [1.2, -3.4, 5.6], 5 features = nd.random.normal(shape=(n_train + n_test, 1)) poly_features = nd.concat(features, nd.power(features, 2), nd.power(features, 3)) labels = (true_w[0] * poly_features[:, 0] + true_w[1] * poly_features[:, 1] + true_w[2] * poly_features[:, 2] + true_b) labels += nd.random.normal(scale=0.1, shape=labels.shape) def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None, title = None,num = None): plt.figure(num) plt.title(title) plt.xlabel((x_label)) plt.ylabel((y_label)) plt.plot(x_vals, y_vals) if x2_vals and y2_vals: plt.plot(x2_vals, y2_vals, linestyle=':') plt.pause(0.1) num_epochs, loss = 100, gloss.L2Loss() def fit_and_plot(train_features, test_features, train_labels, test_labels,title,num): net = nn.Sequential() net.add(nn.Dense(1)) net.initialize() batch_size = min(10, train_labels.shape[0]) train_iter = gdata.DataLoader(gdata.ArrayDataset( train_features, train_labels), batch_size, shuffle=True) trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01}) train_ls, test_ls = [], [] for _ in range(num_epochs): for X, y in train_iter: with autograd.record(): l = loss(net(X), y) l.backward() trainer.step(batch_size) train_ls.append(loss(net(train_features), train_labels).mean().asscalar()) test_ls.append(loss(net(test_features), test_labels).mean().asscalar()) print('final epoch: train loss', train_ls[-1], 'test loss', test_ls[-1]) semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss', range(1, num_epochs + 1), test_ls, title,num) print('weight:', net[0].weight.data().asnumpy(), '\\nbias:', net[0].bias.data().asnumpy()) fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :],labels[:n_train], labels[n_train:],'fit',1) fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train],labels[n_train:],'underfit',2) fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2],labels[n_train:],'overfit',3) plt.waitforbuttonpress() 实验结果：\n拟合 欠拟合 过拟合 ","title":"模型欠拟合和过拟合","uri":"/contents/dl/underfitandoverfit/"},{"categories":["contents"],"content":" softmax回归其实是用来做分类的模型，和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。摘自《动手学深度学习》 softmax回归模型概念 softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。\n数学模型 $$ \\begin{aligned} o_1 \u0026= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\\\ o_2 \u0026= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\\\ o_3 \u0026= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3. \\end{aligned} $$\n若要输出是离散的预测，一个方法是取输出的最大值对应的类别作为预测输出（比如若输出是顺序排列的，则取输出最大值的下标做预测输出），则输出 $\\operatorname*{argmax}_i o_i$。但为了使输出层的输出具有直观意义（例如表示概率值），先对网络输出用softmax运算符做归一化，将网络输出值变换成值为正且和为1的概率分布：\n$$ \\hat{y}_1, \\hat{y}_2, \\hat{y}_3 = \\text{softmax}(o_1, o_2, o_3), $$\n其中\n$$ \\hat{y}_1 = \\frac{ \\exp(o_1)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad \\hat{y}_2 = \\frac{ \\exp(o_2)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad \\hat{y}_3 = \\frac{ \\exp(o_3)}{\\sum_{i=1}^3 \\exp(o_i)}. $$\n用矢量表示，定义：\n$$ \\boldsymbol{W} = \\begin{bmatrix} w_{11} \u0026 w_{12} \u0026 w_{13} \\\\ w_{21} \u0026 w_{22} \u0026 w_{23} \\\\ w_{31} \u0026 w_{32} \u0026 w_{33} \\\\ w_{41} \u0026 w_{42} \u0026 w_{43} \\end{bmatrix},\\quad \\boldsymbol{b} = \\begin{bmatrix} b_1 \u0026 b_2 \u0026 b_3 \\end{bmatrix}, $$ 样本i的特征为： $$ \\boldsymbol{x}^{(i)} = \\begin{bmatrix}x_1^{(i)} \u0026 x_2^{(i)} \u0026 x_3^{(i)} \u0026 x_4^{(i)}\\end{bmatrix}, $$ 输出层的输出为： $$ \\boldsymbol{o}^{(i)} = \\begin{bmatrix}o_1^{(i)} \u0026 o_2^{(i)} \u0026 o_3^{(i)}\\end{bmatrix}, $$ 最终的预测概率为： $$ \\boldsymbol{\\hat{y}}^{(i)} = \\begin{bmatrix}\\hat{y}_1^{(i)} \u0026 \\hat{y}_2^{(i)} \u0026 \\hat{y}_3^{(i)}\\end{bmatrix}. $$\n则softmax回归对样本$i$分类的矢量计算表达式为\n$$ \\begin{aligned} \\boldsymbol{o}^{(i)} \u0026= \\boldsymbol{x}^{(i)} \\boldsymbol{W} + \\boldsymbol{b},\\\\ \\boldsymbol{\\hat{y}}^{(i)} \u0026= \\text{softmax}(\\boldsymbol{o}^{(i)}). \\end{aligned} $$\n以上是单样本的矢量表示，若对小批量的矢量表示如下：\n给定一个小批量样本，其批量大小为$n$，输入个数（特征数）$d$，输出个数（类别数）为$q$。设批量特征为$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$。假设softmax回归的权重和偏差参数分别为$\\boldsymbol{W} \\in \\mathbb{R}^{d \\times q}$和$\\boldsymbol{b} \\in \\mathbb{R}^{1 \\times q}$。softmax回归的矢量计算表达式为\n$$ \\begin{aligned} \\boldsymbol{O} \u0026= \\boldsymbol{X} \\boldsymbol{W} + \\boldsymbol{b},\\\\ \\boldsymbol{\\hat{Y}} \u0026= \\text{softmax}(\\boldsymbol{O}), \\end{aligned} $$\n网络结构 softmax网络模型和线性回归一样，依然是单层全连接层网络，可具体看下面的代码。\n交叉熵损失函数 有了网络的输出$\\boldsymbol{\\hat{y}}^{(i)}$（是一组概率值）和真实标签$\\boldsymbol{y}^{(i)}$，利用交叉熵损失函数计算损失值$l$。\n$$ H(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)} ) = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)}, $$\n对于一个样本只有一个标签的情况，由于每个样本的标签中只有一个值为1，其他为0，上式可简化为\n$$ H(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}) = -\\log \\hat y_{y^{(i)}}^{(i)} $$\n假设训练数据集的样本数为$n$，交叉熵损失函数定义为 $$ \\ell(\\boldsymbol{\\Theta}) = \\frac{1}{n} \\sum_{i=1}^n H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ), $$\n优化算法 和线性回归一样，依然是小批量随机梯度下降，具体看下面的代码。\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import d2lzh as d2l from mxnet import autograd, nd from matplotlib import pyplot as plt batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) num_inputs = 784 num_outputs = 10 W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs)) b = nd.zeros(num_outputs) W.attach_grad() b.attach_grad() def softmax(X): X_exp = X.exp() partition = X_exp.sum(axis=1, keepdims=True) #对一行元素求和 return X_exp / partition # 这里应用了广播机制 def net(X): return softmax(nd.dot(X.reshape((-1, num_inputs)), W) + b) #n*784 def cross_entropy(y_hat, y): return -nd.pick(y_hat, y).log() def accuracy(y_hat, y): return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar() def evaluate_accuracy(data_iter, net): acc_sum, n = 0.0, 0 for X, y in data_iter: y = y.astype('float32') acc_sum += (net(X).argmax(axis=1) == y).sum().asscalar() #net(X).argmax(axis=1)返回一行中最大概率值对应的下标索引 n += y.size return acc_sum / n num_epochs, lr = 10, 0.1 # 本函数已保存在d2lzh包中方便以后使用 def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None): for epoch in range(num_epochs): train_l_sum, train_acc_sum, n = 0.0, 0.0, 0 for X, y in train_iter: with autograd.record(): y_hat = net(X) l = loss(y_hat, y).sum() l.backward() if trainer is None: d2l.sgd(params, lr, batch_size) else: trainer.step(batch_size) # “softmax回归的简洁实现”一节将用到 y = y.astype('float32') train_l_sum += l.asscalar() train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar() n += y.size test_acc = evaluate_accuracy(test_iter, net) print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f' % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc)) train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr) ","title":"Python实现softmax回归分类","uri":"/contents/dl/softmax/"},{"categories":["contents"],"content":"从简单的线性回归模型中可以看到构建一个监督学习网络的基本步骤。下文摘自《动手学深度学习》\n线性回归概念 线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。\n数学模型 $$ \\hat{y}=x_{1} w_{1}+x_{2} w_{2}+b $$\n其中$x_{1},x_{2}$是输入数据的特征，$w_{1},w_{2}$是特征的对应权重，$b$是偏置，这样就可以表示出一个简单的线性模型，用单层神经网络就可以实现线性模型。给定数据和标签，通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小，这个过程叫作模型训练（model training）。\n模型训练的三个要素 1 . 训练数据集 我们通常收集一系列的真实数据，我们希望在这个数据上面寻找模型参数来使预测值$\\hat{y}$与真实值（标签）$y$误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），训练集的每一个数据的每个维度值叫作特征（feature）。特征用来表征样本的特点。 2 . 损失函数 用来衡量预测值和真实值的误差，一个常用的选择是平方函数： $$ \\ell^{(i)}\\left(w_{1}, w_{2}, b\\right)=\\frac{1}{2}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2} $$ 在机器学习里，将衡量误差的函数称为损失函数（loss function）。这里使用的平方误差函数也称为平方损失（square loss）。\n通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量，即 $$ \\ell(w_1, w_2, b) =\\frac{1}{n} \\sum_{i=1}^n \\ell^{(i)}(w_1, w_2, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right)^2. $$ 在模型训练中，我们希望找出一组模型参数，记为$w_{1}^{*}, w_{2}^{*}, b^{*}$来使训练样本平均损失最小：\n$$ w_{1}^{*},w_{2}^{*}, b^{*}=\\underset{w_{1}, w_{2}, b}{\\operatorname{argmin}} \\ell\\left(w_{1}, w_{2}, b\\right) $$\n3 . 优化算法 当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。\n在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch），然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。\n在训练本节讨论的线性回归模型的过程中，模型的每个参数将作如下迭代：\n$$ \\begin{aligned} w_1 \u0026\\leftarrow w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b) }{\\partial w_1} = w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_1^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\\\ w_2 \u0026\\leftarrow w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b) }{\\partial w_2} = w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_2^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\\\ b \u0026\\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b) }{\\partial b} = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right). \\end{aligned} $$\n在上式中，$|\\mathcal{B}|$代表每个小批量中的样本个数（批量大小，batch size），$\\eta$称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。\n模型预测 模型训练完成后，我们将模型参数$w_1, w_2, b$在优化算法停止时的值分别记作$\\hat{w}_1, \\hat{w}_2, \\hat{b}$。注意，这里我们得到的并不一定是最小化损失函数的最优解$w_{1}^{*}, w_{2}^{*}, b^{*}$，而是对最优解的一个近似。得到的优化算法停止时的值用于估算训练集以外的数据的结果，这里的估算也叫作模型预测、模型推断或模型测试。\npython实现 训练一个线性回归模型的大致步骤为：\n数据预处理 处理输入数据集和标签的表示形式，训练的批大小\n定义网络模型 定义网络的结构，网络参数初始化等\n定义损失函数\n定义优化算法\n模型训练\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 from IPython import display from matplotlib import pyplot as plt from mxnet import autograd, nd import random num_inputs = 2 # 特征维度 num_examples = 1000 # 特征数 batch_size = 10 # 批大小 true_w = [2, -3.4] #参数w true_b = 4.2 #参数b features = nd.random.normal(scale=1, shape=(num_examples, num_inputs)) #【1000*2】的ndarray,服从高斯分布，标准差为1 labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b #标签 y labels += nd.random.normal(scale=0.01, shape=labels.shape) #标签加噪声 def use_svg_display(): # 用矢量图显示 display.set_matplotlib_formats('svg') def set_figsize(figsize=(3.5, 2.5)): use_svg_display() # 设置图的尺寸 plt.rcParams['figure.figsize'] = figsize # 本函数已保存在d2lzh包中方便以后使用 def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) # 样本的读取顺序是随机的 for i in range(0, num_examples, batch_size): #每批次都依次从feature中读取batch_size的数据x j = nd.array(indices[i: min(i + batch_size, num_examples)]) #若不能整除，要防止越界 yield features.take(j), labels.take(j) # take函数根据索引返回对应元素 w = nd.random.normal(scale=0.01, shape=(num_inputs, 1)) #初始化权重 b = nd.zeros(shape=(1,)) #初始化偏置 w.attach_grad() b.attach_grad() def linreg(X, w, b): # 本函数已保存在d2lzh包中方便以后使用 return nd.dot(X, w) + b #线性回归模型 def squared_loss(y_hat, y): # 本函数已保存在d2lzh包中方便以后使用 return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 #损失函数 def sgd(params, lr, batch_size): # 本函数已保存在d2lzh包中方便以后使用 for param in params: param[:] = param - lr * param.grad / batch_size #小批量梯度下降 lr = 0.03 #学习率 num_epochs = 3 #迭代次数 net = linreg loss = squared_loss for epoch in range(num_epochs): # 训练模型一共需要num_epochs个迭代周期 # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X # 和y分别是小批量样本的特征和标签 for X, y in data_iter(batch_size, features, labels): with autograd.record(): #记录梯度参数，默认是不记录的，所以要加上这句 l = loss(net(X, w, b), y) # l是有关小批量X和y的损失 l.backward() # 小批量的损失对模型参数求梯度，其实是 l.sum().backward() sgd([w, b], lr, batch_size) # 使用小批量随机梯度下降迭代模型参数 train_l = loss(net(features, w, b), labels) print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy())) print('trained weight :',w.asnumpy()) print('trained bias :',b.asscalar()) set_figsize() plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1); # 加分号只显示图 plt.show() ","title":"Python实现线性回归模型","uri":"/contents/dl/linear-regression/"},{"categories":["contents"],"content":"\r本节记录常量内存的用法，以及如何用时间函数测量CUDA应用的性能。\n概述 常量内存用于保存在核函数执行期间不会发生变化的数据，由于GPU的性能瓶颈通常不在于芯片的数学吞吐能力，而在于芯片的内存带宽，合理利用常量内存能有效减小内存的带宽的消耗。\n如何使用常量内存 声明常量内存的方法： 在声明的变量前加 constant 修饰符，如下： 1 __constant__ Sphere s[num] 此时不需要再用 cudaMalloc() 或者 cudaFree() 来申请或释放内存空间，编译器会自动为这个数组提交一个固定的大小。\n将主机内存复制到设备内存的函数 cudaMemcpy() 会将主机内存复制到全局内存，而 cudaMemcpyToSymbol() 会将主机内存复制到常量内存。\n常量内存为什么有效 对常量内存的单次操作可以广播到其他临近线程，范围为半个线程束（Wrap）。 常量内存的数据将缓存起来，因此对相同地址的连续读操作不会产生额外的内存通信量。 在CUDA架构中，线程束是指包含32个线程的集合，这个线程集合被“编织”在一起并且以“步调一致”的形式执行，在程序的每一行，线程束中的每个线程都在不同的数据中执行相同的操作。\n当这半个线程束读取常量内存相同地址时，才可以大幅度提升性能，否则，这半个线程束的请求会被串行化，在这个情况下性能反而会降低。\n使用事件来测量性能 要测量在核函数中执行的时间（包括核函数和设备内存的赋值操作），可以这样写：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cudaEvent_t start, stop; cudaEventCreate(\u0026start); cudaEventCreate(\u0026stop); cudaEventRecord(start, 0); ***************** 在GPU上执行一些工作（包括前后的设备内存复制） ***************** cudaEventRecord(stop, 0); cudaEventSynchronize(stop); float elapsedTime; cudaEventElapsedTime(\u0026elapsedTime,start, stop); printf(\"Time to generate: %3.1f ms\\n\", elapsedTime); cudaEventDestroy(start); cudaEventDestroy(stop); 不可以用该事件函数对包含主机函数和设备函数的混合代码一起计时！\n简单的光线追踪 下面的代码时利用常量内存实现的一个简单的光线追踪器。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 #include \"cuda.h\" #include \"book.h\" #include \"image.h\" #define DIM 1024 #define rnd( x ) (x * rand() / RAND_MAX) #define INF 2e10f struct Sphere { float r, b, g; float radius; float x, y, z; __device__ float hit(float ox, float oy, float *n) { float dx = ox - x; float dy = oy - y; if (dx*dx + dy*dy \u003c radius*radius) { float dz = sqrtf(radius*radius - dx*dx - dy*dy); //光线投射进球的深度 *n = dz / sqrtf(radius * radius); //归一化 return dz + z; } return -INF; } }; #define SPHERES 20 __constant__ Sphere s[SPHERES]; __global__ void kernel(unsigned char *ptr) { // map from threadIdx/BlockIdx to pixel position int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; float ox = (x - DIM / 2); float oy = (y - DIM / 2); float r = 0, g = 0, b = 0; float maxz = -INF; for (int i = 0; i\u003cSPHERES; i++) { float n; float t = s[i].hit(ox, oy, \u0026n); if (t \u003e maxz) { float fscale = n; r = s[i].r * fscale; g = s[i].g * fscale; b = s[i].b * fscale; maxz = t; } } ptr[offset * 4 + 0] = (int)(r * 255); ptr[offset * 4 + 1] = (int)(g * 255); ptr[offset * 4 + 2] = (int)(b * 255); ptr[offset * 4 + 3] = 255; } // globals needed by the update routine struct DataBlock { unsigned char *dev_bitmap; }; int main(void) { DataBlock data; // capture the start time cudaEvent_t start, stop; HANDLE_ERROR(cudaEventCreate(\u0026start)); HANDLE_ERROR(cudaEventCreate(\u0026stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); IMAGE bitmap(DIM, DIM); unsigned char *dev_bitmap; // allocate memory on the GPU for the output bitmap HANDLE_ERROR(cudaMalloc((void**)\u0026dev_bitmap, bitmap.image_size())); // allocate temp memory, initialize it, copy to constant // memory on the GPU, then free our temp memory Sphere *temp_s = (Sphere*)malloc(sizeof(Sphere) * SPHERES); for (int i = 0; i\u003cSPHERES; i++) { temp_s[i].r = rnd(1.0f); temp_s[i].g = rnd(1.0f); temp_s[i].b = rnd(1.0f); temp_s[i].x = rnd(1000.0f) - 500; temp_s[i].y = rnd(1000.0f) - 500; temp_s[i].z = rnd(1000.0f) - 500; temp_s[i].radius = rnd(100.0f) + 20; } HANDLE_ERROR(cudaMemcpyToSymbol(s, temp_s, sizeof(Sphere) * SPHERES)); free(temp_s); // generate a bitmap from our sphere data dim3 grids(DIM / 16, DIM / 16); dim3 threads(16, 16); kernel \u003c\u003c \u003cgrids, threads \u003e\u003e \u003e(dev_bitmap); // copy our bitmap back from the GPU for display HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost)); // get stop time, and display the timing results HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026elapsedTime, start, stop)); printf(\"Time to generate: %3.1f ms\\n\", elapsedTime); HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); HANDLE_ERROR(cudaFree(dev_bitmap)); // display bitmap.show_image(); } 效果如文章的开头。\n","title":"cuda学习笔记5","uri":"/contents/cuda/cuda-practice5/"},{"categories":["contents"],"content":"简介 trimmed-icp相较于传统的ICP算法，提升了对噪声的鲁棒性，同时允许两个点云集合具有较差的变换参数初始值。由于我只是在某程序中用到了trimmed-icp做点云匹配，没有研究它的代码实现和论文的实验细节部分，而且我的理解可能有很多错误的地方，原汁原味的论文在这里： 《Robust Euclidean alignment of 3D point sets: the trimmed iterative closest point algorithm》。\n在介绍trimmed-icp之前我想说说我对ICP算法的一些理解。\nICP（迭代最近点算法） 给定两个点云集合，两个集合中有一部分重合点，ICP的目的是求出这两个点云集合的最优的旋转矩阵 $R$ 和 平移矢量 $t$，很容易举个例子：比如用两个RGB-D相机对同一物体在不同角度下拍摄（当然要有重叠的部分），利用获取的信息生成点云后，我们想把同一时刻的两帧点云拼接在一起，得到比较完整的物体的三维模型，这时候就需要知道这两帧点云之间的变换矩阵，ICP可以帮助我们实现这个目标。\n算法概述 可以查看三维点云数据拼接中ICP及其改进算法综述获取更多的了解，下面是我从论文中摘取一些我觉得（对我来说）重要的部分。\nICP算法对待拼接的2片点云，首先根据一定的准则确立对应点集 $P$ 与 $Q$ ，其中对应点对的个数为 $n$ 。然后通过最小二乘法迭代计算最优的坐标变换 ，即旋转矩阵$R$和平移矢量$t$，使得误差函数\n$$ E(\\boldsymbol{R}, \\boldsymbol{t})=\\frac{1}{n} \\sum_{k=1}^{n}\\left|q_{k}-\\left(\\boldsymbol{R} p_{k}+\\boldsymbol{t}\\right)\\right|^{2} $$\n最小。\nICP算法计算简便直观且可使拼接具有较好的精度，但是算法的运行速度以及向全局最优的收敛性却在很大程度上依赖于给定的初始变换估计以及在迭代过程中对应关系的确立。各种粗拼接技术可为ICP算法提供较好的初始位置，所以迭代过程中确立正确对应点集以避免迭代陷入局部极值成为各种改进算法的关键 ，决定了算法的收敛速度与最终的拼接精度。\n算法流程 文中将ICP算法流程分为四部分：\n1 . 对原始点云数据进行采样\n常用的是均匀采样法，随机采样法，法矢采样法，其中第三种鲁棒性最好。\n2 . 确定初始对应点集\n确定对应点集的方法，点到点 (point—to—point)、点到投影(point—to-projection)和点到面(point—to—surfaee)。\n3 . 去除错误对应点对\n我粗略看了可以利用 RANSAC 去除错误点，关于RANSAC的介绍戳这里\n4 . 坐标变换的求解\nICP采用最小二乘法迭代求解出两个点云集的变换矩阵 $P$ 后，通过SVD分解，单位四元数等方法求出 $R$ 和 $t$ 来。\ntrimmed-icp trimmed-icp 和 传统ICP算法最大的不同，在于trim-icp对于由以上前三步得到的对应点，并不是直接采用最小二乘法来拟合误差函数，而是采用了一个叫LTS(the least trimmed squares)的方法来拟合误差函数，LTS的思路是，如果直接采用最小二乘法，选取的对应点集中如果存在少数异常值，最后拟合的误差函数受这些异常值影响较大，那为什么不把这些异常值排除在外呢？怎么做到不把这些异常值算进误差函数里面？trim-icp采用的LTS方法是对每组对应点求得的残差做一个升值排序，只截取前面比例为 $\\epsilon$ 的对应点拟合误差函数，然后通过迭代让误差函数最小求解 $R$ 和 $t$ 。在论文的细节部分给出了 $\\epsilon$ 的自适应方法，不过我没有看，想了解可以戳文章开始的论文链接。\n算法思路 定义：\n$$ \\mathbf{p}_{i}(\\mathbf{R}, \\mathbf{t})=\\mathbf{R} \\mathbf{p}_{i}+\\mathbf{t}, \\quad \\mathscr{P}(\\mathbf{R}, \\mathbf{t})={\\mathbf{p}_{i}(\\mathbf{R}, \\mathbf{t})}_{1}^{N_{p}} $$\n其中$\\mathbf{p}_{i}(\\mathbf{R},\\mathbf{t})$为点云集合$\\mathscr{P}$ 中某一点转换到点云集合$\\mathscr{U}$的对应点。$\\mathscr{P}(\\mathbf{R},\\mathbf{t})$ 是转换后的对应点的集合。需要为$\\mathscr{P}(\\mathbf{R},\\mathbf{t})$中每一点在$\\mathscr{U}$中找到对应点，对应点是这样找的：\n$$ \\mathbf{m}_{\\mathrm{cl}}(i, \\mathbf{R}, \\mathbf{t})=\\arg \\min _{\\mathbf{m} \\in \\mathscr{M}}\\left|\\mathbf{m}-\\mathbf{p}_{i}(\\mathbf{R}, \\mathbf{t})\\right| $$\n公式说的是距离最近的点为对应点，找到对应点就可以计算每一组对应点的残差了：\n$$ d_{i}(\\mathbf{R}, \\mathbf{t})=\\left|\\mathbf{m}_{\\mathrm{cl}}(i, \\mathbf{R}, \\mathbf{t})-\\mathbf{p}_{i}(\\mathbf{R}, \\mathbf{t})\\right| $$\nLTS做的是，对得到的 $d_{i}^{2}(\\mathbf{R}, \\mathbf{t})$ 做升值排序，然后截取 $N_{\\mathrm{po}}=\\xi N_{p}$ 个点求和得到误差函数，然后对优化误差函数，最终求解$R$和$t$。\n算法流程 1 . 为点集 $\\mathscr{P}$ 上每一个点找到它在点集 $\\mathscr{U}$ 上的对应点，计算其残差的平方 $d_{i}^{2}$。\n2 . 对$d_{i}^{2}$ 做升值排序，选择前面的 $N_{\\mathrm{po}}$ 点的对应值，求和得到 $S_{\\mathrm{TS}}$。\n3 . 如果满足终止条件，退出迭代，否则把$S_{\\mathrm{TS}}$设置为上一轮的对应点数目$S_{\\mathrm{TS}}^{\\prime}$，开始新一轮迭代。\n4 . 通过最小化$S_{\\mathrm{TS}}$计算一个最优的$(\\mathbf{R},\\mathbf{t})$。\n5 . 由得到的 $(\\mathbf{R},\\mathbf{t})$做对应点的转换，然后回到第1步。\n迭代的终止条件是：\n1 . 达到了设定的最大迭代次数。\n2 . trimmed MSE $e=\\frac{S_{\\mathrm{TS}}}{N_{\\mathrm{po}}}$足够小了。\n3 . trimmed MSE 的前后两次的变换量$\\left|e^{\\prime}-e\\right|$足够小了。\n完整的算法流程如上，这样trimmed-icp的大体思路就介绍完了，后面若有机会理解代码实现再补充细节。\n","title":"关于trimmed-icp的一些理解","uri":"/contents/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/trimmed-icp/"},{"categories":["contents"],"content":"概要 复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率和数据规模之间的增长关系，常见的复杂度有：$ O(1),O(logn),O(n),O(nlogn) , O(n^2) $。\n时间复杂度 大O符号 $$ T(n)=O(f(n)) $$\n表示所有代码的执行时间T(n)与每行代码的执行次数n成正比。大O时间复杂度实际上并不具体代表代码真正执行时间，而是表示代码执行时间随数据增长的变化趋势，也称为渐进时间复杂度，简称时间复杂度。\n时间复杂度分析技巧 1.只关心循环执行次数最多的一段代码\r2.加法法则：总复杂度等于量级最大的那段代码的复杂度（当量级不确定时不可简单这样计算）\r3.乘法法则：（嵌套代码的复杂度等于嵌套内外代码的复杂度的乘积）\r关于时间复杂度的更多内容 1.最好情况时间复杂度\n在最理想情况下，执行这段代码的时间复杂度。\n2.最坏情况时间复杂度\n在最坏情况下，执行这段代码的时间复杂度。\n3.平均情况时间复杂度\n把每种情况发生的概率考虑进去，对发生的所有情况的耗时做加权平均，得到的期望值，它的量级称为加权平均时间复杂度。\n以上三种时间复杂度，当在同一块代码中不同情况下，时间复杂度有量级的差距，才加以区分。\n4.均摊时间复杂度\n对于一个数据结构进行一组连续操作中，大部分情况下时间复杂度很低，个别情况下时间复杂度很高，而且这些操作之间存在前后连贯的时序关系，这时可以把这组操作放在一块分析，看能否把较高时间复杂度的操作的耗时均摊到其他的时间复杂度低的操作上。 ** 一般均摊时间复杂度等于最好情况时间复杂度 **。\n空间复杂度 空间复杂度全称是渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系。常见的空间复杂度有 $ O(1),O(n),O(n^2)$\n","title":"算法复杂度分析","uri":"/contents/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/suanfa1/"},{"categories":["contents"],"content":"本文主要介绍c++标准库中 vector 的基本用法。\n本文主要记录：\nvector 的初始化\n访问 vector 的元素\n迭代器\n概述 标准库类型 vector表示对象的集合，其中的所有对象的类型都相同， vector 是一个类模板而不是类型，需要提供额外的信息给编译器将 vector 实例化，比如：\n1 2 vector\u003cint\u003e ivec; vector\u003cvector\u003cstring\u003e\u003e file; vector 常用操作：\n1 2 3 4 5 6 7 8 9 v.empty() v.size() v.push_back(t) v[n] v1 = v2 ////拷贝 v1 = {a,b,c...} v1 == v2 ////当且仅当它们元素个数和元素值都相同 v1 != v2 \u003c ,V= ,\u003e ,\u003e= ////以字典顺序比较 一些要点 vector 初始化 可以对 vector 对象直接初始化；或者先定义为一个空的对象，在运行时再用 push_back 添加具体值。前一种情况适用于：\n初始值已知并且数量较少； 1 vector\u003cint\u003e ivec{1,2,3,4,5}; ////列表初始化 初始值是另一个 vector 对象的副本； 1 vector\u003cint\u003e ivec = ivec1; 所有元素的初始值都一样； 1 vector\u003cint\u003e ivec(10,1); ////初始化为10个1 除此之外，建议采用第二种方式初始化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // cpp_practice.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #include \u003cvector\u003e #include \u003cstring\u003e #include \u003ciostream\u003e using std::vector; using namespace std; int main() { vector\u003cstring\u003e int_str; string str; char cont = 'y'; while (cin \u003e\u003e str) { int_str.push_back(str); cout \u003c\u003c \"continue ?'y or n'\" \u003c\u003c endl; cin \u003e\u003e cont; if (cont == 'n' || cont == 'N') break; } for (auto i : int_str) { cout \u003c\u003c i \u003c\u003c endl; } return 0; } 访问 vector 的元素 利用 范围for 遍历 vector 的所有元素。 利用下标访问 vector 中已经存在的元素。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #include \"stdafx.h\" #include \u003cvector\u003e #include \u003cstring\u003e #include \u003ciostream\u003e using std::vector; using namespace std; int main() { vector\u003cint\u003e ivec; for (decltype(ivec.size())ix = 0; ix != 10; ix++) { ivec.push_back(ix); ////初始化为空的 vector 用 push_back 添加元素 } for (auto \u0026i : ivec) { i *= 2; ////范围for,需要用引用的格式修改其中的元素的值。 } for (decltype(ivec.size())ix = 0; ix != 10; ix++) { ivec[ix] += 1; ////利用下标访问已经存在的元素，显然易见，空vector不可使用下标方式访问 } for (auto i : ivec) { cout \u003c\u003c i \u003c\u003c endl; } return 0; } 迭代器 标准库中的所有容器和 string 都可以使用迭代器访问对象的元素，迭代器常用的成员：\n1 2 iter.begin(); ////返回指向对象第一个元素的迭代器 iter.end(); ////返回对象尾元素的下一个位置的迭代器，该位置为空，不可对该位置的元素进行解引。 利用迭代器遍历对象中的元素：\n1 2 3 4 5 6 string str{ \"some string\" }; for(auto s = str.begin(); s != str.end(); ++s) { *s = toupper(*s); } cout \u003c\u003c str \u003c\u003c endl; 若对象只需读操作而不需要写操作，可用 iter.cbegin() 和 iter.cend() 来访问对象中元素，这样返回的会是一个 const_iterator ，不论访问的对象本身是不是常量对象。\n但凡使用了迭代器的循环体，都不要向迭代器所属容器添加元素！\n比如不能用 push_back 向 vector添加元素，该操作会使迭代器失效。\n迭代器运算符:\n所有的迭代器都支持 == ， ！= 运算符， vector 和 string 支持更多的运算如下：\niter + n ////得到当前迭代器位置超前原来迭代器n个位置\riter - n\riter += n\riter -= n\riter1 - iter2 ////返回两个迭代器的距离，类型是一个带符号类型： difference_type\r\u003e, \u003c, \u003e=, \u003c= ////所在位置前后的比较，必须在同一个容器对象中比较才有意义。 ","title":"标准库类型vector笔记","uri":"/contents/c++/cpp-vector/"},{"categories":["contents"],"content":"本文主要介绍如何用GPU共享内存，线程块和线程的二维索引，线程同步的概念，并利用这些概念实现生成位图。\n本文概要 线程块和线程的二维索引表示\nGPU上进行点积运算\n生成位图\n线程块和线程的二维索引表示 同样地，和前文一维索引的调用方法相比，只需修改核函数的索引计算方法和核函数的调用方式即可。\n核函数的索引计算方法： 将： 1 int tid = threadIdx.x + blockIdx.x * blockDim.x; 改为：\n1 2 3 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int tid = x + y * blockDim.x * gridDim.x; 这样每一个 tid 索引 在二维表示中将指向唯一的线程，二维表示在处理图像上是常用的。\n核函数的调用方式： 将： 1 func_kernel \u003c\u003c \u003c(N + 127) / 128, 128 \u003e\u003e（参数） 改为：\n1 2 3 dim3 blocks(DIM/16,DIM/16); ////二维线程块 dim3 threads(16,16); ////二维线程 func_kernel\u003c\u003c\u003cblocks,threads\u003e\u003e\u003e（参数）； GPU上进行点积运算 将线程块分解为线程的目的，除了物理设备上线程块最大数目的限制，还有一个原因是 CUDA C支持共享内存。对于GPU上的每一个线程块，编译器都为该共享变量创建一个副本，而线程块中的每一个线程共享这块内存。由于共享内存驻留在物理GPU上而不是GPU之外的系统内存中，访问共享内存的延迟要远低于访问普通内缓存区的延迟。\n下面的例子是在GPU上实现点积运算，GPU中 每个线程块都完成以下的操作：\n首先要确定每个线程的起始位置和每次计算的偏移量，将计算的结果累加到该线程块中该线程对应的共享内存区域里。 每个线程完成计算后，执行同步操作，确保每个所有线程都已经执行完前面的操作。 规约，将每个线程块的共享内存区域的值相加，送给GPU的全局变量。 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 #include \"../common/book.h\" #define imin(a,b) (a\u003cb?a:b) const int N = 33 * 1024; const int threadsPerBlock = 256; const int blocksPerGrid = imin(32, (N + threadsPerBlock - 1) / threadsPerBlock); __global__ void dot(float *a, float *b, float *c) { __shared__ float cache[threadsPerBlock]; ////共享内存缓存区（驻留在物理GPU上），编译器为每一个线程块生成一个共享变量的副本。同一线程块中的每个线程共享这块内存。 int tid = threadIdx.x + blockIdx.x * blockDim.x; ////索引偏置 int cacheIndex = threadIdx.x; ////由于每一个线程块都具有一个共享内存的副本，故共享内存的索引就是该线程块上的线程索引。 float temp = 0; while (tid \u003c N) { temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } // set the cache values cache[cacheIndex] = temp; ////每个线程处理的数据，相加放在对应的共享内存区域中。 // synchronize threads in this block __syncthreads(); ////线程同步，当所有的线程都执行完以上操作时，才能继续执行下一步。 // for reductions, threadsPerBlock must be a power of 2 // because of the following code ////归约，将共享内存区域每一个储存的值相加起来，由于规约每次迭代数量减半，要求 threadsPerBlock 是2 的指数倍。 int i = blockDim.x / 2; while (i != 0) { if (cacheIndex \u003c i) cache[cacheIndex] += cache[cacheIndex + i]; __syncthreads(); ////线程同步。同样地，执行下一次规约迭代前，必须确保所有线程都已经执行完上面相加的操作。 i /= 2; } if (cacheIndex == 0) c[blockIdx.x] = cache[0]; ////把一个线程块中的最后计算得到的相加值返还给全局变量。 } int main(void) { float *a, *b, c, *partial_c; float *dev_a, *dev_b, *dev_partial_c; // allocate memory on the cpu side a = (float*)malloc(N * sizeof(float)); b = (float*)malloc(N * sizeof(float)); partial_c = (float*)malloc(blocksPerGrid * sizeof(float)); ////该变量的内存大小是线程块的数目* float内存大小。因为在GPU中，每个线程块最后返还一个相加值。 // allocate the memory on the GPU HANDLE_ERROR(cudaMalloc((void**)\u0026dev_a, N * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026dev_b, N * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026dev_partial_c, blocksPerGrid * sizeof(float))); // fill in the host memory with data for (int i = 0; i\u003cN; i++) { a[i] = i; b[i] = i * 2; } // copy the arrays 'a' and 'b' to the GPU HANDLE_ERROR(cudaMemcpy(dev_a, a, N * sizeof(float), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, N * sizeof(float), cudaMemcpyHostToDevice)); dot \u003c\u003c \u003cblocksPerGrid, threadsPerBlock \u003e\u003e \u003e(dev_a, dev_b, dev_partial_c); // copy the array 'c' back from the GPU to the CPU HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost)); // finish up on the CPU side c = 0; for (int i = 0; i\u003cblocksPerGrid; i++) { c += partial_c[i]; } ////cpu求点积 #define sum_squares(x) (x*(x+1)*(2*x+1)/6) printf(\"Does GPU value %.6g = %.6g?\\n\", c, 2 * sum_squares((float)(N - 1))); // free memory on the gpu side HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaFree(dev_b)); HANDLE_ERROR(cudaFree(dev_partial_c)); // free memory on the cpu side free(a); free(b); free(partial_c); } 值得注意的一点是，上面代码的线程同步 __syncthreads() 中，需确保每一个线程都执行该操作，否则任何线程都不能执行 __syncthreads() 后面的操作。比如：\n将：\n1 2 3 4 5 6 while (i != 0) { if (cacheIndex \u003c i) cache[cacheIndex] += cache[cacheIndex + i]; __syncthreads(); i /= 2; } 改为：\nwhile (i != 0) {\rif (cacheIndex \u003c i)\r{\rcache[cacheIndex] += cache[cacheIndex + i];\r__syncthreads();\r}\ri /= 2;\r} 后，代码将运行错误，因为不是所有线程都满足 “cacheIndex \u003c i” 该条件。\n生成位图 最后是一个利用线程块和线程的二维索引生成位图的例子：涉及共享内存，线程同步，二位索引的利用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 #include \"cuda.h\" #include \"../common/book.h\" #include \"../common/image.h\" #define DIM 1024 #define PI 3.1415926535897932f __global__ void kernel( unsigned char *ptr ) { // map from threadIdx/BlockIdx to pixel position int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; __shared__ float shared[16][16]; ////二维共享内存数组 // now calculate the value at that position const float period = 128.0f; shared[threadIdx.x][threadIdx.y] = 255 * (sinf(x*2.0f*PI/ period) + 1.0f) * (sinf(y*2.0f*PI/ period) + 1.0f) / 4.0f; // removing this syncthreads shows graphically what happens // when it doesn't exist. this is an example of why we need it. __syncthreads(); ////同步 ptr[offset*4 + 0] = 0; ptr[offset*4 + 1] = shared[15-threadIdx.x][15-threadIdx.y]; ptr[offset*4 + 2] = 0; ptr[offset*4 + 3] = 255; } // globals needed by the update routine struct DataBlock { unsigned char *dev_bitmap; }; int main( void ) { DataBlock data; IMAGE bitmap( DIM, DIM ); unsigned char *dev_bitmap; HANDLE_ERROR( cudaMalloc( (void**)\u0026dev_bitmap, bitmap.image_size() ) ); data.dev_bitmap = dev_bitmap; dim3 grids(DIM/16,DIM/16); dim3 threads(16,16); kernel\u003c\u003c\u003cgrids,threads\u003e\u003e\u003e( dev_bitmap ); HANDLE_ERROR( cudaMemcpy( bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost ) ); HANDLE_ERROR( cudaFree( dev_bitmap ) ); bitmap.show_image(); } ","title":"cuda学习笔记4","uri":"/contents/cuda/cuda-practice4/"},{"categories":["contents"],"content":"本文介绍如何在GPU上实现多线程块中开多个线程处理任务。\n单线程块里开启多个线程 在代码实现上，和开启多个线程块，每个线程块只有一个线程的区别：\n将 add«\u003cN,1»\u003e(dev_a,dev_b,dev_c) 改为 add«\u003c1,N»\u003e(dev_a,dev_b,dev_c) 。 容易看出 add«\u003cN,1»\u003e 的第一个参数是开启的线程块的数目，第二个参数是一个线程块里的线程数。\n将 int tid = blockIdx.x; 改为 int tid = threadIdx.x; 代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #include \"cuda_runtime.h\" #include \"device_launch_parameters.h\" #include \u003cstdio.h\u003e #include \u003cmath.h\u003e #include \u003chelper_cuda.h\u003e using namespace std; #define N 100 __global__ void add_kernel(double *a, double *b, double *c) { int tid = threadIdx.x; if (tid \u003c N) { c[tid] = a[tid] + b[tid]; } } __global__ void value_init_kernel(double *a, double *b) { int tid = threadIdx.x; if (tid \u003c N) { a[tid] = 1.0*tid; b[tid] = (1.0*tid*tid); } } int main(void) { cudaError_t err1 = cudaSuccess, err2 = cudaSuccess, err3 = cudaSuccess; double a[N], b[N], c[N]; double *dev_a, *dev_b, *dev_c; HANDLE_ERROR(cudaMalloc((void**)\u0026dev_a, N * sizeof(double))); HANDLE_ERROR(cudaMalloc((void**)\u0026dev_b, N * sizeof(double))); HANDLE_ERROR(cudaMalloc((void**)\u0026dev_c, N * sizeof(double))); value_init_kernel \u003c\u003c \u003c1, N \u003e\u003e \u003e (dev_a, dev_b);////在GPU上赋值操作 add_kernel \u003c\u003c \u003c1, N \u003e\u003e \u003e (dev_a, dev_b, dev_c);////在GPU上相加操作 HANDLE_ERROR(cudaMemcpy(a, dev_a, N * sizeof(double), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaMemcpy(b, dev_b, N * sizeof(double), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaMemcpy(c, dev_c, N * sizeof(double), cudaMemcpyDeviceToHost)); for (int i = 0; i \u003c N; i++) { printf(\"%f + %f = %f\\n\", a[i], b[i], c[i]); } ////释放GPU内存 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c); return 0; }\t多个线程块开启多个（固定数量的）线程 和上文第一种情况相比，依然只需要改变核函数的索引计算方法和核函数的调用方式即可：\n核函数的索引计算方法： 将：\n1 int tid = threadIdx.x; 改为：\n1 int tid = threadIdx.x + blockIdx.x * blockDim.x; 注意的是， blockDim 其实是一个三维的线程数组，但在例子中只用到一维线程块，故只需要用到 blockIdx.x ，该变量是一个常数，保存的是线程块中每一维的线程数量。索引 tid 的值显然为：当前线程块中的线程索引 + 每一个线程块中线程数（这里只有一维） * 当前线程块索引（从0开始）。\n核函数的调用方式： 将： 1 add_kernel \u003c\u003c \u003c1, N \u003e\u003e \u003e 改为：\n1 add_kernel \u003c\u003c \u003c(N + 127) / 128, 128 \u003e\u003e \u003e 由于 N 是一个整型，N+127/128 是为了向上取整（可多不可少）。\n在GPU上实现任意长度的矢量求和 由于单线程格里的线程块有一个最大值（比如65535），当GPU处理的矢量数目超过，在情况2中当 (N + 127) / 128 超过65535时，函数就会调用失败，这时候可以这样处理：将调用的线程块数目和每个线程块中调用线程的数目都固定，比如都固定为128。然后利用这个线程格中的 128 * 128 个线程顺序处理GPU上的所有任务。比如：索引号为1的线程处理完id为 1 的任务后会跳转到id为 1+ 128 * 128 的任务进行处理，故我们只需要知道如何计算每一个并行线程的初始索引，以及如何确定递增量的大小，就可完成GPU上所有任务的处理。\n和情况2相比：\n核函数的索引计算方法： 将： 1 2 3 4 5 int tid = threadIdx.x + blockIdx.x * blockDim.x; if (tid \u003c N) { c[tid] = a[tid] + b[tid]; } 改为：\n1 2 3 4 5 6 int tid = threadIdx.x + blockIdx.x * blockDim.x; if (tid \u003c N) { c[tid] = a[tid] + b[tid]; tid += blockDim.x * gridDim.x; ////blockDim.x * gridDim.x 表示当前线程格上的所有在运行的线程，即128*128 } 核函数的调用方式： 将： 1 add_kernel \u003c\u003c \u003c(N + 127) / 128, 128 \u003e\u003e \u003e 改为：\n1 add_kernel \u003c\u003c \u003c 128, 128 \u003e\u003e \u003e 即可。\n完整代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 #include \"cuda_runtime.h\" #include \"device_launch_parameters.h\" #include \u003cstdio.h\u003e #include \u003cmath.h\u003e #include \u003chelper_cuda.h\u003e using namespace std; #define N 1280 __global__ void add_kernel(double *a, double *b, double *c) { int tid = threadIdx.x + blockIdx.x * blockDim.x; if (tid \u003c N) { c[tid] = a[tid] + b[tid]; tid += blockDim.x * gridDim.x; } } __global__ void value_init_kernel(double *a, double *b) { int tid = threadIdx.x + blockIdx.x * blockDim.x; if (tid \u003c N) { a[tid] = 1.0*tid; b[tid] = (1.0*tid*tid); tid += blockDim.x * gridDim.x; } } int main(void) { cudaError_t err1 = cudaSuccess, err2 = cudaSuccess, err3 = cudaSuccess; double a[N], b[N], c[N]; double *dev_a, *dev_b, *dev_c; HANDLE_ERROR(cudaMalloc((void**)\u0026dev_a, N * sizeof(double))); HANDLE_ERROR(cudaMalloc((void**)\u0026dev_b, N * sizeof(double))); HANDLE_ERROR(cudaMalloc((void**)\u0026dev_c, N * sizeof(double))); value_init_kernel \u003c\u003c \u003c128, 128 \u003e\u003e \u003e (dev_a, dev_b);////在GPU上赋值操作 add_kernel \u003c\u003c \u003c 128, 128 \u003e\u003e \u003e (dev_a, dev_b, dev_c);////在GPU上相加操作 HANDLE_ERROR(cudaMemcpy(a, dev_a, N * sizeof(double), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaMemcpy(b, dev_b, N * sizeof(double), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaMemcpy(c, dev_c, N * sizeof(double), cudaMemcpyDeviceToHost)); for (int i = 0; i \u003c N; i++) { printf(\"%f + %f = %f\\n\", a[i], b[i], c[i]); } ////释放GPU内存 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c); return 0; } ","title":"cuda学习笔记3","uri":"/contents/cuda/cuda-practice3/"},{"categories":["contents"],"content":"本文主要介绍如何用cuda进行并行计算，还有一个有趣的实验：生成julia分形图。\n本节要点 CUDA实现并行性 一种错误处理的宏定义 基于GPU的矢量求和 看代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #include \"cuda_runtime.h\" #include \"device_launch_parameters.h\" #include \u003cstdio.h\u003e #include \u003cmath.h\u003e #include \u003chelper_cuda.h\u003e using namespace std; #define N 100 __global__ void add_kernel(double *a, double *b, double *c) { int tid = blockIdx.x; if (tid \u003c N) { c[tid] = a[tid] + b[tid]; } } __global__ void value_init_kernel(double *a, double *b) { int tid = blockIdx.x; if (tid \u003c N) { a[tid] = 1.0*tid; b[tid] = (1.0*tid*tid); } } int main(void) { cudaError_t err1 = cudaSuccess, err2 = cudaSuccess, err3 = cudaSuccess; double a[N], b[N], c[N]; double *dev_a, *dev_b, *dev_c; err1 = cudaMalloc((void**)\u0026dev_a, N * sizeof(double)); err2 = cudaMalloc((void**)\u0026dev_b, N * sizeof(double)); err3 = cudaMalloc((void**)\u0026dev_c, N * sizeof(double)); if (err1 != cudaSuccess || err2 != cudaSuccess || err3 != cudaSuccess) { fprintf(stderr, \"Failed to allocate device value (error code (%s,%s,%s))!\\n\", cudaGetErrorString(err1), cudaGetErrorString(err2), cudaGetErrorString(err3)); exit(EXIT_FAILURE); } value_init_kernel \u003c\u003c\u003cN, 1 \u003e\u003e\u003e (dev_a, dev_b);////在GPU上赋值操作 add_kernel \u003c\u003c\u003cN, 1 \u003e\u003e\u003e (dev_a, dev_b, dev_c);////在GPU上相加操作 err1 = cudaMemcpy(a, dev_a, N * sizeof(double), cudaMemcpyDeviceToHost); err2 = cudaMemcpy(b, dev_b, N * sizeof(double), cudaMemcpyDeviceToHost); err3 = cudaMemcpy(c, dev_c, N * sizeof(double), cudaMemcpyDeviceToHost); if (err1 != cudaSuccess || err2 != cudaSuccess || err3 != cudaSuccess) { fprintf(stderr, \"Failed to copy device value to host value (error code (%s,%s,%s))!\\n\", cudaGetErrorString(err1), cudaGetErrorString(err2), cudaGetErrorString(err3)); exit(EXIT_FAILURE); } for (int i = 0; i \u003c N; i++) { printf(\"%f + %f = %f\\n\", a[i], b[i], c[i]); } ////释放GPU内存 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c); return 0;\t} 代码实现在GPU上对变量赋值，然后相加返回给主机函数，上面每次对错误处理的代码太冗长了，可以用一个宏定义来简化：\n1 2 3 4 5 6 7 8 9 10 static void HandleError( cudaError_t err, const char *file, int line ) { if (err != cudaSuccess) { printf( \"%s in %s at line %d\\n\", cudaGetErrorString( err ), file, line ); exit( EXIT_FAILURE ); } } #define HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ )) julia分形图案 最后是一个有趣的例子：Julia分形图案：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 #include \u003cstdio.h\u003e #include \"cuda_runtime.h\" #include \"device_launch_parameters.h\" #include \"../common/book.h\" ////GPU高性能编程CUDA实战代码 #include \"../common/image.h\" #include \"book.h\" #include \"image.h\" #define DIM1 5760 #define DIM2 5760 //每一维度的长度 #define iter_N 200 struct cuComplex { float r; float i; __device__ cuComplex(float a, float b) :r(a), i(b) {} __device__ float magnitude2(void) { return r*r + i*i; } ////返回复数的模的平方 __device__ cuComplex operator*(const cuComplex\u0026 a) { return cuComplex(r*a.r - i*a.i, i*a.r + r*a.i); } __device__ cuComplex operator+(const cuComplex\u0026 a) { return cuComplex(r + a.r, i + a.i); } }; __device__ int julia(int x, int y) { const float scale = 1.5; float jx = scale * (float)(DIM1 / 2 - x) / (DIM1/ 2); float jy = scale * (float)(DIM2 / 2 - y) / (DIM2 / 2); cuComplex c(-0.8, 0.156); //-0.8,0.156; cuComplex a(jx, jy); for (int i = 1; i \u003c iter_N; i++) { a = a * a + c; if (a.magnitude2() \u003e 1000) return i; } return 0; } __global__ void kernel(unsigned char *ptr) { int x = blockIdx.x; int y = blockIdx.y; int offset = x + y * gridDim.x; int juliaValue = julia(x, y); ////美工部分。。。。。。 if (juliaValue ==0) { ptr[offset * 4 + 0] = 0; ptr[offset * 4 + 1] = 0; ptr[offset * 4 + 2] = 0; ptr[offset * 4 + 3] = 255; } if (juliaValue \u003c 90 \u0026\u0026 juliaValue \u003e= 1) { ptr[offset * 4 + 0] = (int)(255 * juliaValue / (2.0 * iter_N)); ptr[offset * 4 + 1] = 0; ptr[offset * 4 + 2] = 0; ptr[offset * 4 + 3] = 255; } if (juliaValue \u003c 120 \u0026\u0026 juliaValue \u003e=90) { ptr[offset * 4 + 0] = 255; ptr[offset * 4 + 1] = 255 - (int)(255 * juliaValue / (5.0 * iter_N)); ptr[offset * 4 + 2] = 255 - (int)(255 * juliaValue / (5.0 * iter_N)); ptr[offset * 4 + 3] = 255; } if (juliaValue \u003c 180 \u0026\u0026 juliaValue \u003e=120) { ptr[offset * 4 + 0] = 10; ptr[offset * 4 + 1] = 215; ptr[offset * 4 + 2] = 200; ptr[offset * 4 + 3] = 255; } if (juliaValue \u003c= 255 \u0026\u0026 juliaValue \u003e=180) { ptr[offset * 4 + 0] = (int)(255 * juliaValue / (1.0 * iter_N)); ptr[offset * 4 + 1] = 0; ptr[offset * 4 + 2] = 0; ptr[offset * 4 + 3] = 255; } } struct DataBlock { unsigned char *dev_bitmap; }; int main(void) { DataBlock data; IMAGE bitmap(DIM1, DIM2); unsigned char *dev_bitmap; HANDLE_ERROR(cudaMalloc((void**)\u0026dev_bitmap, bitmap.image_size())); data.dev_bitmap = dev_bitmap; dim3 grid(DIM1, DIM2); ////实际上是DIM1*DIM2*1的三维线程格 kernel \u003c\u003c \u003cgrid, 1 \u003e\u003e \u003e (dev_bitmap); HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaFree(dev_bitmap)); imwrite(\"C:/Users/Lenovo/Pictures/image/julia.png\", bitmap.image); bitmap.show_image(); } ","title":"cuda学习笔记2","uri":"/contents/cuda/cuda-practice2/"},{"categories":["contents"],"content":"本文主要介绍为主机编写代码和为设备编写代码的区别及细节。\n本节目标 为主机（Host）编写代码和为设备（Device）编写代码的区别 从主机运行设备代码 如何在支持CUDA的设备上使用设备内存 如何查询系统中支持CUDA的设备信息 要点 主机代码和设备代码 在GPU设备上执行的函数称为核函数，NVIDIA工具使用CUDA C编写核函数，CUDA C为标准C增加 global 修饰符，这个修饰符将告诉编译器，函数应该编译为设备而不是主机运行。具有该修饰符的函数将作为核函数交给CUDA编译器编译。\n从主机运行设备代码 如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 cudaError_t err = cudaSuccess; ////参数传递 int c; int *dev_c; err = cudaMalloc((void**)\u0026dev_c, sizeof(int)); add_kernel \u003c\u003c \u003c1, 1 \u003e\u003e \u003e (2, 7, dev_c); ////核函数处理 if (err != cudaSuccess) { fprintf(stderr, \"Failed to allocate device value dev_c (error code %s)!\\n\", cudaGetErrorString(err)); exit(EXIT_FAILURE); } err = cudaMemcpy(\u0026c, dev_c, sizeof(int), cudaMemcpyDeviceToHost); ////主机代码中，不能对cudaMalloc()返回的指针进行读取，写入内存操作（即不可修改），但可以作参数传递，执行算术运算。 if (err != cudaSuccess) { fprintf(stderr, \"Failed to copy device value dev_c to c (error code %s)!\\n\", cudaGetErrorString(err)); exit(EXIT_FAILURE); } printf(\"2+7=%d\\n\", c); cudaFree(dev_c); ////不可使用free()，因为在主机代码中不可修改该内存的值 ， 总的来说，主机指针只能访问主机代码的内存，设备指针只能访问设备代码的内存 从主机运行设备代码，以下几点值得注意：\n1.可以像调用C函数那样将参数传递给核函数。 2.当设备执行任何有用操作时，都需要分配内存，例如将计算值返回主机。\n使用设备内存 1.关于cudaMalloc()分配到的指针dev_c：\n可以将dev_c传递给设备上执行的函数。可以在设备代码中使用dev_c进行内存读/写操作（修改该内存的值）。可以将dev_c传递给主机上执行的函数，但是不能在主机代码中对dev_c进行内存读/写操作。因为dev_c申请的是设备内存，不能在主机代码中用主机指针访问设备内存，反过来也一样，不能用设备指针访问主机内存。\n2.利用cudaMencpy()在主机代码中访问设备内存，设备指针用完后用cudaFree()释放，不可以直接Free(),原因同上，不能在主机代码中直接访问设备内存。\n查询系统中支持CUDA的设备信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ////查询设备 cudaDeviceProp prop; int count; err = cudaGetDeviceCount(\u0026count); for (int i = 0; i \u003c count; i++) { err = cudaGetDeviceProperties(\u0026prop, i); if (err != cudaSuccess) { fprintf(stderr, \"Failed to get device %d properties (error code %s)!\\n\", i, cudaGetErrorString(err)); exit(EXIT_FAILURE); } printf(\"Name: %s\\n\", prop.name);////这里打印设备信息，具体包含哪些信息查看《NVIDIA CUDA Programming Guide》\tprintf(\"Compute capability: %d . %d\\n\", prop.major,prop.minor); printf(\"Total global Mem:%lld\\n\", prop.totalGlobalMem); printf(\"Total constant Mem:%ld\\n\", prop.totalConstMem); } 完整代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #include \"cuda_runtime.h\" #include \"device_launch_parameters.h\" #include \u003cstdio.h\u003e #include \u003chelper_cuda.h\u003e using namespace std; __global__ void add_kernel(int a,int b, int *c) { *c = a + b; } int main(void) { cudaError_t err = cudaSuccess; ////参数传递 int c; int *dev_c; err = cudaMalloc((void**)\u0026dev_c, sizeof(int)); add_kernel \u003c\u003c \u003c1, 1 \u003e\u003e \u003e (2, 7, dev_c); ////核函数处理 if (err != cudaSuccess) { fprintf(stderr, \"Failed to allocate device value dev_c (error code %s)!\\n\", cudaGetErrorString(err)); exit(EXIT_FAILURE); } err = cudaMemcpy(\u0026c, dev_c, sizeof(int), cudaMemcpyDeviceToHost); ////主机代码中，不能对cudaMalloc()返回的指针进行读取，写入内存操作（即不可修改），但可以作参数传递，执行算术运算。 if (err != cudaSuccess) { fprintf(stderr, \"Failed to copy device value dev_c to c (error code %s)!\\n\", cudaGetErrorString(err)); exit(EXIT_FAILURE); } printf(\"2+7=%d\\n\", c); cudaFree(dev_c); ////不可使用free()，因为在主机代码中不可修改该内存的值 ， 总的来说，主机指针只能访问主机代码的内存，设备指针只能访问设备代码的内存 ////查询设备 cudaDeviceProp prop; int count; err = cudaGetDeviceCount(\u0026count); for (int i = 0; i \u003c count; i++) { err = cudaGetDeviceProperties(\u0026prop, i); if (err != cudaSuccess) { fprintf(stderr, \"Failed to get device %d properties (error code %s)!\\n\", i, cudaGetErrorString(err)); exit(EXIT_FAILURE); } printf(\"Name: %s\\n\", prop.name);////这里打印设备信息，具体包含哪些信息查看《NVIDIA CUDA Programming Guide》\tprintf(\"Compute capability: %d . %d\\n\", prop.major,prop.minor); printf(\"Total global Mem:%lld\\n\", prop.totalGlobalMem); printf(\"Total constant Mem:%ld\\n\", prop.totalConstMem); } return 0;\t} ","title":"cuda学习笔记1","uri":"/contents/cuda/cuda-practice1/"},{"categories":["contents"],"content":"本文主要介绍c++标准库中 string 的基本用法。\n标准库类型 string表示可变长字符序列，其定义在命名空间std中，使用时记得包含头文件：\n1 # include \u003cstring\u003e string常用的操作 1 2 3 4 5 6 7 8 9 10 11 os\u003c\u003cs ////将s写入到输出流os中，返回os is\u003e\u003es ////从is中读取字符串到s中，字符串用空白（空格符，换行，制表符）隔开，返回is getline(is,s) ////从is中读取一行赋给s，返回is s.empty() ////判断s是否为空 s.size() ////返回s的字符个数 s[n] ////返回s中第n+1个字符的引用，使用时需要判断s是否为空！ s1+s2 ////连接 s1=s2 ////用s2的副本代替掉s1 s1==s2 ////等性判断，对大小写敏感 s1!=s2 ////等性判断，对大小写敏感 \u003c,\u003c=,\u003e,\u003e= ////比较字符串大小关系，对大小写敏感 几个要点 cin»s和getline(cin,s) 输入输出流最常用的还是标准库中的iostream，这里想说cin»s和 getline(cin,s) 的区别：\ncin»s 会忽略掉输入开头的空白，从第一个真正字符开始读起，直到遇见下一个空白为止（该空白没被读入）。 getline(cin,s)从给定输入流开始读取，直至遇到换行符结束，注意换行符也被读入，然后将读取的所有内容（除了最后的换行符外）放到s中。（s不包含换行符）。 s.size() 的返回类型 s.size() 是一个string::size_type类型，把它简单理解为一个无符号整形就好，但特殊的是该类型能够存放任何 string 对象的大小（理解为一个很大的值，手动狗头…）\nauto和 decltype c++11标准中允许用auto和decltype推断一个变量的类型。使用auto必须初始化变量，比如：\n1 2 auto s; ////错误，s没初始化，这让编译器怎么推断... auto s = \"hello\"; ////正确，编译器推断\"hello\"的类型，并认为也是s的类型。 decltype可用于不初始化一个变量这种情况，这样用：\n1 decltype(s.size()) s1; 其实是编译器从 s.size() 中推断出一个类型，并认为s1也是该类型。\n处理string对象中的字符 遍历处理对象中的字符串 c++11提供的范围for语句很好用，如下：\n1 2 3 4 5 6 7 string s(\"hello world!\"); for(auto \u0026c : s) { c = toupper(c); } cout\u003c\u003c s \u003c\u003cendl; 这里值得注意的是：若要改变string对象中字符的值，必须把循环变量定义成引用类型。\n只处理一部分字符 可使用下标或者使用迭代器访问单个字符。\n使用下标：下面的例子是将输入的一行字符串中第二个单词全部大写，最后输出。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #include \"stdafx.h\" #include \u003ciostream\u003e #include\u003cstring\u003e using namespace std; ////利用下标运算符处理特定位置的部分字符串 int main() { string line1; getline(cin, line1); /////获取输入第一行 if (!line1.empty() \u0026\u0026 !isspace(line1[0])) { int word_count = 0; for (decltype(line1.size())index = 0; index != line1.size(); index++) { if (isspace(line1[index])) word_count++; if (word_count == 1) ////处理第二个单词 { line1[index] = toupper(line1[index]); ////将第二个单词大写 } } cout \u003c\u003c line1 \u003c\u003c endl; } else cout \u003c\u003c \"input error\" \u003c\u003c endl; return 0; } 若要用下标的方式访问字符，一定要先检查字符串是否为空，否则容易出现不可预知的结果。\n使用迭代器 1 2 3 4 5 6 string str{ \"some string\" }; for(auto s = str.begin(); s != str.end(); ++s) { *s = toupper(*s); } cout \u003c\u003c str \u003c\u003c endl; ","title":"标准库类型string笔记","uri":"/contents/c++/cpp-string/"}]
