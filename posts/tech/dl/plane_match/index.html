<!DOCTYPE html>
<html lang="zh-CN">
    <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
    <meta charset="UTF-8" />

    <meta name="generator" content="Hugo 0.109.0"><meta name="theme-color" content="#fff" />
    <meta name="color-scheme" content="light dark">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <meta name="format-detection" content="telephone=no, date=no, address=no, email=no" />
    
    <meta http-equiv="Cache-Control" content="no-transform" />
    
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <title>基于图匹配的平面匹配算法 | W</title>

    <link rel="stylesheet" href="/css/meme.min.152b147db3841e9efa3de2d4fff9f3d2bc5f0fceb27c241950179b56506f9b15.css"/>

    
    
        <script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js" defer></script><script src="/js/meme.min.0dcc2c623a04ff92ce77e8931cceb9c80f52b8b6b72812f3cf8010bdba7e9dce.js"></script>

    

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />

        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM&#43;Plex&#43;Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;family=Comfortaa:wght@700&amp;display=swap" media="print" onload="this.media='all'" />
        <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM&#43;Plex&#43;Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;family=Comfortaa:wght@700&amp;display=swap" /></noscript>

    <meta name="author" content="JiaJie" /><meta name="description" content="在人类感知的尺度上，平面是人类社会环境中最常见的结构之一，并且具有强大的约束能力，约束着大量的点/线及其所携带的信息。各种曲面都可以用平面进行近似，根据精度要求选择拟合的平面数量。实际应用中，许多计算机视觉任务都需要平面信息，比如：机器人领域中，识别地面、墙面等平面可用于路径规划、视觉导航，识别桌面、书架等平面可辅助机械手抓取和放置物品；增强现实、混合现实中，利用平面信息放置物品，或者更换桌面、地板、墙面的纹理可以进行快速展示；三维场景重建中，用平面而非点云可以实现对一个城市大规模、简洁的重建。
" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2a6df4" />
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-title" content="W" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="mobile-web-app-capable" content="yes" />
    <meta name="application-name" content="W" />
    <meta name="msapplication-starturl" content="../../../../" />
    <meta name="msapplication-TileColor" content="#fff" />
    <meta name="msapplication-TileImage" content="../../../../icons/mstile-150x150.png" />
    <link rel="manifest" href="/manifest.json" />

    
    

    
    <link rel="canonical" href="https://wjiajie.github.io/posts/tech/dl/plane_match/" />
    

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "datePublished": "2021-01-12T15:18:48+08:00",
        "dateModified": "2021-02-05T05:05:18+08:00",
        "url": "https://wjiajie.github.io/posts/tech/dl/plane_match/",
        "headline": "基于图匹配的平面匹配算法",
        "description": "在人类感知的尺度上，平面是人类社会环境中最常见的结构之一，并且具有强大的约束能力，约束着大量的点/线及其所携带的信息。各种曲面都可以用平面进行近似，根据精度要求选择拟合的平面数量。实际应用中，许多计算机视觉任务都需要平面信息，比如：机器人领域中，识别地面、墙面等平面可用于路径规划、视觉导航，识别桌面、书架等平面可辅助机械手抓取和放置物品；增强现实、混合现实中，利用平面信息放置物品，或者更换桌面、地板、墙面的纹理可以进行快速展示；三维场景重建中，用平面而非点云可以实现对一个城市大规模、简洁的重建。\n",
        "inLanguage" : "zh-CN",
        "articleSection": "posts",
        "wordCount":  4303 ,
        "image": ["https://i.loli.net/2021/01/12/1F89yXeTJ3K2nHW.png","https://i.loli.net/2021/01/30/JypZtAfBxYrG1cQ.png","https://i.loli.net/2021/02/01/BNzyh7UIxHDj14o.jpg","https://i.loli.net/2021/02/01/6lM3jFnu4aCWpIJ.jpg","https://i.loli.net/2021/01/30/utPzsFLSaUqGrmv.png","https://i.loli.net/2021/02/01/KNvgIDfE56c7AHU.png","https://i.loli.net/2021/02/01/Y79CliJdkZu2bc1.png","https://i.loli.net/2021/02/01/OswHpJDXMcEqe7Z.png","https://i.loli.net/2021/02/01/6NQidJBqVz1mGFv.png"],
        "author": {
            "@type": "Person",
            "description": "静心得意",
            "email": "3208920286@qq.com",
            "image": "https://images.cnblogs.com/cnblogs_com/jiajiewu/1756037/o_200502040806760242504.jpg",
            "url": "https://wjiajie.github.io/",
            "name": "JiaJie"
        },
        "license": "[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)",
        "publisher": {
            "@type": "Organization",
            "name": "W",
            "logo": {
                "@type": "ImageObject",
                "url": "https://wjiajie.github.io/icons/apple-touch-icon.png"
            },
            "url": "https://wjiajie.github.io/"
        },
        "mainEntityOfPage": {
            "@type": "WebSite",
            "@id": "https://wjiajie.github.io/"
        }
    }
</script>

    

<meta name="twitter:card" content="summary_large_image" />



    



<meta property="og:title" content="基于图匹配的平面匹配算法" />
<meta property="og:description" content="在人类感知的尺度上，平面是人类社会环境中最常见的结构之一，并且具有强大的约束能力，约束着大量的点/线及其所携带的信息。各种曲面都可以用平面进行近似，根据精度要求选择拟合的平面数量。实际应用中，许多计算机视觉任务都需要平面信息，比如：机器人领域中，识别地面、墙面等平面可用于路径规划、视觉导航，识别桌面、书架等平面可辅助机械手抓取和放置物品；增强现实、混合现实中，利用平面信息放置物品，或者更换桌面、地板、墙面的纹理可以进行快速展示；三维场景重建中，用平面而非点云可以实现对一个城市大规模、简洁的重建。
" />
<meta property="og:url" content="https://wjiajie.github.io/posts/tech/dl/plane_match/" />
<meta property="og:site_name" content="W" />
<meta property="og:locale" content="zh-cn" /><meta property="og:image" content="https://i.loli.net/2021/01/12/1F89yXeTJ3K2nHW.png" />
<meta property="og:type" content="article" />
    <meta property="article:published_time" content="2021-01-12T15:18:48&#43;08:00" />
    <meta property="article:modified_time" content="2021-02-05T05:05:18&#43;08:00" />
    
    <meta property="article:section" content="posts" />



    
    

    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css">
<script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>



<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@700&amp;text=reuixiy&amp;display=swap" media="print" onload="this.media='all'" />
<noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@700&amp;text=reuixiy&amp;display=swap" /></noscript>





</head>

    <body>
        <div class="container">
            
    <header class="header">
        
            <div class="header-wrapper">
                <div class="header-inner single">
                    
    <div class="site-brand">
        
            <a href="/" class="brand">W</a>
        
    </div>

                    <nav class="nav">
    <ul class="menu" id="menu">
        
            
        
        
        
        
            
                <li class="menu-item"><a href="/posts/tech/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon wpexplorer"><path d="M512 256c0 141.2-114.7 256-256 256C114.8 512 0 397.3 0 256S114.7 0 256 0s256 114.7 256 256zm-32 0c0-123.2-100.3-224-224-224C132.5 32 32 132.5 32 256s100.5 224 224 224 224-100.5 224-224zM160.9 124.6l86.9 37.1-37.1 86.9-86.9-37.1 37.1-86.9zm110 169.1l46.6 94h-14.6l-50-100-48.9 100h-14l51.1-106.9-22.3-9.4 6-14 68.6 29.1-6 14.3-16.5-7.1zm-11.8-116.3l68.6 29.4-29.4 68.3L230 246l29.1-68.6zm80.3 42.9l54.6 23.1-23.4 54.3-54.3-23.1 23.1-54.3z"/></svg><span class="menu-item-name">Tech</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/posts/life/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon pencil-alt"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg><span class="menu-item-name">Life</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/categories/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon folder"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg><span class="menu-item-name">Categories</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/photos/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" class="icon eye"><path d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"/></svg><span class="menu-item-name">Photos</span></a>
                </li>
            
        
            
                
                    
                    
                        <li class="menu-item">
                            <a id="theme-switcher" href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5l48.8-97.5a18 18 0 0128 0l48.8 97.5 103.4 -34.5a18 18 0 0119.8 19.8l-34.5 103.4l97.5 48.8a18 18 0 010 28l-97.5 48.8 34.5 103.4a18 18 0 01-19.8 19.8l-103.4-34.5-48.8 97.5a18 18 0 01-28 0l-48.8-97.5l-103.4 34.5a18 18 0 01-19.8-19.8l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8-34.5-103.4a18 18 0 0119.8-19.8zM256 128a128 128 0 10.01 0M256 160a96 96 0 10.01 0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412a256 256 0 10154-407a11.5 11.5 0 00-5 20a201.5 201.5 0 01-134 374a11.5 11.5 0 00-15 13"/></svg></a>
                        </li>
                    
                
            
        
            
                
            
        
            
                <li class="menu-item search-item">
                        <form id="search" class="search" role="search">
    <label for="search-input"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon search-icon"><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></label>
    <input type="search" id="search-input" class="search-input">
</form>

<template id="search-result" hidden>
    <article class="content post">
        <h2 class="post-title"><a class="summary-title-link"></a></h2>
        <summary class="summary"></summary>
        <div class="read-more-container">
            <a class="read-more-link"> »</a>
        </div>
    </article>
</template>

                    </li>
                
            
        
    </ul>
</nav>

                    
                </div>
            </div>
            
    <input type="checkbox" id="nav-toggle" aria-hidden="true" />
    <label for="nav-toggle" class="nav-toggle"></label>
    <label for="nav-toggle" class="nav-curtain"></label>


        
    </header>




            
            
    <main class="main single" id="main">
    <div class="main-inner">

        

        <article class="content post h-entry" data-small-caps="true" data-align="justify" data-type="posts" data-toc-num="true">

            <h1 class="post-title p-name">基于图匹配的平面匹配算法</h1>

            

            

            
                

<div class="post-meta">
    
        
        <time datetime="2021-01-12T15:18:48&#43;08:00" class="post-meta-item published dt-published"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M148 288h-40c-6.6 0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h48c26.5 0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"/></svg>&nbsp;2021.1.12</time>
    
    
        
        <time datetime="2021-02-05T05:05:18&#43;08:00" class="post-meta-item modified dt-updated"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M400 64h-48V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H160V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H48C21.49 64 0 85.49 0 112v352c0 26.51 21.49 48 48 48h352c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm-6 400H54a6 6 0 0 1-6-6V160h352v298a6 6 0 0 1-6 6zm-52.849-200.65L198.842 404.519c-4.705 4.667-12.303 4.637-16.971-.068l-75.091-75.699c-4.667-4.705-4.637-12.303.068-16.971l22.719-22.536c4.705-4.667 12.303-4.637 16.97.069l44.104 44.461 111.072-110.181c4.705-4.667 12.303-4.637 16.971.068l22.536 22.718c4.667 4.705 4.636 12.303-.069 16.97z"/></svg>&nbsp;2021.2.5</time>
    
    
    
        
        
            
            
                
                
                
                
                    
                    
                    
                        
                            
                            
                        
                    
                
            
            
            
                <span class="post-meta-item category"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>&nbsp;<a href="/posts/tech/dl/" class="category-link p-category">posts\Tech\DL\</a></span>
            
        
        
    
    
        
        <span class="post-meta-item wordcount"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;4303</span>
    
    
        
        <span class="post-meta-item reading-time"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;9&nbsp;</span>
    
    
        
            
            <span class="post-meta-item busuanzi-page-pv" id="busuanzi_container_page_pv"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" class="icon post-meta-icon"><path d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"/></svg>&nbsp;<span id="busuanzi_value_page_pv"></span></span>
        
    
    
</div>

            

            <nav class="contents">
  <h2 id="contents" class="contents-title"></h2><ol class="toc">
    <li>
      <ol>
        <li><a id="contents:平面提取" href="#平面提取">平面提取</a>
          <ol>
            <li><a id="contents:几何方法" href="#几何方法">几何方法</a></li>
            <li><a id="contents:神经网络提取平面" href="#神经网络提取平面">神经网络提取平面</a></li>
            <li><a id="contents:关于平面提取还可以改善的地方" href="#关于平面提取还可以改善的地方">关于平面提取还可以改善的地方</a></li>
            <li><a id="contents:用实例分割提取平面" href="#用实例分割提取平面">用实例分割提取平面</a></li>
            <li><a id="contents:mean-shift聚类算法" href="#mean-shift聚类算法">Mean Shift聚类算法</a></li>
            <li><a id="contents:损失函数" href="#损失函数">损失函数</a></li>
            <li><a id="contents:实验结果" href="#实验结果">实验结果</a></li>
          </ol>
        </li>
        <li><a id="contents:提取平面特征构建特征描述子" href="#提取平面特征构建特征描述子">提取平面特征构建特征描述子</a>
          <ol>
            <li><a id="contents:构建图节点" href="#构建图节点">构建图节点</a></li>
          </ol>
        </li>
        <li><a id="contents:利用图匹配的方法匹配平面" href="#利用图匹配的方法匹配平面">利用图匹配的方法匹配平面</a>
          <ol>
            <li><a id="contents:两阶段图匹配算法" href="#两阶段图匹配算法">两阶段图匹配算法</a></li>
            <li><a id="contents:实验结果-1" href="#实验结果-1">实验结果</a></li>
          </ol>
        </li>
        <li><a id="contents:参考文献" href="#参考文献">参考文献</a></li>
      </ol>
    </li>
  </ol>
</nav><div class="post-body e-content">
                <p>在人类感知的尺度上，平面是人类社会环境中最常见的结构之一，并且具有强大的约束能力，约束着大量的点/线及其所携带的信息。各种曲面都可以用平面进行近似，根据精度要求选择拟合的平面数量。实际应用中，许多计算机视觉任务都需要平面信息，比如：机器人领域中，识别地面、墙面等平面可用于路径规划、视觉导航，识别桌面、书架等平面可辅助机械手抓取和放置物品；增强现实、混合现实中，利用平面信息放置物品，或者更换桌面、地板、墙面的纹理可以进行快速展示；三维场景重建中，用平面而非点云可以实现对一个城市大规模、简洁的重建。</p>
<p>整个流程主要分成三个模块：</p>
<ol>
<li>平面提取</li>
<li>提取平面特征构建特征描述子</li>
<li>利用图匹配的方法匹配平面</li>
</ol>
<h3 id="平面提取"><a href="#平面提取" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:平面提取" class="headings">平面提取</a></h3>
<p>基于单张彩色图输入的平面提取任务旨在从图像中分割出所拍摄场景中的平面结构，并同时估计相机到平面的深度信息。</p>
<p>基于单张彩色图输入的平面提取首先是从图片中提取特征。传统方法关注的是几何基元的提取，比如点、线段等；也会使用纹理信息，比如颜色、形状等。神经网络强大的特征提取能力可以从像素点中提取信息，然后聚合成平面；也有研究是用神经网络提取图片中点、线之类的几何基元来构成平面。</p>
<h4 id="几何方法"><a href="#几何方法" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:几何方法" class="headings">几何方法</a></h4>
<p>几何方法提取平面使用一种自下而上的方法，即先寻找单张彩色图片中的几何基 元来 恢 复 三 维 信 息，从 而 进 一 步 提 取 平面。为了提高平面提取精度，大部分几何方法使用了一定的场景约束，最常用的是曼哈顿世界假设（曼哈顿世界假设是指场景中不同朝向的平面应相互正交，即分别对应三维笛卡尔坐标系中的ｘｙ面、ｘｚ面和ｙｚ 面。），这使得它们的应用场景也受到了极大得约束。</p>
<h4 id="神经网络提取平面"><a href="#神经网络提取平面" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:神经网络提取平面" class="headings">神经网络提取平面</a></h4>
<p>神经网络提取平面的主要流程是利用现有的网络作为编码器对图片信息进行提取，然后通过几个分支将这些信息解码成平面/非平面掩膜、平面实例分割掩膜和深度图，最终整合为完整的3Ｄ 平面模型，流程图如下：</p>
<img src="https://i.loli.net/2021/01/12/1F89yXeTJ3K2nHW.png" width = "80%" height = "40%" div align = center />
<p>大部分文献通过聚合像素点来提取平面，主要思路是将平面提取问题转化为语义分割问题或实例
分割问题。也有文献在曼哈顿世界假设下，提取图片中的３Ｄ线框（wireframe）然后提取平面。此方法用直线和连接点组成平面，因此提取出的平面边缘整齐平滑，而聚合像素的方法提取出的平面边缘相对粗糙。但该方法相应地对边缘为曲线的平面提取效果较差。</p>
<p>本文采纳上图的思想，利用单张RGB图像估计平面/非平面掩膜、平面实例分割掩膜和深度图。由于在本文中，平面提取模块主要用于提取平面区域对应的特征构建平面特征描述子，故不涉及对平面三维信息的准确估计。(为提高平面特征描述子的表达能力，后面也尝试做了归一化深度的估计，但是否对描述子表达能力有促进作用还待实验证明)</p>
<h4 id="关于平面提取还可以改善的地方"><a href="#关于平面提取还可以改善的地方" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:关于平面提取还可以改善的地方" class="headings">关于平面提取还可以改善的地方</a></h4>
<p>无论是何种方法，提取的平面边缘都存在一定问题：使用像素点聚合提取的平面边缘粗糙不平
滑，而通过直线段构成平面的方法对边缘为曲线的平面提取效果较差。基于单张彩色图输入的平面提取的未来工作主要是进一步提升平面分割和深度估计的精度。除此之外，还有很多拓展研究，比如：</p>
<ol>
<li>
<p>平面边缘的优化。如果平面由像素点构成，可对平面边缘进行平滑处理，或使用直线/曲线拟
合，或从图片中提取边缘信息后与平面的边缘结合。如果平面由线条构成，可以针对性地使用直线/曲线构成平面。</p>
</li>
<li>
<p>遮挡推理。单张彩色图包含的信息有限，从中提取的平面会因遮挡而部分缺失，比如被桌子遮
挡的墙面。使用图片中已有的纹理对平面缺失处进行填充，可极大提高平面重建的完整性和观赏性。</p>
</li>
<li>
<p>绝对深度。由于缺失深度的维度，根据单张图片重建出的三维模型与真实世界相差一个因子，即尺度。可以通过图片内特定物体的尺寸来确定绝对深度。</p>
</li>
<li>
<p>几何方法和神经网络方法的结合。两类方法各有优缺点，后续工作可通过取长补短的方式结合
两类方法，有以下两种思路可供参考：一是，传统方法难以有效建模图像中的不规则线段等几何信息，因此通过神经网络预测图像中的灭点、线段的位置等能更有效的提取图片中的几何信息，进而用传统几何方法根据已知的几何结构实现更高效的平面提取；二 是，利用神经网络提取的特征点 （例 如SuperPoint）替代传统人工特征点，提取更具有描述性和重复性的特征，进行平面位置以及平面边缘的定位。</p>
</li>
</ol>
<h4 id="用实例分割提取平面"><a href="#用实例分割提取平面" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:用实例分割提取平面" class="headings">用实例分割提取平面</a></h4>
<p>为了得到平面的特征表示，在本实验将神经网络(骨干网络选择Res101)作为编码器对图片信息进行提取，然后通过几个分支将这些信息解码成平面/非平面掩膜、平面实例分割掩膜和深度图。为了让网络推理速度更快，我没有采用如Mask-rcnn的两阶段法（即先回归锚框位置再实例分割），而是采用论文 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">[1]</a></sup>中的单阶段方法，首先通过编码器提取输入图像的高维信息，然后用三个解码器分别学习到平面/非平面掩膜，平面嵌入特征(Plane embeddings)以及深度信息。利用其中的平面嵌入特征和平面/非平面掩膜通过均值漂移(Mean Shift)聚类算法得到平面的分割实例。</p>
<h4 id="mean-shift聚类算法"><a href="#mean-shift聚类算法" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:mean-shift聚类算法" class="headings">Mean Shift聚类算法</a></h4>
<p>由于在实际场景中的平面数不固定，一些受初始聚类中心数目影响的算法(例如K-Means，K-Means++)并不适用于处理聚类个数未知的情形，对此，有一些改进的算法如Mean Shift算法被提出。与K-Means算法一样，Mean Shift算法是基于聚类中心的聚类算法，不同的是，Mean Shift算法不需要事先制定类别个数k。</p>
<p>对于给定的 $d$ 维空间 $R^d$ 中的 $n$ 个样本点 $x_i,i=1,⋯,n$，对于其中的点 $x$， 定义 Mean Shift向量的基本形式为：</p>
<p>$$
M_h(x)=\frac{1}{k}\sum_{x_i\in S_h}(x_i-x)
$$</p>
<p>其中，$S_h$ 指的是一个半径为h的高维球区域，如上图中的圆形区域。$S_h$ 的定义为：</p>
<p>$$
S_h(x)=(y \mid (y-x)( y-x)^T \leqslant h^2)
$$</p>
<p>为了使得随着样本与被偏移点的距离不同，其偏移量对均值偏移向量的贡献也不同，需要在Mean Shift算法中引入核函数，此时改进的Mean Shift向量形式：</p>
<p>$$
M_h(x)=\frac{\sum_{i=1}^{n}G(\frac{x_i-x}{h_i})(x_i-x)}{\sum_{i=1}^{n}G(\frac{x_i-x}{h_i})}
$$</p>
<p>其中， $$G(\frac{x_i-x}{h_i})$$ 为核函数(常见的核函数如高斯核函数，余弦函数等)。</p>
<p>计算 $M_h$ 时考虑距离的影响，同时也可以认为在所有的样本点 $X$ 中,重要性并不一样,因此对每个样本还引入一个权重系数。如此以来就可以把Mean Shift形式扩展为：</p>
<p>$$
M_h (x)=\frac{\sum_{i=1}^{n}G(\frac{x_i-x}{h_i})w(x_i)(x_i-x)}{\sum_{i=1}^{n}G(\frac{x_i-x}{h_i})w(x_i)}
$$</p>
<p>其中，$w(x_i)$ 是一个赋给采样点的权重。</p>
<p>对于Mean Shift算法，它是一个迭代的步骤，即先算出当前点的偏移均值，将该点移动到此偏移均值，然后以此为新的起始点，继续移动，直到满足最终的条件。</p>
<p>Mean-Shift 聚类就是对于集合中的每一个元素，对它执行下面的操作：把该元素移动到它邻域中所有元素的特征值的均值的位置，不断重复直到收敛。准确的说，不是真正移动元素，而是把该元素与它的收敛位置的元素标记为同一类。</p>
<p>一般的均值漂移聚类会在每次迭代中对每个像素有 $N$ 次核函数的计算( $N$ 为像素总数)，计算复杂度是 $O(N^2)$ 。为了提高均值漂移聚类的效率，论文<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>中生成 $k^d$ 个锚点，其中 $k$ 是每个维度的锚点数，$d$ 是嵌入特征的维度，然后仅仅计算 锚点与每个像素的核函数，计算复杂度是 $O(k^d \times N)$, 当 $k^d &laquo; N$ 时，算法执行效率大大提高。聚类完成后，论文<sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>再将小于设定阈值的锚点合并作为平面实例的聚类中心。</p>
<p><img src="https://i.loli.net/2021/01/30/JypZtAfBxYrG1cQ.png" alt="a-fistful-of-dollars.png"><span class="caption">◎ 应用均值漂移聚类后的像素分布示意图</span></p>
<p><a href="https://www.biaodianfu.com/mean-shift.html" target="_blank" rel="noopener">文章链接</a></p>
<h4 id="损失函数"><a href="#损失函数" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:损失函数" class="headings">损失函数</a></h4>
<ul>
<li>平面/非平面掩膜的损失函数采用平衡交叉熵损失：</li>
</ul>
<p>$$
L_{S}=-(1-w) \sum_{i \in \mathcal{F}} \log p_{i}-w \sum_{i \in \mathcal{B}} \log \left(1-p_{i}\right)
$$</p>
<p>其中 $\mathcal{F}$ 和 $\mathcal{B}$ 是 前景和背景像素的集合， $p_i$ 是第 $i$ 个像素属于前景(平面)的概率， $w$ 是前景和背景的比率，是一个超参数。</p>
<ul>
<li>嵌入特征的损失函数采用判别损失(discriminative loss),</li>
</ul>
<p>该损失包括两部分：<em>pull loss</em> 和 <em>push loss</em>,  <em>pull loss</em>将高维嵌入特征向聚类中心靠拢， <em>push loss</em> 使 聚类中心相互远离。</p>
<p>$$
L_{E}=L_{p u l l}+L_{p u s h}
$$</p>
<p>其中：</p>
<p>$$
L_{\text {pull }}=\frac{1}{C} \sum_{c=1}^{C} \frac{1}{N_{c}} \sum_{i=1}^{N_{c}} \max \left(\left|\mu_{c}-x_{i}\right|-\delta_{\mathrm{v}}, 0\right)
$$</p>
<p>$$
L_{\text {push }}=\frac{1}{C(C-1)} \sum_{c_{A}=1}^{C} \sum_{c_{B}=1}^{C} \max \left(\delta_{\mathrm{d}}-\left|\mu_{c_{A}}-\mu_{c_{B}}\right|,0\right).
c_{A} \neq c_{B}
$$</p>
<p>其中， $C$ 是聚类中心的个数（平面个数）， $N_c$ 是聚类区域 $c$ 中的元素数， $x_i$ 是 嵌入特征， $\mu_c$ 是聚类区域 $c$ 中的平均嵌入特征， $\delta_{\mathrm{v}}$ 和 $\delta_{\mathrm{d}}$ 是超参数。</p>
<ul>
<li>深度图回归采用 <em>Huber loss</em></li>
</ul>
<p>$$
L_D = \operatorname{loss}(x, y)=\frac{1}{n} \sum_{i} z_{2}
$$</p>
<p>其中</p>
<p>$$
z_{\imath}=0.5\left(x_{2}-y_{2}\right)^{2}, \text { if }\left|x_{2}-y_{i}\right|&lt;1
$$</p>
<p>$$
z_{\imath}= \left|x_{\imath}-y_{\imath}\right|-0.5, \text { otherwise }
$$</p>
<h4 id="实验结果"><a href="#实验结果" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:实验结果" class="headings">实验结果</a></h4>
<p>第一二三四行依次为：平面分割结果， 平面分割和RGB图像混合结果， 平面/非平面掩模， 深度图。第一二三列依次为：原图， 网络预测结果， 真值。</p>
<pre tabindex="0"><code></code></pre><p><img src="https://i.loli.net/2021/02/01/BNzyh7UIxHDj14o.jpg" alt="226567203.jpg"></p>
<p><img src="https://i.loli.net/2021/02/01/6lM3jFnu4aCWpIJ.jpg" alt="1770929547.jpg"></p>
<h3 id="提取平面特征构建特征描述子"><a href="#提取平面特征构建特征描述子" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:提取平面特征构建特征描述子" class="headings">提取平面特征构建特征描述子</a></h3>
<h4 id="构建图节点"><a href="#构建图节点" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:构建图节点" class="headings">构建图节点</a></h4>
<p>在前面已经得到平面实例分割的前提下，对于每一个平面实例，选用一个矩形包围框，该包围框恰好能包含对应平面实例。然后用该包围框裁取前面特征提取网络编码器输出的深度嵌入特征区域以及对应的深度图对应区域，将两部分区域重采样到 $128 \times 128 \times N$， 其中 $N$ 是通道维数，对于深度嵌入特征，$N=64$， 对于深度图 $N=1$。</p>
<p>为了减低后续的网络的计算复杂度，提取的深度嵌入特征和深度图经过两次 <em>max pooling</em> 和一次 <em>mean pooling</em>， <em>max pooling</em> 同时还具有一定的抗平面旋转/平移的作用。</p>
<p>深度嵌入特征和深度图经过多层感知机(MLP)统一特征维度后级联在一起当做图网络的节点特征。</p>
<h3 id="利用图匹配的方法匹配平面"><a href="#利用图匹配的方法匹配平面" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:利用图匹配的方法匹配平面" class="headings">利用图匹配的方法匹配平面</a></h3>
<h4 id="两阶段图匹配算法"><a href="#两阶段图匹配算法" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:两阶段图匹配算法" class="headings">两阶段图匹配算法</a></h4>
<p>论文<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">[2]</a></sup> 中利用图内卷积和跨图卷积汇聚节点邻域特征，然后通过最优传输理论得到两个图之间的节点匹配关系，该方法由于将节点特征学习和全局匹配分开对待，因此网络运算效率不高；此外， 由于仅考虑局部嵌入，该方法可能倾向于不一致地匹配图之间的邻域。(邻域一致性保证邻域的节点不会匹配到待匹配图的不同区域)</p>
<p>论文<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">[3]</a></sup> 提出了一种完全可微分两阶段图匹配的方法，旨在由数据驱动解决
邻域一致的图节点匹配问题，而无需在网络推理阶段解决任何优化问题。</p>
<p>二阶段图匹配网络结构图如下所示：</p>
<pre tabindex="0"><code></code></pre><p><img src="https://i.loli.net/2021/01/30/utPzsFLSaUqGrmv.png" alt=""><span class="caption">◎ 二阶段图匹配网络</span></p>
<p>给定源图 $\mathcal{G}_{s}$ 和 目标图 $\mathcal{G}_{t}$, 首先通过GNN特征汇聚节点邻域特征, 即 $\boldsymbol{H}_{s}=\boldsymbol{\Psi}_{\theta_{1}}\left(\boldsymbol{X}_{s}, \boldsymbol{A}_{s}, \boldsymbol{E}_{s}\right) \in \mathbb{R}^{\left|\mathcal{V}_{s}\right| \times .}$ 和 $\boldsymbol{H}_{t}=\boldsymbol{\Psi}_{\theta_{1}}\left(\boldsymbol{X}_{t}, \boldsymbol{A}_{t}, \boldsymbol{E}_{t}\right) \in \mathbb{R}^{\left|\mathcal{V}_{t}\right| \times .}$, 其中 ${\Psi}_{\theta_{1}}$ 选用：</p>
<p>$$
\vec{h}_{i}^{(t+1)}= {\Psi}_{\theta_{1}}(\vec{h}_{i}^{(t)}) = \sigma\left(\boldsymbol{W}^{(t+1)} \vec{h}_{i}^{(t)}+\sum_{j \rightarrow i} \Phi_{\theta}^{(t+1)}\left(\vec{e}_{j, i}\right) \cdot \vec{h}_{j}^{(t)}\right)
$$</p>
<p>这样，通过特征汇聚得到 $\boldsymbol{H}_{s}$ 和 $\boldsymbol{H}_{t}$ 后，通过矩阵行间的softmax 正则化后，计算源图和目标图的节点相似度，选取相似度最高的当做匹配节点，得到初始的匹配矩阵 $\boldsymbol{S}^{(0)}$。</p>
<p>损失函数：</p>
<p>$$
\mathcal{L}^{\text {(initial) }}=-\sum_{i \in \mathcal{V}_{s}} \log \left(S_{i, \pi_{\mathrm{gr}}(i)}^{(0)}\right)
$$</p>
<p>其中 $\pi_{\mathrm{gr}}$ 是监督标签的目标图中与源图节点 $i$ 匹配的节点。</p>
<p>由于初始的匹配仅基于局部的匹配，网络有可能会让节点特征相似但不满足局部一致性的节点匹配上，这需要引入二阶段的匹配一致性学习。</p>
<p>对于二阶段的学习， 给定第 $l$ 次迭代时网络的匹配矩阵是 $S^{(l)}$, 通过共享权重的图网络 $\Psi_{\theta_{2}}$ 执行同步消息传递：</p>
<p>$$
\boldsymbol{O}_{s}=\boldsymbol{\Psi}_{\theta_{2}}\left(\boldsymbol{I}_{\left|\mathcal{V}_{s}\right|}, \boldsymbol{A}_{s}, \boldsymbol{E}_{s}\right) \quad \text { and } \quad \boldsymbol{O}_{t}=\boldsymbol{\Psi}_{\theta_{2}}\left(\boldsymbol{S}_{(l)}^{\top} \boldsymbol{I}_{\left|\mathcal{V}_{s}\right|}, \boldsymbol{A}_{t}, \boldsymbol{E}_{t}\right)
$$</p>
<p>上面的 $\Phi_{\theta_{1}}$ 和 $\Phi_{\theta_{2}}$ 都采用相同的网络结构：</p>
<p>$$
\vec{h}_{i}^{(t+1)}=\sigma\left(\boldsymbol{W}^{(t+1)} \vec{h}_{i}^{(t)}+\sum_{j \rightarrow i} \Phi_{\theta}^{(t+1)}\left(\vec{e}_{j, i}\right) \cdot \vec{h}_{j}^{(t)}\right)
$$</p>
<p>其中 $\Phi_{\theta}(.)$ 是关于边特征的可学习参数。</p>
<p>通过计算差值向量 $\vec{d}_{i, j}=\vec{o}_{i}^{(s)}-\vec{o}_{j}^{(t)}$ ,  更新 匹配矩阵：</p>
<p>$$
S_{i, j}^{(l+1)}=\operatorname{softmax}\left(\hat{\boldsymbol{S}}^{(l+1)}\right)_{i, j} \text { with } \hat{S}_{i, j}^{(l+1)}=\hat{S}_{i, j}^{(l)}+\Phi_{\theta_{3}}\left(\vec{d}_{j, i}\right)
$$</p>
<p>其中 $\Phi_{\theta_{3}}$ 是多层感知机。</p>
<p>网络的最终损失由特征匹配损失和邻域一致性损失构成：$\mathcal{L}=\mathcal{L}^{(\text {initial) }}+\mathcal{L}^{\text {(refined) }}$ with $\mathcal{L}^{\text {(refined) }}=-\sum_{i \in \mathcal{V}_{s}} \log \left(S_{i, \pi_{\mathrm{m}}(i)}^{(L)}\right)$ 。</p>
<h4 id="实验结果-1"><a href="#实验结果-1" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:实验结果-1" class="headings">实验结果</a></h4>
<p>每张图第一第二行是匹配真值，第三四行是网络预测匹配结果。</p>
<img src="https://i.loli.net/2021/02/01/KNvgIDfE56c7AHU.png" width = "100%" height = "40%" div align = center />
<img src="https://i.loli.net/2021/02/01/Y79CliJdkZu2bc1.png" width = "100%" height = "40%" div align = center />
<img src="https://i.loli.net/2021/02/01/OswHpJDXMcEqe7Z.png" width = "100%" height = "40%" div align = center />
<img src="https://i.loli.net/2021/02/01/6NQidJBqVz1mGFv.png" width = "100%" height = "40%" div align = center />
<h3 id="参考文献"><a href="#参考文献" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:参考文献" class="headings">参考文献</a></h3>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Yu Z, Zheng J, Lian D, et al. Single-image piece-wise planar 3d reconstruction via associative embedding[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 1029-1037.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512" class="icon footnote-icon"><path d="M177 159.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 255.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 329.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1z"/></svg></a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>R. Wang, J. Yan, and X. Yang. Learning combinatorial embedding networks for deep graph matching. In ICCV, 2019b.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512" class="icon footnote-icon"><path d="M177 159.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 255.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 329.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1z"/></svg></a></p>
</li>
<li id="fn:3">
<p>Fey M, Lenssen J E, Morris C, et al. Deep graph matching consensus[J]. arXiv preprint arXiv:2001.09621, 2020.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512" class="icon footnote-icon"><path d="M177 159.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 255.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 329.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1z"/></svg></a></p>
</li>
</ol>
</div>
            </div>

            


        </article>

        

        
    <div class="updated-badge-container">
        <span title="Updated @ 2021-02-05 05:05:18 CST" style="cursor:help">

<svg xmlns="http://www.w3.org/2000/svg" width="130" height="20" class="updated-badge"><linearGradient id="b" x2="0" y2="100%"><stop offset="0" stop-color="#bbb" stop-opacity=".1"/><stop offset="1" stop-opacity=".1"/></linearGradient><clipPath id="a"><rect width="130" height="20" rx="3" fill="#fff"/></clipPath><g clip-path="url(#a)"><path class="updated-badge-left" d="M0 0h55v20H0z"/><path class="updated-badge-right" d="M55 0h75v20H55z"/><path fill="url(#b)" d="M0 0h130v20H0z"/></g><g fill="#fff" text-anchor="middle" font-size="110"><text x="285" y="150" fill="#010101" fill-opacity=".3" textLength="450" transform="scale(.1)">updated</text><text x="285" y="140" textLength="450" transform="scale(.1)">updated</text><text x="915" y="150" fill="#010101" fill-opacity=".3" textLength="650" transform="scale(.1)">2021-02-05</text><text x="915" y="140" textLength="650" transform="scale(.1)">2021-02-05</text></g></svg>
        </span></div>



        


        


        
    
    
        <div class="related-posts">
            <h2 class="related-title"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon related-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm144 276c0 6.6-5.4 12-12 12h-92v92c0 6.6-5.4 12-12 12h-56c-6.6 0-12-5.4-12-12v-92h-92c-6.6 0-12-5.4-12-12v-56c0-6.6 5.4-12 12-12h92v-92c0-6.6 5.4-12 12-12h56c6.6 0 12 5.4 12 12v92h92c6.6 0 12 5.4 12 12v56z"/></svg></h2>
            <ul class="related-list">
                
                    <li class="related-item">
                        <a href="/posts/tech/dl/tensorrt_intro/" class="related-link">TensorRT学习笔记</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/tech/attention/att_intro/" class="related-link">Transformer简介</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/tech/dl/deploy_torch_script/" class="related-link">利用TorchScript+libtorch部署c++模型</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/tech/gnn/gin/" class="related-link">GIN</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/tech/attention/swim_transformer/" class="related-link">Swin Transformer总结</a>
                    </li>
                
            </ul>
        </div>
    



        


        
    <footer class="minimal-footer">
        
            <div class="post-tag"><a href="/tags/dl/" rel="tag" class="post-tag-link">#dl</a></div>
        
        
            <div class="post-category">
                <a href="/posts/" class="post-category-link active">posts</a>
            </div>
        
        
    </footer>



        


        
    
        
        
    
    
    
    
        <ul class="post-nav">
            
                <li class="post-nav-prev">
                    <a href="/posts/tech/dl/deploy_torch_script/" rel="prev">&lt; 利用TorchScript+libtorch部署c++模型</a>
                </li>
            
            
                <li class="post-nav-next">
                    <a href="/posts/tech/dl/basic_network_struct/" rel="next">pytorch 搭建网络结构 &gt;</a>
                </li>
            
        </ul>
    



        


    </div>
</main>


            
    <div id="back-to-top" class="back-to-top">
        <a href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a>
    </div>


            
    <footer id="footer" class="footer">
        <div class="footer-inner">
            <div class="site-info">2019–2023&nbsp;<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon footer-icon"><path d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"/></svg>&nbsp;JiaJie</div>

            
    
        <ul class="socials"><li class="socials-item">
                    <a href="https://s2.loli.net/2022/03/20/f6lhgGukJ9QT2Yv.jpg" target="_blank" rel="external noopener" title="wechat"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon social-icon"><path d="M0 224h192V32H0v192zM64 96h64v64H64V96zm192-64v192h192V32H256zm128 128h-64V96h64v64zM0 480h192V288H0v192zm64-128h64v64H64v-64zm352-64h32v128h-96v-32h-32v96h-64V288h96v32h64v-32zm0 160h32v32h-32v-32zm-64 0h32v32h-32v-32z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://github.com/Wjiajie" target="_blank" rel="external noopener" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a>
                </li></ul>
    



            

<div class="container" role="main" itemscope itemtype="http://schema.org/Article">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
           
            <article role="main" class="blog-post" itemprop="articleBody" id="content">
              
                
                
                
            </article>



        </div>
    </footer>


        </div>
        

        


    <script>
    if (typeof MathJax === 'undefined') {
        window.MathJax = {
            loader: {
                load: ['[tex]/mhchem']
            },
            
            tex: {
                inlineMath: {'[+]': [['$', '$']]},
                tags: 'ams',
                packages: {'[+]': ['mhchem']}
            }
        };
        (function() {
            const script = document.createElement('script');
            script.src = 'https:\/\/cdn.jsdelivr.net\/npm\/mathjax@3.1.2\/es5\/tex-mml-chtml.js';
            script.defer = true;
            document.head.appendChild(script);
        })();
    } else {
        MathJax.texReset();
        MathJax.typeset();
    }
</script>




    <script src="https://cdn.jsdelivr.net/npm/mermaid@8.8.3/dist/mermaid.min.js"></script>
<script>
    const mermaidConfig = {
        startOnLoad: true,
        flowchart: {
            useMaxWidth: false,
            htmlLabels: true
        },
        theme: 'default'
    };
    mermaid.initialize(mermaidConfig);
</script>





    <script src="/js/medium-zoom.min.js"></script>

<script>
    mediumZoom(document.querySelectorAll('div.post-body img'), {
        background: 'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'
    })
</script>




    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module" defer></script>



    
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    




    </body>
</html>
